{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb5e9e91-2c23-47f6-9d8a-704b154caa51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ======================\n",
    "# Bibliotecas padrão para utilidades básicas e manipulação de arquivos:\n",
    "# ======================\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import zipfile\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ======================\n",
    "# Registros de logs:\n",
    "# ======================\n",
    "import logging\n",
    "\n",
    "# ======================\n",
    "# Requisições e manipulação de conteúdo web:\n",
    "# ======================\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ======================\n",
    "# Utilidades e manipulação de dados:\n",
    "# ======================\n",
    "from collections import Counter\n",
    "import chardet\n",
    "import string\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import shutil\n",
    "#import pycep_correios  # Descomente se necessário\n",
    "import brazilcep\n",
    "\n",
    "# ======================\n",
    "# Manipulação de datas:\n",
    "# ======================\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# ======================\n",
    "# Criptografia:\n",
    "# ======================\n",
    "import secrets\n",
    "import base64\n",
    "from Crypto.Cipher import AES\n",
    "from Crypto.Random import get_random_bytes\n",
    "\n",
    "# ======================\n",
    "# Multitarefas:\n",
    "# ======================\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# ======================\n",
    "# Spark:\n",
    "# ======================\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import (\n",
    "    regexp_replace,\n",
    "    when,\n",
    "    length,\n",
    "    to_date,\n",
    "    upper,\n",
    "    lower,\n",
    "    col,\n",
    "    udf,\n",
    "    split,\n",
    "    explode,\n",
    "    coalesce,\n",
    "    concat_ws,\n",
    "    concat,\n",
    "    lit,\n",
    "    broadcast,\n",
    "    regexp_extract,\n",
    "    expr\n",
    ")\n",
    "import pyspark.sql.functions as F\n",
    "from functools import reduce\n",
    "\n",
    "# ======================\n",
    "# Geolocalização:\n",
    "# ======================\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b83b4476-9d8c-4320-9b1c-6106a41a3a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a5add14-6a82-4bfb-893e-51926de1bd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bibliotecas padrão: Essas são bibliotecas padrão do Python para operações gerais e manipulação de arquivos.\n",
    "\n",
    "#Registros de logs: Para fazer registros de logs.\n",
    "\n",
    "#Requisições e manipulação de conteúdo web: Usado para fazer requisições web e manipular conteúdo HTML/XML.\n",
    "\n",
    "#Utilidades e manipulação de dados: Conjunto diversificado de bibliotecas para manipulação de dados, codificação de arquivos e outras utilidades.\n",
    "\n",
    "#Manipulação de datas: Para cálculos e manipulações relacionados a datas.\n",
    "\n",
    "#Criptografia: Utilitários e bibliotecas para lidar com criptografia.\n",
    "\n",
    "#Multitarefas: Permite a execução paralela de tarefas.\n",
    "\n",
    "#Spark: Todas as importações relacionadas ao PySpark.\n",
    "\n",
    "#Geolocalização: Bibliotecas para geolocalização e geocodificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62032f92-b64c-45ba-ad13-a96ae536d2c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\apps\\opt\\spark-3.5.0-bin-hadoop3\n",
      "C:\\dev\\Hadoop\\hadoop-3.2.0\n",
      "C:\\Program Files\\Java\\jre-1.8\n"
     ]
    }
   ],
   "source": [
    "print(os.environ.get(\"SPARK_HOME\"))\n",
    "print(os.environ.get(\"HADOOP_HOME\"))\n",
    "print(os.environ.get(\"JAVA_HOME\"))\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c84a0-bdb1-4aca-9c85-87eb63788c04",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configuração do Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862a4b89-8b75-4333-abf1-b12c5816f79c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "    .master(\"local[*]\")  \n",
    "    .config(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\")\n",
    "    .config(\"spark.executor.resource.gpu.amount\", \"1\") \n",
    "    .config(\"spark.rapids.memory.pinnedPool.size\", \"2G\")\n",
    "    .config(\"spark.driver.cores\", \"3\")\n",
    "    .config(\"spark.driver.memory\", \"10g\")  \n",
    "    .config(\"spark.default.parallelism\", \"10\")  # Considerando o uso de 10 threads no total\n",
    "    .config(\"spark.executor.cores\", \"2\")  \n",
    "    .config(\"spark.executor.instances\", \"2\")  \n",
    "    .config(\"spark.executor.memory\", \"18g\")  \n",
    "    .config(\"spark.memory.fraction\", \"0.8\")\n",
    "    .config(\"spark.memory.storageFraction\", \"0.5\")\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "    .config(\"spark.memory.offHeap.size\", \"2g\")\n",
    "    .config('spark.sql.repl.eagerEval.enabled', True)\n",
    "    .config(\"spark.sql.repl.eagerEval.maxNumRows\", 10)\n",
    "    .appName('dataset_cnpj')\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87126c24-d22e-4b01-8612-b6016c02c925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RAM: 32GB\n",
    "# CPU: 6 núcleos (12 threads)\n",
    "# Storage: SSD de 1TB\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .master(\"local[*]\")  # Utilizar todos os 12 threads disponíveis\n",
    "    .config(\"spark.driver.cores\", \"3\")  # Alocar metade dos núcleos para o driver\n",
    "    .config(\"spark.driver.memory\", \"12g\")  # Alocar 8GB para a memória do driver\n",
    "    .config(\"spark.default.parallelism\", \"100\")  # Paralelismo padrão baseado no número de threads\n",
    "    .config(\"spark.executor.cores\", \"2\")  # Como está em modo local, o executor pode usar metade dos núcleos\n",
    "    .config(\"spark.executor.instances\", \"2\")  # Em modo local, você geralmente tem apenas 1 instância de executor\n",
    "    .config(\"spark.executor.memory\", \"4g\")  # Alocar 16GB para a memória do executor\n",
    "    .config(\"spark.memory.fraction\", \"0.8\")  # Fraction da heap memory a ser usada para armazenamento e cache\n",
    "    .config(\"spark.memory.storageFraction\", \"0.5\")  # Fraction da memória de armazenamento que é reservada como memória não imune a evicção\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\")  # Habilitar memória off-heap\n",
    "    .config(\"spark.memory.offHeap.size\", \"2g\")  # Alocar 4GB para off-heap\n",
    "    .config(\"spark.executor.memoryOverhead\", \"1g\")  # Overhead de memória fora do heap para o executor. Se não for definido, Spark calculará um valor padrão\n",
    "    .config('spark.sql.repl.eagerEval.enabled', True)\n",
    "    .config(\"spark.sql.repl.eagerEval.maxNumRows\", 10)\n",
    "    .appName('dataset_cnpj')\n",
    "    .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f18e528-a78a-4606-b2a2-e9f8d1c7b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "    .master(\"local[8]\")  # Use all 8 threads.\n",
    "    .config(\"spark.driver.cores\", \"3\")  # Use half the threads for the driver.\n",
    "    .config(\"spark.driver.memory\", \"8g\")  # Allocate 12GB to the driver to leave room for the OS and other processes.\n",
    "    .config(\"spark.default.parallelism\", \"16\")  # Default parallelism, you can adjust based on your data and tasks. Double the thread count is a good start.\n",
    "    .config(\"spark.executor.cores\", \"2\")  # Use 4 cores per executor.\n",
    "    .config(\"spark.executor.instances\", \"1\")  # Since it's local mode, only one executor instance.\n",
    "    .config(\"spark.executor.memory\", \"4g\")  # Assign 8GB for executor memory. Adjust based on your needs.\n",
    "    .config(\"spark.memory.fraction\", \"0.7\")  # Fraction of (heap space - 300MB) used for execution and storage. Adjust if needed.\n",
    "    .config(\"spark.memory.storageFraction\", \"0.5\")  # Amount of storage memory immune to eviction, expressed as a fraction of the size of the region set aside by spark.memory.fraction.\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\")  \n",
    "    .config(\"spark.memory.offHeap.size\", \"1g\")  # 3GB off-heap memory.\n",
    "    .config(\"spark.executor.memoryOverhead\", \"1g\")  # Overhead memory. You might need to adjust depending on your tasks.\n",
    "    .config('spark.sql.repl.eagerEval.enabled', True)\n",
    "    .config(\"spark.sql.repl.eagerEval.maxNumRows\", 10)\n",
    "    .appName('dataset_cnpj')\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fd92aef-8048-4c0e-9d72-fdf76dfd96a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>dataset_cnpj</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2445de594c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee053cf7-d2fe-47f6-b844-c7659c9b316e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-magic\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Installing collected packages: python-magic\n",
      "Successfully installed python-magic-0.4.27\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20f2d2d6-083e-46fe-b08f-bf68650a459e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-magic-bin\n",
      "  Downloading python_magic_bin-0.4.14-py2.py3-none-win_amd64.whl (409 kB)\n",
      "     -------------------------------------- 409.3/409.3 kB 8.5 MB/s eta 0:00:00\n",
      "Installing collected packages: python-magic-bin\n",
      "Successfully installed python-magic-bin-0.4.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-magic-bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd7799c1-e915-4333-b50d-b867406edc8a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_22072/4068691271.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\PEDRO~1.ALM\\AppData\\Local\\Temp/ipykernel_22072/4068691271.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    pip install chardet\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install chardet\n",
    "pip install python-magic\n",
    "pip install python-magic-bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9965f590-1262-4760-b18f-cf270e1c335e",
   "metadata": {},
   "source": [
    "# Classe extração receita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8607b9a-ac1a-4d97-9ef8-790b2f4a71e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReceitaCNPJApi:\n",
    "    \"\"\"\n",
    "    Classe ReceitaCNPJApi:\n",
    "\n",
    "    Esta classe é responsável por interagir com a API de Dados Abertos da Receita Federal, facilitando \n",
    "    a obtenção de informações relacionadas a CNPJs, como empresas, sócios, municípios, entre outros. \n",
    "    A classe inclui funcionalidades para baixar, descompactar e salvar dados, além de utilitários \n",
    "    para manipular e consultar URLs.\n",
    "    \"\"\"\n",
    "\n",
    "    # URL base da API de dados abertos da Receita Federal.\n",
    "    BASE_URL = \"https://dadosabertos.rfb.gov.br/CNPJ\"\n",
    "\n",
    "    # Prefixos de arquivos que podem ser baixados da API.\n",
    "    FILE_PREFIXES = ['Estabelecimentos', 'Municipios', 'Simples', 'Empresas', 'Cnaes', 'Socios', 'Naturezas','Qualificacoes','Paises','Motivos']\n",
    "\n",
    "    # Número máximo de tentativas ao fazer uma solicitação para a API.\n",
    "    MAX_ATTEMPTS = 15\n",
    "\n",
    "    # Tempo de espera entre tentativas de solicitações (em segundos).\n",
    "    WAIT_TIME = 180\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializador da classe. Configura o logger para registrar atividades e erros.\n",
    "        \"\"\"\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_last_month():\n",
    "        \"\"\"\n",
    "        Retorna a data atual.\n",
    "        \"\"\"\n",
    "        return (datetime.today()).date()\n",
    "\n",
    "    def get_output_path_for_prefix(self, prefix):\n",
    "        \"\"\"\n",
    "        Gera o caminho completo de saída para armazenar os dados com base em um prefixo fornecido.\n",
    "\n",
    "        Args:\n",
    "        prefix (str): Prefixo do arquivo para o qual o caminho de saída será gerado.\n",
    "\n",
    "        Returns:\n",
    "        str: Caminho completo de saída para o prefixo especificado.\n",
    "        \"\"\"\n",
    "        return os.path.join(self.BASE_OUTPUT_PATH, prefix, f\"{prefix}.csv\")\n",
    "\n",
    "    def get_most_common_date(self):\n",
    "        \"\"\"\n",
    "        Consulta a API para obter a data mais comum em que os arquivos foram atualizados.\n",
    "        \"\"\"\n",
    "\n",
    "        # Etapa 1: Tentar consultar a API e obter uma resposta.\n",
    "        try:\n",
    "            response = requests.get(self.BASE_URL)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Etapa 2: Usar a biblioteca BeautifulSoup para analisar a resposta HTML.\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Etapa 3: Selecionar todos os elementos de data da resposta HTML.\n",
    "            # Assumindo que a data é o terceiro elemento filho da tag 'tr'.\n",
    "            date_elements = soup.select('tr > td:nth-child(3)')\n",
    "\n",
    "            # Etapa 4: Transformar cada elemento de data em um objeto datetime.\n",
    "            dates = [datetime.strptime(elem.get_text().strip(), '%Y-%m-%d %H:%M') for elem in date_elements if elem.get_text().strip() != '']\n",
    "\n",
    "            # Etapa 5: Contar as ocorrências de cada data e determinar a data mais comum.\n",
    "            most_common_date, _ = Counter([date.date() for date in dates]).most_common(1)[0]\n",
    "\n",
    "            # Etapa 6: Retornar a data mais comum.\n",
    "            return datetime.combine(most_common_date, datetime.min.time()).date()\n",
    "        except requests.RequestException as e:\n",
    "            # Etapa 7: Em caso de qualquer erro na consulta da API, registrar o erro e retornar None.\n",
    "            self.logger.error(f\"Failed to get most common date. Error: {e}\")\n",
    "            return None\n",
    "\n",
    "    def lista_urls_receita(self, *prefixes):\n",
    "        \"\"\"\n",
    "        Gera uma lista de URLs para download com base nos prefixos fornecidos. As URLs são \n",
    "        determinadas com base nos prefixos de arquivo conhecidos. Se nenhum prefixo for \n",
    "        fornecido, o método gerará URLs para todos os tipos de arquivos conhecidos.\n",
    "\n",
    "        Args:\n",
    "        *prefixes (str): Prefixos de arquivos para os quais as URLs serão geradas.\n",
    "\n",
    "        Returns:\n",
    "        list: Lista de URLs completas para os arquivos correspondentes aos prefixos.\n",
    "        \"\"\"\n",
    "        # Etapa 1: Inicializar a lista vazia para armazenar as URLs.\n",
    "        urls = []\n",
    "\n",
    "        # Etapa 2: Se nenhum prefixo for fornecido como argumento, use todos os prefixos de arquivo conhecidos.\n",
    "        if not prefixes:\n",
    "            prefixes = self.FILE_PREFIXES\n",
    "\n",
    "        # Etapa 3: Iterar sobre cada prefixo fornecido.\n",
    "        for prefix in prefixes:\n",
    "            # Etapa 3.1: Se o prefixo estiver na lista de prefixos especificados, \n",
    "            # adicione a URL correspondente à lista de URLs.\n",
    "            if prefix in ['Municipios', 'Cnaes', 'Naturezas', 'Simples','Qualificacoes','Paises','Motivos']:\n",
    "                urls.append(f\"{self.BASE_URL}/{prefix}.zip\")\n",
    "\n",
    "            # Etapa 3.2: Se o prefixo estiver na lista de prefixos de arquivo conhecidos, \n",
    "            # gere URLs para cada arquivo (de 0 a 9) e adicione-as à lista de URLs.\n",
    "            elif prefix in self.FILE_PREFIXES:\n",
    "                urls.extend([f\"{self.BASE_URL}/{prefix}{i}.zip\" for i in range(10)])\n",
    "\n",
    "            # Etapa 3.3: Se o prefixo fornecido não for reconhecido, \n",
    "            # imprima uma mensagem informando que o prefixo não é reconhecido.\n",
    "            else:\n",
    "                print(f\"Prefixo '{prefix}' não reconhecido!\")\n",
    "\n",
    "        # Etapa 4: Retorne a lista completa de URLs geradas.\n",
    "        return urls\n",
    "\n",
    "    def fetch_data(self, url, save_path, log_accumulator=None, max_attempts=15, wait_time=180):\n",
    "        \"\"\"\n",
    "        Baixa um arquivo da URL especificada e o salva no caminho de saída especificado. Em caso de \n",
    "        falha na tentativa de download, tentará novamente até o número máximo de tentativas ser atingido.\n",
    "\n",
    "        Args:\n",
    "        url (str): URL de onde o arquivo será baixado.\n",
    "        save_path (str): Caminho onde o arquivo baixado será salvo.\n",
    "        log_accumulator (list, optional): Um acumulador para armazenar mensagens de log. Padrão para None.\n",
    "        max_attempts (int, optional): Número máximo de tentativas de download. Padrão para 15.\n",
    "        wait_time (int, optional): Tempo de espera entre tentativas em segundos. Padrão para 150.\n",
    "\n",
    "        Raises:\n",
    "        Exception: Se o número máximo de tentativas for atingido sem sucesso.\n",
    "\n",
    "        Returns:\n",
    "        str: Caminho completo do arquivo baixado.\n",
    "        \"\"\"\n",
    "\n",
    "        # Etapa 1: Criar o diretório no caminho de salvamento, caso ele não exista.\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        # Etapa 2: Derivar o nome do arquivo da URL fornecida.\n",
    "        file_name = url.split('/')[-1]\n",
    "        file_path = os.path.join(save_path, file_name)\n",
    "\n",
    "        # Etapa 3: Definir os cabeçalhos da solicitação.\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "\n",
    "        # Etapa 4: Iniciar tentativas de download do arquivo.\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                # Etapa 4.1: Fazer uma solicitação GET para a URL.\n",
    "                response = requests.get(url, headers=headers)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                # Etapa 4.2: Escrever o conteúdo da resposta no arquivo.\n",
    "                with open(file_path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "                return file_path\n",
    "\n",
    "            except requests.RequestException as e:\n",
    "                # Etapa 4.3: Registrar falha na tentativa de download.\n",
    "                msg = f\"Tentativa {attempt + 1} de {max_attempts} falhou. Erro: {e}\"\n",
    "                if log_accumulator:\n",
    "                    log_accumulator.append([msg, \"ERRO NO REQUEST! RETRY SENDO FEITO\"])\n",
    "\n",
    "                # Etapa 4.4: Se for a última tentativa, registrar o erro persistente e lançar uma exceção.\n",
    "                if attempt == max_attempts - 1:\n",
    "                    if log_accumulator:\n",
    "                        log_accumulator.append([\"ERRO PERSISTENTE AO TENTAR BAIXAR O ARQUIVO\"])\n",
    "                    raise\n",
    "\n",
    "                # Etapa 4.5: Aguardar o tempo especificado antes de tentar novamente.\n",
    "                time.sleep(wait_time)\n",
    "\n",
    "                \n",
    "    def download_and_unzip(self, url, save_base_path=\"./temp\", output_base_path=\"./output\", headers=None, log_accumulator=None, data_update=True):\n",
    "        \"\"\"\n",
    "        Baixa um arquivo zip da URL fornecida, descompacta e salva no caminho especificado. Se os \n",
    "        dados da Receita Federal não foram atualizados nos últimos 30 dias e o parâmetro data_update \n",
    "        estiver ativado, uma mensagem de log será gerada.\n",
    "\n",
    "        Args:\n",
    "        url (str): URL de onde o arquivo zip será baixado.\n",
    "        save_base_path (str, optional): Caminho base onde o arquivo baixado será salvo. Padrão para \"./temp\".\n",
    "        output_base_path (str, optional): Caminho base onde os dados descompactados serão armazenados. Padrão para \"./output\".\n",
    "        headers (dict, optional): Cabeçalhos HTTP para serem usados no pedido. Padrão para None.\n",
    "        log_accumulator (list, optional): Uma acumulador para armazenar mensagens de log. Padrão para None.\n",
    "        data_update (bool, optional): Determina se a função deve verificar se os dados foram atualizados nos últimos 30 dias. Padrão para True.\n",
    "\n",
    "        Returns:\n",
    "        tuple: Um tuple contendo a URL e uma mensagem indicando \"Success\" ou a razão da falha.\n",
    "        \"\"\"\n",
    "\n",
    "        # Etapa 1: Identificar o nome do arquivo e seu prefixo a partir da URL fornecida.\n",
    "        file_name = url.split('/')[-1]\n",
    "        prefix = next((p for p in self.FILE_PREFIXES if file_name.startswith(p)), None)\n",
    "        if not prefix:\n",
    "            return (url, \"Failed to determine prefix\")\n",
    "\n",
    "        # Etapa 2: Configurar o caminho de salvamento e garantir que o diretório de salvamento exista.\n",
    "        save_path = os.path.join(save_base_path)\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        # Etapa 3: Obter a data mais comum de atualização dos arquivos da API e verificar se é válida.\n",
    "        data_atualizacao = self.get_most_common_date()\n",
    "        if data_atualizacao is None:\n",
    "            return (url, \"Failed to determine most common date\")\n",
    "\n",
    "        # Etapa 4: Obter a data atual.\n",
    "        data_atual = self.get_last_month()\n",
    "\n",
    "        # Etapa 5: Verificar se os dados da Receita Federal foram atualizados nos últimos 30 dias.\n",
    "        if not data_update or (data_update and (data_atual - data_atualizacao).days <= 30):\n",
    "            try:\n",
    "                # Etapa 5.1: Tentar baixar o arquivo zip da URL fornecida.\n",
    "                zip_file_path = self.fetch_data(url, save_path, headers)\n",
    "\n",
    "                # Etapa 5.2: Configurar o caminho de saída e garantir que o diretório de saída exista.\n",
    "                output_path = os.path.join(output_base_path)\n",
    "                os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "                # Etapa 5.3: Tentar descompactar o arquivo baixado.\n",
    "                self.unzip_files(zip_file_path, output_path)\n",
    "\n",
    "                # Etapa 5.4: Registrar a conclusão bem-sucedida da descompactação.\n",
    "                self.logger.info(f\"File {url} has been unzipped successfully to {output_path}\")\n",
    "                return (url, \"Success\")\n",
    "            except Exception as e:\n",
    "                # Etapa 5.5: Em caso de qualquer erro, registrar o erro e retornar a mensagem.\n",
    "                self.logger.error(f\"Failed to download and unzip {url}. Error: {e}\")\n",
    "                return (url, str(e))\n",
    "        else:\n",
    "            # Etapa 6: Se os dados não foram atualizados nos últimos 30 dias e data_update está ativado, registrar uma mensagem.\n",
    "            log_msg = \"Os dados da receita federal não foram atualizados nos últimos 30 dias\"\n",
    "            if log_accumulator:\n",
    "                log_accumulator.add([log_msg])\n",
    "            self.logger.info(log_msg)\n",
    "            return (url, log_msg)\n",
    "\n",
    "    def unzip_files(self, zip_file_path, output_base_path, log_accumulator=None):\n",
    "        \"\"\"\n",
    "        Descompacta o arquivo fornecido e salva no caminho especificado. Substitui arquivos existentes.\n",
    "\n",
    "        Args:\n",
    "        - zip_file_path (str): Caminho completo do arquivo zip que precisa ser descompactado.\n",
    "        - output_base_path (str): Caminho base onde o arquivo descompactado deve ser salvo.\n",
    "        - log_accumulator (list, optional): Uma lista que pode ser fornecida para acumular mensagens de log, útil para rastrear erros ou informações. Se não for fornecido, apenas os logs padrão serão usados.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Etapa 1: Extraia o nome do arquivo zip da rota fornecida.\n",
    "        zip_file_name = os.path.basename(zip_file_path)\n",
    "\n",
    "        # Etapa 2: Determine o prefixo do arquivo com base nos prefixos conhecidos.\n",
    "        prefix = next((p for p in self.FILE_PREFIXES if zip_file_name.startswith(p)), None)\n",
    "        if not prefix:\n",
    "            msg = f\"File {zip_file_path} does not match expected patterns.\"\n",
    "\n",
    "            # Etapa 2.1: Se o prefixo não for encontrado, adicione uma mensagem de erro ao acumulador de log (se fornecido) e retorne.\n",
    "            if log_accumulator:\n",
    "                log_accumulator.append(msg)\n",
    "            self.logger.error(msg)\n",
    "            return\n",
    "\n",
    "        # Etapa 3: Configurar o caminho de saída e garantir que o diretório de saída exista.\n",
    "        output_path = os.path.join(output_base_path, prefix)\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        # Etapa 4: Abrir o arquivo zip.\n",
    "        self.logger.info(f\"Trying to unzip {zip_file_path}\")\n",
    "        with zipfile.ZipFile(zip_file_path, \"r\") as z:\n",
    "\n",
    "            # Etapa 4.1: Pegue o nome do primeiro arquivo dentro do arquivo zip.\n",
    "            file_inside_zip = z.namelist()[0]\n",
    "\n",
    "            # Etapa 4.2: Extraia a parte numérica do nome do arquivo zip para nomear corretamente o arquivo csv resultante.\n",
    "            number_in_zip = ''.join(filter(str.isdigit, zip_file_name))\n",
    "            final_file_path = os.path.join(output_path, f\"{prefix}{number_in_zip}.csv\")\n",
    "\n",
    "            # Etapa 4.3: Se o arquivo csv já existir no caminho de destino, exclua-o para garantir que o novo arquivo não seja sobreposto.\n",
    "            if os.path.exists(final_file_path):\n",
    "                os.remove(final_file_path)\n",
    "\n",
    "            # Etapa 4.4: Descompacte o conteúdo do arquivo zip diretamente para o caminho de saída desejado.\n",
    "            with z.open(file_inside_zip) as zf, open(final_file_path, 'wb') as f_out:\n",
    "                shutil.copyfileobj(zf, f_out)\n",
    "\n",
    "        # Etapa 5: Exclua o arquivo zip original após a extração.\n",
    "        os.remove(zip_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb7d6bf-8d1e-432a-8a6f-e5065b4e52ff",
   "metadata": {},
   "source": [
    "## Extrando de forma uni thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d97fbfb-abc8-4d6a-bb60-c0e4c212122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = ReceitaCNPJApi()\n",
    "urls = api.lista_urls_receita()\n",
    "urlz = urls[22:24]\n",
    "for url in urlz:\n",
    "    result = api.download_and_unzip(url)\n",
    "    print(f\"Result for {url}: {result[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d104995-a1f8-489c-9824-5c2de9d6066e",
   "metadata": {},
   "source": [
    "## Usando multi thread do python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425f4587-d39f-4fa3-86d8-8bddfa1bd64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = ReceitaCNPJApi()\n",
    "x = api.lista_urls_receita()\n",
    "urls = api.lista_urls_receita('Empresas')\n",
    "\n",
    "def process_url(url):\n",
    "    return api.download_and_unzip(url)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:  # Altere o `max_workers` conforme o número desejado de threads.\n",
    "    results = list(executor.map(process_url, urls))\n",
    "\n",
    "for url, result in zip(urls, results):\n",
    "    print(f\"Result for {url}: {result[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6a2d2f7d-3865-43a3-a6d6-9f5ed0c3cc12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Trying to unzip ./temp\\Empresas2.zip\n",
      "INFO:__main__:File https://dadosabertos.rfb.gov.br/CNPJ/Empresas2.zip has been unzipped successfully to ./output\n",
      "INFO:__main__:Trying to unzip ./temp\\Empresas0.zip\n",
      "INFO:__main__:File https://dadosabertos.rfb.gov.br/CNPJ/Empresas0.zip has been unzipped successfully to ./output\n",
      "INFO:__main__:Trying to unzip ./temp\\Empresas4.zip\n",
      "INFO:__main__:File https://dadosabertos.rfb.gov.br/CNPJ/Empresas4.zip has been unzipped successfully to ./output\n",
      "INFO:__main__:Trying to unzip ./temp\\Empresas5.zip\n",
      "INFO:__main__:File https://dadosabertos.rfb.gov.br/CNPJ/Empresas5.zip has been unzipped successfully to ./output\n",
      "INFO:__main__:Trying to unzip ./temp\\Empresas1.zip\n",
      "INFO:__main__:File https://dadosabertos.rfb.gov.br/CNPJ/Empresas1.zip has been unzipped successfully to ./output\n",
      "INFO:__main__:Trying to unzip ./temp\\Empresas7.zip\n",
      "INFO:__main__:File https://dadosabertos.rfb.gov.br/CNPJ/Empresas7.zip has been unzipped successfully to ./output\n",
      "INFO:__main__:Trying to unzip ./temp\\Empresas8.zip\n",
      "INFO:__main__:File https://dadosabertos.rfb.gov.br/CNPJ/Empresas8.zip has been unzipped successfully to ./output\n",
      "INFO:__main__:Trying to unzip ./temp\\Empresas9.zip\n",
      "INFO:__main__:File https://dadosabertos.rfb.gov.br/CNPJ/Empresas9.zip has been unzipped successfully to ./output\n",
      "INFO:__main__:Trying to unzip ./temp\\Empresas3.zip\n",
      "INFO:__main__:File https://dadosabertos.rfb.gov.br/CNPJ/Empresas3.zip has been unzipped successfully to ./output\n",
      "INFO:__main__:Trying to unzip ./temp\\Empresas6.zip\n",
      "INFO:__main__:File https://dadosabertos.rfb.gov.br/CNPJ/Empresas6.zip has been unzipped successfully to ./output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/Empresas0.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/Empresas1.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/Empresas2.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/Empresas3.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/Empresas4.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/Empresas5.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/Empresas6.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/Empresas7.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/Empresas8.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/Empresas9.zip: Success\n"
     ]
    }
   ],
   "source": [
    "api = ReceitaCNPJApi()\n",
    "x = api.lista_urls_receita()\n",
    "urls = api.lista_urls_receita('Empresas')\n",
    "\n",
    "def process_url(url):\n",
    "    return api.download_and_unzip(url)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:  # Altere o `max_workers` conforme o número desejado de threads.\n",
    "    results = list(executor.map(process_url, urls))\n",
    "\n",
    "for url, result in zip(urls, results):\n",
    "    print(f\"Result for {url}: {result[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "380903fe-f563-4b66-ae13-c380e8f220a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Trying to unzip ./temp\\Naturezas.zip\n",
      "INFO:__main__:File https://dadosabertos.rfb.gov.br/CNPJ/Naturezas.zip has been unzipped successfully to ./output\n",
      "INFO:__main__:Trying to unzip ./temp\\Cnaes.zip\n",
      "INFO:__main__:Trying to unzip ./temp\\Municipios.zip\n",
      "INFO:__main__:File https://dadosabertos.rfb.gov.br/CNPJ/Cnaes.zip has been unzipped successfully to ./output\n",
      "INFO:__main__:Trying to unzip ./temp\\Qualificacoes.zip\n",
      "INFO:__main__:File https://dadosabertos.rfb.gov.br/CNPJ/Qualificacoes.zip has been unzipped successfully to ./output\n",
      "INFO:__main__:File https://dadosabertos.rfb.gov.br/CNPJ/Municipios.zip has been unzipped successfully to ./output\n",
      "INFO:__main__:Trying to unzip ./temp\\Motivos.zip\n",
      "INFO:__main__:File https://dadosabertos.rfb.gov.br/CNPJ/Motivos.zip has been unzipped successfully to ./output\n",
      "INFO:__main__:Trying to unzip ./temp\\Paises.zip\n",
      "INFO:__main__:File https://dadosabertos.rfb.gov.br/CNPJ/Paises.zip has been unzipped successfully to ./output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/Municipios.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/Cnaes.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/Naturezas.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/Qualificacoes.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/Paises.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/Motivos.zip: Success\n"
     ]
    }
   ],
   "source": [
    "api = ReceitaCNPJApi()\n",
    "x = api.lista_urls_receita()\n",
    "urls = api.lista_urls_receita('Municipios', 'Cnaes', 'Naturezas', 'Qualificacoes', 'Paises','Motivos')\n",
    "\n",
    "def process_url(url):\n",
    "    return api.download_and_unzip(url)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:  # Altere o `max_workers` conforme o número desejado de threads.\n",
    "    results = list(executor.map(process_url, urls))\n",
    "\n",
    "for url, result in zip(urls, results):\n",
    "    print(f\"Result for {url}: {result[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "da4a0a20-9569-42ff-8976-3dd2f09d0b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Trying to unzip ./temp\\Simples.zip\n",
      "INFO:__main__:File https://dadosabertos.rfb.gov.br/CNPJ/Simples.zip has been unzipped successfully to ./output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/Simples.zip: Success\n"
     ]
    }
   ],
   "source": [
    "api = ReceitaCNPJApi()\n",
    "x = api.lista_urls_receita()\n",
    "urls = api.lista_urls_receita('Simples')\n",
    "\n",
    "def process_url(url):\n",
    "    return api.download_and_unzip(url)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:  # Altere o `max_workers` conforme o número desejado de threads.\n",
    "    results = list(executor.map(process_url, urls))\n",
    "\n",
    "for url, result in zip(urls, results):\n",
    "    print(f\"Result for {url}: {result[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80475928-6df4-452c-84c9-ff8168c462f0",
   "metadata": {},
   "source": [
    "## Paralelizando com Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f74f43-4e20-4150-9e83-b359bb308eb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "api = ReceitaCNPJApi()\n",
    "# Defina a função de download e descompactação\n",
    "def download_and_unzip_spark(url):\n",
    "    api = ReceitaCNPJApi()\n",
    "    return api.download_and_unzip(url)\n",
    "\n",
    "# Crie um RDD das URLs\n",
    "x = api.lista_urls_receita()\n",
    "urls = x[22:24]\n",
    "urls_rdd = spark.sparkContext.parallelize(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749f6875-0e88-4e3d-b81f-0549081666c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Faça o download e descompacte as URLs em paralelo usando o Spark\n",
    "results = urls_rdd.map(download_and_unzip_spark).collect()\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Result for {result[0]}: {result[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0a00b5-9c70-48e0-bc3d-9b19dc0ea4cb",
   "metadata": {},
   "source": [
    "# CLasse TL Receita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5006b3-853a-4cbb-9c32-7334b16c0085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import os\n",
    "import chardet\n",
    "import logging\n",
    "import glob\n",
    "import secrets\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import when, col, to_date, regexp_extract, length, regexp_replace, concat_ws, lit, upper, concat, udf\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ebeff66-3e18-4ac5-92d0-3b535a3d3e51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReceitaLT:\n",
    "    \"\"\"\n",
    "    A classe `ReceitaLT` facilita a manipulação e análise de dados da Receita Federal do Brasil.\n",
    "\n",
    "    Atributos:\n",
    "        spark (SparkSession): Sessão Spark para manipulação de dataframes.\n",
    "        logger (Logger): Logger para capturar e exibir logs.\n",
    "        \n",
    "    Atributos estáticos:\n",
    "        - estabelecimentos: Schema para dados de estabelecimentos.\n",
    "        - empresas: Schema para dados das empresas.\n",
    "        - municipios: Schema para municípios.\n",
    "        - cnaes: Schema para CNAEs.\n",
    "        - paises: Schema para países.\n",
    "        - qualificacoes: Schema para qualificações.\n",
    "        - socios: Schema para sócios.\n",
    "        - simples: Schema para opções do Simples Nacional.\n",
    "        - naturezas: Schema para naturezas jurídicas.\n",
    "        - motivos: Schema para motivos de situações cadastrais.\n",
    "        - dic_provedor: Dicionário para correção de nomes de provedores de email.\n",
    "        \n",
    "    Métodos:\n",
    "        detect_encoding(file_pattern_or_path, num_bytes=10000): Detecta a codificação do arquivo ou arquivos fornecidos.\n",
    "\n",
    "    Uso:\n",
    "        1. Instancie a classe com uma sessão Spark.\n",
    "        2. Utilize os schemas estáticos para leitura de arquivos.\n",
    "        3. Use o método `detect_encoding` para determinar a codificação de arquivos antes de lê-los.\n",
    "        \n",
    "    Exemplo:\n",
    "        from pyspark.sql import SparkSession\n",
    "        \n",
    "        spark_session = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "        receita_helper = ReceitaLT(spark_session)\n",
    "        encodings = receita_helper.detect_encoding(\"path/to/datafile.csv\")\n",
    "        df = spark_session.read.csv(\"path/to/datafile.csv\", schema=ReceitaLT.empresas, encoding=encodings[\"path/to/datafile.csv\"])\n",
    "    \"\"\"\n",
    "    def __init__(self, spark: SparkSession):\n",
    "        \"\"\"\n",
    "        Inicializa a classe ReceitaLT.\n",
    "        \n",
    "        Parâmetros:\n",
    "        spark (SparkSession): Uma sessão Spark ativa.\n",
    "        \"\"\"\n",
    "        self.spark = spark\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        logging.basicConfig(level=logging.INFO) \n",
    "        \n",
    "    # Definindo os schemas:\n",
    "    estabelecimentos = StructType([\n",
    "        StructField(\"CNPJ_BASICO\", StringType(), nullable=True),\n",
    "        StructField(\"CNPJ_ORDEM\", StringType(), nullable=True),\n",
    "        StructField(\"CNPJ_DV\", StringType(), nullable=True),\n",
    "        StructField(\"MATRIZ_FILIAL\", StringType(), nullable=True),\n",
    "        StructField(\"NOME_FANTASIA\", StringType(), nullable=True),\n",
    "        StructField(\"SIT_CADASTRAL\", IntegerType(), nullable=True),\n",
    "        StructField(\"DT_SIT_CADASTRAL\", StringType(), nullable=True),\n",
    "        StructField(\"MOTIVO_CADASTRAL\", StringType(), nullable=True),\n",
    "        StructField(\"NOME_CIDADE_EXTERIOR\", StringType(), nullable=True),\n",
    "        StructField(\"PAIS\", StringType(), nullable=True),\n",
    "        StructField(\"DT_INICIO_ATIVIDADE\", StringType(), nullable=True),\n",
    "        StructField(\"CNAE_1\", StringType(), nullable=True),\n",
    "        StructField(\"CNAE_2\", StringType(), nullable=True),\n",
    "        StructField(\"TIPO_LOUGRADOURO\", StringType(), nullable=True),\n",
    "        StructField(\"LOGRADOURO\", StringType(), nullable=True),\n",
    "        StructField(\"NUMERO\", IntegerType(), nullable=True),\n",
    "        StructField(\"COMPLEMENTO\", StringType(), nullable=True),\n",
    "        StructField(\"BAIRRO\", StringType(), nullable=True),\n",
    "        StructField(\"CEP\", IntegerType(), nullable=True),\n",
    "        StructField(\"UF\", StringType(), nullable=True),\n",
    "        StructField(\"MUNICIPIO\", StringType(), nullable=True),\n",
    "        StructField(\"DDD1\", StringType(), nullable=True),\n",
    "        StructField(\"TEL1\", StringType(), nullable=True),\n",
    "        StructField(\"DDD2\", StringType(), nullable=True),\n",
    "        StructField(\"TEL2\", StringType(), nullable=True),\n",
    "        StructField(\"DDD_FAX\", IntegerType(), nullable=True),\n",
    "        StructField(\"FAX\", IntegerType(), nullable=True),\n",
    "        StructField(\"EMAIL\", StringType(), nullable=True),\n",
    "        StructField(\"SIT_ESPECIAL\", StringType(), nullable=True),\n",
    "        StructField(\"DT_SIT_ESPECIAL\", StringType(), nullable=True)])\n",
    "\n",
    "    empresas = StructType([\n",
    "        StructField(\"CNPJ\", StringType(), nullable=True),\n",
    "        StructField(\"NOME_EMPRESA\", StringType(), nullable=True),\n",
    "        StructField(\"COD_NAT_JURICA\", StringType(), nullable=True),\n",
    "        StructField(\"QUALIF_RESPONVAVEL\", StringType(), nullable=True),\n",
    "        StructField(\"CAP_SOCIAL\", StringType(), nullable=True),\n",
    "        StructField(\"PORTE\", StringType(), nullable=True),\n",
    "        StructField(\"ENTE_FEDERATIVO\", StringType(), nullable=True)])\n",
    "\n",
    "    municipios = StructType([\n",
    "        StructField(\"ID_MUNICPIO\", StringType(), nullable=True),\n",
    "        StructField(\"MUNICIPIO\", StringType(), nullable=True)])\n",
    "\n",
    "    cnaes = StructType([\n",
    "        StructField(\"COD_CNAE\", StringType(), nullable=True),\n",
    "        StructField(\"CNAE\", StringType(), nullable=True)])\n",
    "    \n",
    "    paises = StructType([\n",
    "        StructField(\"COD_PAIS\", StringType(), nullable=True),\n",
    "        StructField(\"NM_PAIS\", StringType(), nullable=True)])\n",
    "    \n",
    "    qualificacoes = StructType([\n",
    "        StructField(\"COD_QUALIFICACAO\", StringType(), nullable=True),\n",
    "        StructField(\"NM_QUALIFICACAO\", StringType(), nullable=True)])\n",
    "\n",
    "    socios = StructType([\n",
    "        StructField(\"CNPJ_BASICO\", StringType(), nullable=True),\n",
    "        StructField(\"IDENTIFICADOR_SOCIO\", IntegerType(), nullable=True),\n",
    "        StructField(\"NOME_SOCIO_RAZAO_SOCIAL\", StringType(), nullable=True),\n",
    "        StructField(\"CNPJ_CPF_SOCIO\", StringType(), nullable=True),\n",
    "        StructField(\"QUALIFICAÇAO_SOCIO\", StringType(), nullable=True),\n",
    "        StructField(\"DATA_ENTRADA_SOCIEDADE\", StringType(), nullable=True),\n",
    "        StructField(\"PAIS\", StringType(), nullable=True),\n",
    "        StructField(\"REPRESENTANTE_LEGAL\", StringType(), nullable=True),\n",
    "        StructField(\"NOME_REPRESENTANTE\", StringType(), nullable=True),\n",
    "        StructField(\"QUALIFICACAO_REPRESENTANTE_LEGAL\", StringType(), nullable=True),\n",
    "        StructField(\"FAIXA_ETARIA\", StringType(), nullable=True)])\n",
    "\n",
    "    simples = StructType([\n",
    "        StructField(\"CNPJ_BASICO\", StringType(), nullable=True),\n",
    "        StructField(\"OPCAO_PELO_SIMPLES\", StringType(), nullable=True),\n",
    "        StructField(\"DATA_OPCAO_PELO_SIMPLES\", StringType(), nullable=True),\n",
    "        StructField(\"DATA_EXCLUSAO_SIMPLES\", StringType(), nullable=True),\n",
    "        StructField(\"OPÇAO_PELO_MEI\", StringType(), nullable=True),\n",
    "        StructField(\"DATA_OPCAO_PELO_MEI\", StringType(), nullable=True),\n",
    "        StructField(\"DATA_EXCLUSAO_MEI\", StringType(), nullable=True)])\n",
    "\n",
    "    naturezas = StructType([\n",
    "        StructField(\"COD_NAT_JURICA\", StringType(), nullable=True),\n",
    "        StructField(\"NAT_JURICA\", StringType(), nullable=True)])\n",
    "    \n",
    "    motivos = StructType([\n",
    "        StructField(\"COD_MOTIVO\", StringType(), nullable=True),\n",
    "        StructField(\"NM_MOTIVO\", StringType(), nullable=True)])\n",
    "    \n",
    "    dic_provedor = {'0UTLOOK': 'OUTLOOK', '123GMAIL': 'GMAIL', '12GMAIL': 'GMAIL', '19GMAIL': 'GMAIL', '1HOTMAIL': 'HOTMAIL', \n",
    "                    '2010HOTMAIL': 'HOTMAIL', '20GMAIL': 'GMAIL', '23GMAIL': 'GMAIL', '2GMAIL': 'GMAIL', '2HOTMAIL': 'HOTMAIL', \n",
    "                    '30GMAIL': 'GMAIL', '7GMAIL': 'GMAIL', 'ADV': 'ADV', 'AGMAIL': 'GMAIL', 'AHOO': 'YAHOO', 'AIL': 'AOL', 'ALUNO': 'ALUNO', \n",
    "                    'AOL': 'AOL', 'AUTLOOK': 'OUTLOOK', 'BB': 'BB', 'BOL': 'BOL', 'BOLL': 'BOL', 'BOOL': 'BOL', 'BRTURBO': 'OI', \n",
    "                    'CAIXA': 'CAIXA', 'CLICK21': 'CLICK21', 'CLOUD': 'ICLOUD', 'CRECI': 'CRECI', 'EDU': 'EDU', 'EMAIL': 'EMAIL', \n",
    "                    'FACEBOOK': 'FACEBOOK', 'FMAIL': 'GMAIL', 'G': 'GMAIL', 'G-MAIL': 'GMAIL', 'GAIL': 'GMAIL', 'GAMAIL': 'GMAIL', \n",
    "                    'GAMEIL': 'GMAIL', 'GAMIAL': 'GMAIL', 'GAMIL': 'GMAIL', 'GEMAIL': 'GMAIL', 'GGMAIL': 'GMAIL', 'GHMAIL': 'GMAIL', \n",
    "                    'GHOTMAIL': 'HOTMAIL', 'GIMAIL': 'GMAIL', 'GLOBO': 'GLOBO', 'GLOBOMAIL': 'LWMAIL', 'GMA': 'GMAIL', 'GMAAIL': 'GMAIL', \n",
    "                    'GMAI': 'GMAIL', 'GMAIAL': 'GMAIL', 'GMAII': 'GMAIL', 'GMAIIL': 'GMAIL', 'GMAIK': 'GMAIL', 'GMAIL': 'GMAIL', \n",
    "                    'GMAILC': 'GMAIL', 'GMAILGMAIL': 'GMAIL', 'GMAILL': 'GMAIL', 'GMAILMAIL': 'GMAIL', 'GMAILO': 'GMAIL', 'GMAIM': 'GMAIL', \n",
    "                    'GMAIO': 'GMAIL', 'GMAIOL': 'GMAIL', 'GMAIS': 'GMAIL', 'GMAISL': 'GMAIL', 'GMAIUL': 'GMAIL', 'GMAL': 'GMAIL', \n",
    "                    'GMALI': 'GMAIL', 'GMAOL': 'GMAIL', 'GMAQIL': 'GMAIL', 'GMASIL': 'GMAIL', 'GMAUIL': 'GMAIL', 'GMAUL': 'GMAIL',\n",
    "                    'GMEIL': 'GMAIL', 'GMIAL': 'GMAIL', 'GMIL': 'GMAIL', 'GML': 'GMAIL', 'GMMAIL': 'GMAIL', 'GMNAIL': 'GMAIL', \n",
    "                    'GMQIL': 'GMAIL', 'GMSIL': 'GMAIL', 'GNAIL': 'GMAIL', 'GNMAIL': 'GMAIL', 'GOMAIL': 'GMAIL', 'GOOGLEMAIL': 'GMAIL',\n",
    "                    'GOTMAIL': 'HOTMAIL', 'GTMAIL': 'GMAIL', 'H0TMAIL': 'HOTMAIL', 'HAHOO': 'YAHOO', 'HATMAIL': 'HOTMAIL', 'HAYOO': 'YAHOO', \n",
    "                    'HGMAIL': 'GMAIL', 'HHOTMAIL': 'HOTMAIL', 'HIOTMAIL': 'HOTMAIL', 'HITMAIL': 'HOTMAIL', 'HJOTMAIL': 'HOTMAIL', \n",
    "                    'HMAIL': 'HOTMAIL', 'HOITMAIL': 'HOTMAIL', 'HOLMAIL': 'HOTMAIL', 'HOLTMAIL': 'HOTMAIL', 'HOMAIL': 'HOTMAIL', \n",
    "                    'HOMTAIL': 'HOTMAIL', 'HOMTIAL': 'HOTMAIL', 'HOMTMAIL': 'HOTMAIL', 'HOOTMAIL': 'HOTMAIL', 'HOPTMAIL': 'HOTMAIL', \n",
    "                    'HORMAIL': 'HOTMAIL', 'HORTMAIL': 'HOTMAIL', 'HOT': 'HOTMAIL', 'HOTAIL': 'HOTMAIL', 'HOTAMAIL': 'HOTMAIL', \n",
    "                    'HOTAMIL': 'HOTMAIL', 'HOTEMAIL': 'HOTMAIL', 'HOTGMAIL': 'HOTMAIL', 'HOTIMAIL': 'HOTMAIL', 'HOTIMAL': 'HOTMAIL', \n",
    "                    'HOTLMAIL': 'HOTMAIL', 'HOTLOOK': 'OUTLOOK', 'HOTMA': 'HOTMAIL', 'HOTMAAIL': 'HOTMAIL', 'HOTMAI': 'HOTMAIL', \n",
    "                    'HOTMAIAL': 'HOTMAIL', 'HOTMAII': 'HOTMAIL', 'HOTMAIIL': 'HOTMAIL', 'HOTMAIL': 'HOTMAIL', 'HOTMAILC': 'HOTMAIL', \n",
    "                    'HOTMAILL': 'HOTMAIL', 'HOTMAILO': 'HOTMAIL', 'HOTMAIM': 'HOTMAIL', 'HOTMAIO': 'HOTMAIL', 'HOTMAIOL': 'HOTMAIL', \n",
    "                    'HOTMAIUL': 'HOTMAIL', 'HOTMAL': 'HOTMAIL', 'HOTMALI': 'HOTMAIL', 'HOTMAMIL': 'HOTMAIL', 'HOTMAOL': 'HOTMAIL', \n",
    "                    'HOTMAQIL': 'HOTMAIL', 'HOTMASIL': 'HOTMAIL', 'HOTMAUIL': 'HOTMAIL', 'HOTMAUL': 'HOTMAIL', 'HOTMEIL': 'HOTMAIL', \n",
    "                    'HOTMIAIL': 'HOTMAIL', 'HOTMIAL': 'HOTMAIL', 'HOTMIL': 'HOTMAIL', 'HOTMMAIL': 'HOTMAIL', 'HOTMNAIL': 'HOTMAIL',\n",
    "                    'HOTMQIL': 'HOTMAIL', 'HOTMSIL': 'HOTMAIL', 'HOTNAIL': 'HOTMAIL', 'HOTOMAIL': 'HOTMAIL', 'HOTRMAIL': 'HOTMAIL', \n",
    "                    'HOTTMAIL': 'HOTMAIL', 'HOTYMAIL': 'HOTMAIL', 'HOUTLOOK': 'OUTLOOK', 'HOYMAIL': 'HOTMAIL', 'HPTMAIL': 'HOTMAIL', \n",
    "                    'HTMAIL': 'HOTMAIL', 'HTOMAIL': 'HOTMAIL', 'HYAHOO': 'YAHOO', 'IAHOO': 'YAHOO', 'IBEST': 'IBEST', 'ICLAUD': 'ICLOUD', \n",
    "                    'ICLOD': 'ICLOUD', 'ICLOID': 'ICLOUD', 'ICLOOD': 'ICLOUD', 'ICLOU': 'ICLOUD', 'ICLOUD': 'ICLOUD', 'ICLOUDE': 'ICLOUD', \n",
    "                    'ICLOULD': 'ICLOUD', 'ICLOUND': 'ICLOUD', 'ICLUD': 'ICLOUD', 'ICLUOD': 'ICLOUD', 'ICOUD': 'ICLOUD', 'ICOULD': 'ICLOUD', \n",
    "                    'ID': 'IG', 'IG': 'IG', 'IGMAIL': 'GMAIL', 'IGUI': 'IG', 'IMAIL': 'GMAIL', 'INCLOUD': 'ICLOUD', 'ITELEFONICA': 'ITELEFONICA',\n",
    "                    'JMAIL': 'GMAIL', 'JOTMAIL': 'HOTMAIL', 'LIVE': 'LIVE', 'LWMAIL': 'LWMAIL', 'MAIL': 'MAIL', 'ME': 'ME', 'MSM': 'MSN', \n",
    "                    'MSN': 'MSN', 'NETSITE': 'NETSITE', 'OI': 'OI', 'OIMAIL': 'HOTMAIL', 'OITLOOK': 'OUTLOOK', 'OLTLOOK': 'OUTLOOK', \n",
    "                    'OOUTLOOK': 'OUTLOOK', 'OTLOOK': 'OUTLOOK', 'OTMAIL': 'HOTMAIL', 'OUL': 'UOL', 'OULOOK': 'OUTLOOK', 'OULTLOOK': 'OUTLOOK',\n",
    "                    'OULTOOK': 'OUTLOOK', 'OUTILOOK': 'OUTLOOK', 'OUTIOOK': 'OUTLOOK', 'OUTLLOK': 'OUTLOOK', 'OUTLLOOK': 'OUTLOOK', \n",
    "                    'OUTLOCK': 'OUTLOOK', 'OUTLOK': 'OUTLOOK', 'OUTLOKK': 'OUTLOOK', 'OUTLOOCK': 'OUTLOOK', 'OUTLOOK': 'OUTLOOK', \n",
    "                    'OUTLOOKL': 'OUTLOOK', 'OUTLOOL': 'OUTLOOK', 'OUTLOOOK': 'OUTLOOK', 'OUTLUK': 'OUTLOOK', 'OUTOLOOK': 'OUTLOOK',\n",
    "                    'OUTOOK': 'OUTLOOK', 'OUTOOLK': 'OUTLOOK', 'OUTTLOOK': 'OUTLOOK', 'OUTULOOK': 'OUTLOOK', 'POP': 'POP',\n",
    "                    'PROTON': 'PROTONMAIL', 'PROTONMAIL': 'PROTONMAIL', 'PUTLOOK': 'OUTLOOK', 'R7': 'R7', 'ROCKETMAIL': 'ROCKETMAIL', \n",
    "                    'ROCKTMAIL': 'ROCKETMAIL', 'ROTMAIL': 'HOTMAIL', 'SERCOMTEL': 'SERCOMTEL', 'SETELAGOASGML': 'GMAIL', \n",
    "                    'SUPERIG': 'SUPERIG', 'TAHOO': 'YAHOO', 'TERRA': 'TERRA', 'TERRRA': 'TERRA', 'TMAIL': 'GMAIL', \n",
    "                    'TVGLOBO': 'GLOBO', 'UAHOO': 'YAHOO', 'UAI': 'UAI', 'UFV': 'UFV', 'UNESP': 'UNESP', 'UNOCHAPECO': 'UNOCHAPECO', \n",
    "                    'UO': 'UOL', 'UOL': 'UOL', 'UOTLOOK': 'OUTLOOK', 'UPF': 'UPF', 'USP': 'USP', 'UTLOOK': 'OUTLOOK', 'VELOXMAIL': 'VELOXMAIL',\n",
    "                    'WINDOWSLIVE': 'WINDOWSLIVE', 'YAAHOO': 'YAHOO', 'YAGOO': 'YAHOO', 'YAHAOO': 'YAHOO', 'YAHHO': 'YAHOO', 'YAHHOO': 'YAHOO', \n",
    "                    'YAHO': 'YAHOO', 'YAHOO': 'YAHOO', 'YAHOOCOM': 'YAHOO', 'YAHOOL': 'YAHOO', 'YAHOOO': 'YAHOO', 'YAHOOU': 'YAHOO', \n",
    "                    'YANHOO': 'YAHOO', 'YAOO': 'YAHOO', 'YAOOL': 'YAHOO', 'YAROO': 'YAHOO', 'YHAOO': 'YAHOO', 'YHOO': 'YAHOO', 'YMAIL': 'YMAIL', \n",
    "                    'YOHOO': 'YAHOO', 'YOPMAIL': 'HOTMAIL', 'ZIPMAIL': 'ZIPMAIL', '_HOTMAIL': 'HOTMAIL',     'GMAUL': 'GMAIL','GMALE': 'GMAIL', \n",
    "                    'GMAILE': 'GMAIL', 'GMILE': 'GMAIL', 'HOTMEL': 'HOTMAIL', 'HOTMELL': 'HOTMAIL','HOTMEAL': 'HOTMAIL', 'OUTLOKES': 'OUTLOOK', \n",
    "                    'OTLOOKS': 'OUTLOOK', 'YAHU': 'YAHOO', 'YOHU': 'YAHOO', 'YAHUO': 'YAHOO', 'YAHEE': 'YAHOO', 'UOLL': 'UOL',\n",
    "                    'UOOL': 'UOL', 'UULL': 'UOL', 'ICLODUE': 'ICLOUD', 'ICLAWD': 'ICLOUD', 'ROCKEDMAIL': 'ROCKETMAIL', 'ROKETMAIL': 'ROCKETMAIL',\n",
    "                    'OUTLOKE': 'OUTLOOK', 'OUTLOOCKE': 'OUTLOOK', 'YAAHO': 'YAHOO', 'YAHOOE': 'YAHOO', 'YAHUE': 'YAHOO', 'HOTMILE': 'HOTMAIL', 'HOTMELE': 'HOTMAIL',\n",
    "                    'FACEBOKE': 'FACEBOOK', 'FACBOOK': 'FACEBOOK', 'FCEBOOK': 'FACEBOOK', 'BOLL': 'BOL', 'BOLLE': 'BOL', 'BULE': 'BOL', 'GLOBOE': 'GLOBO',\n",
    "                    'GLOBU': 'GLOBO', 'GMILE': 'GMAIL', 'MSNE': 'MSN', 'MSNN': 'MSN', 'ICLOOUD': 'ICLOUD', 'OUTLUKE': 'OUTLOOK', 'OUTLLOKE': 'OUTLOOK',\n",
    "                    'GMAUL': 'GMAIL','GMALE': 'GMAIL', 'GMAILE': 'GMAIL', 'GMILE': 'GMAIL', 'HOTMEL': 'HOTMAIL', 'HOTMELL': 'HOTMAIL',\n",
    "                    'HOTMEAL': 'HOTMAIL', 'OUTLOKES': 'OUTLOOK', 'OTLOOKS': 'OUTLOOK', 'YAHU': 'YAHOO', 'YOHU': 'YAHOO', 'YAHUO': 'YAHOO', \n",
    "                    'YAHEE': 'YAHOO', 'UOLL': 'UOL', 'UOOL': 'UOL', 'UULL': 'UOL', 'ICLODUE': 'ICLOUD', 'ICLAWD': 'ICLOUD',  'ROCKEDMAIL': 'ROCKETMAIL',\n",
    "                    'ROKETMAIL': 'ROCKETMAIL', 'OUTLOKE': 'OUTLOOK', 'OUTLOOCKE': 'OUTLOOK', 'YAAHO': 'YAHOO', 'YAHOOE': 'YAHOO', 'YAHUE': 'YAHOO', \n",
    "                    'HOTMILE': 'HOTMAIL', 'HOTMELE': 'HOTMAIL', 'FACEBOKE': 'FACEBOOK', 'FACBOOK': 'FACEBOOK', 'FCEBOOK': 'FACEBOOK', 'BOLL': 'BOL',\n",
    "                    'BOLLE': 'BOL', 'BULE': 'BOL', 'GLOBOE': 'GLOBO',  'GLOBU': 'GLOBO', 'GMILE': 'GMAIL', 'MSNE': 'MSN', 'MSNN': 'MSN', 'ICLOOUD': 'ICLOUD',\n",
    "                    'OUTLUKE': 'OUTLOOK', 'OUTLLOKE': 'OUTLOOK', 'PROTONMIAL': 'PROTONMAIL',  'PROTONMALE': 'PROTONMAIL', 'PROTOMAIL': 'PROTONMAIL', \n",
    "                    'OULOOKCOM': 'OUTLOOK', 'YAHCOM': 'YAHOO',  'YAHOCOM': 'YAHOO','GAMILCOM': 'GMAIL', 'GMALCOM': 'GMAIL',  'HOTMALCOM': 'HOTMAIL',  \n",
    "                    'HOTMILCOM': 'HOTMAIL', 'HOTMELCOM': 'HOTMAIL', 'ROCKMAIL': 'ROCKETMAIL', 'ROKMAIL': 'ROCKETMAIL', 'TERA': 'TERRA', 'TEERA': 'TERRA', \n",
    "                    'FACBOOKCOM': 'FACEBOOK', 'FACEBOOKCOM': 'FACEBOOK', 'ICLOWD': 'ICLOUD', 'ICLOUND': 'ICLOUD', 'UOOLCOM': 'UOL', 'UOLLCOM': 'UOL', \n",
    "                    'UOLCOMBR': 'UOL','LIVECOM': 'LIVE', 'LIVECOMBR': 'LIVE', 'GMAICOM': 'GMAIL',  'GMAILCOMBR': 'GMAIL',  'YAHOOBR': 'YAHOO', \n",
    "                    'YAHOOOCOMBR': 'YAHOO', 'YAHOOOCOM': 'YAHOO', 'ZIPMAILE': 'ZIPMAIL', 'ZIPMAILL': 'ZIPMAIL',  'IBESTT': 'IBEST', 'IBESTE': 'IBEST'}\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def detect_encoding(file_pattern_or_path, num_bytes=10000):\n",
    "        \"\"\"\n",
    "        Detecta a codificação do arquivo ou arquivos fornecidos.\n",
    "        \n",
    "        Parâmetros:\n",
    "            file_pattern_or_path (str): Caminho ou padrão do arquivo para detecção.\n",
    "            num_bytes (int, opcional): Número de bytes para ler para a detecção. Padrão é 10000.\n",
    "        \n",
    "        Retorna:\n",
    "            dict: Dicionário com caminho do arquivo como chave e codificação detectada como valor.\n",
    "        \"\"\"\n",
    "        files = glob.glob(file_pattern_or_path)\n",
    "        encodings = {}\n",
    "        for file_path in files:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                rawdata = f.read(num_bytes)\n",
    "                encodings[file_path] = chardet.detect(rawdata)[\"encoding\"]\n",
    "        return encodings\n",
    "\n",
    "\n",
    "    def read_data(self, schema_name, base_path=None):\n",
    "        \"\"\"\n",
    "        Lê dados de vários arquivos CSV de acordo com o esquema e caminho base fornecidos, consolidando-os \n",
    "        em um único DataFrame do Spark.\n",
    "\n",
    "        Parâmetros:\n",
    "            schema_name (str): Nome do esquema a ser usado para a leitura dos arquivos.\n",
    "                               Deve ser uma das chaves do dicionário `schemas`.\n",
    "\n",
    "            base_path (str, opcional): Caminho base dos arquivos CSV.\n",
    "                                       Se não for fornecido, ele tentará buscar da variável de ambiente 'BASE_PATH'.\n",
    "                                       Caso não encontre, o padrão \"./output\" será utilizado.\n",
    "\n",
    "        Retorna:\n",
    "            DataFrame: DataFrame do Spark contendo os dados consolidados dos arquivos CSV.\n",
    "\n",
    "        Exceções:\n",
    "            Pode lançar uma exceção se o arquivo não estiver presente no caminho especificado ou\n",
    "            se houver problemas de codificação ao ler o arquivo.\n",
    "\n",
    "        Exemplo:\n",
    "            receita_helper = ReceitaLT(spark_session)\n",
    "            df = receita_helper.read_data(\"estabelecimentos\", \"/path/to/csv/files\")\n",
    "\n",
    "        Notas:\n",
    "            - A função primeiro detecta a codificação dos arquivos antes de lê-los para garantir que \n",
    "              eles sejam lidos corretamente.\n",
    "            - A função lida com múltiplos arquivos CSV e os une em um único DataFrame.\n",
    "            - O formato de arquivo assumido é CSV com delimitador \";\", sem cabeçalho e com aspas para delimitar campos.\n",
    "        \"\"\"\n",
    "        schemas = {\n",
    "            \"estabelecimentos\": self.estabelecimentos,\n",
    "            \"empresas\": self.empresas,\n",
    "            \"municipios\": self.municipios,\n",
    "            \"cnaes\": self.cnaes,\n",
    "            \"socios\": self.socios,\n",
    "            \"simples\": self.simples,\n",
    "            \"naturezas\": self.naturezas,\n",
    "            \"qualificacoes\": self.qualificacoes,\n",
    "            \"motivos\": self.motivos,\n",
    "            \"paises\": self.paises}\n",
    "\n",
    "        # Se o base_path não for fornecido, pegar da variável de ambiente ou usar um padrão.\n",
    "        if not base_path:\n",
    "            base_path = os.environ.get('BASE_PATH', \"./output\")\n",
    "\n",
    "        if schema_name in ['estabelecimentos', 'empresas', 'socios']:\n",
    "            file_location_pattern = os.path.join(base_path, schema_name.capitalize(), '*.csv')\n",
    "        else:\n",
    "            file_location_pattern = os.path.join(base_path, schema_name.capitalize(), f\"{schema_name.capitalize()}.csv\")\n",
    "\n",
    "        # Detectar codificações\n",
    "        encodings = self.detect_encoding(file_location_pattern)\n",
    "        self.logger.info(f\"Detected encodings: {encodings}\")\n",
    "\n",
    "        # Agora, vamos ler cada arquivo com sua codificação correta e armazenar em uma lista de DataFrames\n",
    "        dfs = []\n",
    "        for file_location, encoding in encodings.items():\n",
    "            df = (self.spark.read.format(\"csv\")\n",
    "                  .option(\"sep\", \";\")\n",
    "                  .option(\"header\", \"false\")\n",
    "                  .option('quote', '\"')\n",
    "                  .option(\"escape\", '\"')\n",
    "                  .option(\"encoding\", encoding)\n",
    "                  .schema(schemas[schema_name])\n",
    "                  .load(file_location))\n",
    "            dfs.append(df)\n",
    "\n",
    "        # Unir todos os DataFrames em um único DataFrame\n",
    "        if dfs:\n",
    "            final_df = reduce(lambda a, b: a.union(b), dfs)\n",
    "        else:\n",
    "            final_df = self.spark.createDataFrame([], schemas[schema_name])\n",
    "\n",
    "        return final_df\n",
    "    \n",
    "        # Original function\n",
    "    def geocode_address(address):\n",
    "        \"\"\"\n",
    "        Geocodifica um endereço, convertendo-o em coordenadas de latitude e longitude.\n",
    "\n",
    "        Parâmetros:\n",
    "            address (str): Endereço a ser geocodificado.\n",
    "\n",
    "        Retorna:\n",
    "            tuple: Um par contendo a latitude e a longitude do endereço fornecido. \n",
    "                   Se o endereço não puder ser geocodificado, retorna (None, None).\n",
    "\n",
    "        Exemplo:\n",
    "            lat, lon = geocode_address(\"1600 Amphitheatre Parkway, Mountain View, CA\")\n",
    "\n",
    "        Notas:\n",
    "            - Usa o serviço Nominatim para a geocodificação.\n",
    "            - Incorpora um limitador de taxa para garantir que não excedamos os limites de requisições por segundo \n",
    "              do serviço.\n",
    "        \"\"\"\n",
    "        geolocator = Nominatim(user_agent=\"CNPJ_GEOLOCATION\")\n",
    "        geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n",
    "        location = geocode(address)\n",
    "        if location:\n",
    "            return (location.latitude, location.longitude)\n",
    "        else:\n",
    "            return (None, None)\n",
    "\n",
    "    # Define the UDF\n",
    "    schema = StructType([\n",
    "        StructField(\"latitude\", FloatType(), nullable=True),\n",
    "        StructField(\"longitude\", FloatType(), nullable=True)\n",
    "    ])\n",
    "\n",
    "    @udf(schema)\n",
    "    def geocode_udf(address):\n",
    "        \"\"\"\n",
    "        UDF do Spark para geocodificar um endereço dentro de um DataFrame.\n",
    "\n",
    "        Parâmetros:\n",
    "            address (str): Endereço a ser geocodificado.\n",
    "\n",
    "        Retorna:\n",
    "            dict: Dicionário contendo a 'latitude' e a 'longitude' do endereço fornecido.\n",
    "                  Se o endereço não puder ser geocodificado, os valores serão None.\n",
    "\n",
    "        Exemplo:\n",
    "            df.withColumn(\"location\", geocode_udf(df[\"address\"]))\n",
    "\n",
    "        Notas:\n",
    "            - Esta UDF encapsula a função `geocode_address`.\n",
    "            - Retorna um tipo de dado complexo (Struct) com dois campos: 'latitude' e 'longitude'.\n",
    "        \"\"\"\n",
    "        lat, lon = geocode_address(address)\n",
    "        return {\"latitude\": lat, \"longitude\": lon}\n",
    "    \n",
    "    def process_estabelecimentos(self, df):\n",
    "        \n",
    "        \"\"\"\n",
    "        Processa e enriquece o DataFrame de estabelecimentos com informações adicionais e transformações.\n",
    "\n",
    "        Parâmetros:\n",
    "            df (DataFrame): DataFrame inicial contendo informações de estabelecimentos.\n",
    "\n",
    "        Retorna:\n",
    "            DataFrame: DataFrame processado e enriquecido com novas colunas e informações.\n",
    "\n",
    "        Descrição:\n",
    "            - Lê dataframes adicionais relacionados a países, municípios, cnaes e motivos.\n",
    "            - Realiza renomeações de colunas para facilitar junções.\n",
    "            - Enriquece o dataframe com informações de motivos, cnaes, municípios e países.\n",
    "            - Processa colunas de e-mail, separando provedores e corrigindo valores.\n",
    "            - Converte colunas de data de string para formato de data.\n",
    "            - Deriva colunas de ano e mês a partir de datas.\n",
    "            - Processa e deriva novas colunas com base em mapeamentos para situação cadastral e tipo de estabelecimento.\n",
    "            - Valida endereços de e-mail usando expressões regulares.\n",
    "            - Combina informações de endereço para formar uma coluna completa de endereço.\n",
    "            - Utiliza a função de geocodificação para obter coordenadas com base no endereço e, em caso de falha, com base no CEP.\n",
    "            - Realiza correções na coluna de provedor de e-mail usando um dicionário de mapeamento.\n",
    "\n",
    "        Notas:\n",
    "            - Esta função faz uso intensivo das operações de DataFrame do PySpark.\n",
    "            - Dependências: A função depende de outras funções e UDFs, como 'geocode_udf', bem como de variáveis de instância, como 'dic_provedor'.\n",
    "        \"\"\"\n",
    "        \n",
    "        df_pais = self.read_data(schema_name='paises')\n",
    "        df_mun = self.read_data(schema_name='municipios')\n",
    "        df_cnaes = self.read_data(schema_name='cnaes')\n",
    "        df_motivos = self.read_data(schema_name='motivos')\n",
    "        \n",
    "        df = df.withColumnRenamed(\"CNAE_1\", \"COD_CNAE\")\n",
    "        df = df.withColumnRenamed(\"MUNICIPIO\", \"ID_MUNICPIO\")\n",
    "        df = df.withColumnRenamed(\"PAIS\", \"COD_PAIS\")\n",
    "        df = df.withColumnRenamed(\"MOTIVO_CADASTRAL\", \"COD_MOTIVO\") \n",
    "        \n",
    "\n",
    "        df = df.join(broadcast(df_motivos), \"COD_MOTIVO\", \"left\").drop(df.COD_MOTIVO)\n",
    "        df = df.join(broadcast(df_cnaes), \"COD_CNAE\", \"left\").drop(df.COD_CNAE)\n",
    "        df = df.join(broadcast(df_mun), \"ID_MUNICPIO\", \"left\").drop(df.ID_MUNICPIO)\n",
    "        df = df.join(broadcast(df_pais), \"COD_PAIS\", \"left\").drop(df.COD_PAIS)\n",
    "        \n",
    "        dic_provedor = self.dic_provedor\n",
    "        # Tratamento da coluna provedor\n",
    "        df = df.withColumn(\"PROVEDOR\",  regexp_extract(\"EMAIL\", \"(?<=@)[^.]+(?=\\\\.)\", 0))\n",
    "        \n",
    "        # Colocando em caixa alta o provedor\n",
    "        df = df.withColumn(\"PROVEDOR\", upper(col(\"PROVEDOR\")))\n",
    "        \n",
    "        # Colocando em caixa baixa o email\n",
    "        df = df.withColumn(\"EMAIL\", lower(col(\"EMAIL\")))\n",
    "\n",
    "        # Convertendo colunas de data\n",
    "        df = df.withColumn(\"DT_SIT_CADASTRAL\", to_date(col('DT_SIT_CADASTRAL'), \"yyyyMMdd\"))\n",
    "        df = df.withColumn(\"DT_INICIO_ATIVIDADE\", to_date(col('DT_INICIO_ATIVIDADE'), \"yyyyMMdd\"))\n",
    "        df = df.withColumn(\"DT_SIT_ESPECIAL\", to_date(col('DT_SIT_ESPECIAL'), \"yyyyMMdd\"))\n",
    "        \n",
    "        df = df.withColumn( \"ano_cadastro\", F.year('DT_INICIO_ATIVIDADE'))\n",
    "        df = df.withColumn( \"mes_cadastro\", F.month('DT_INICIO_ATIVIDADE'))\n",
    "        df = df.withColumn( \"ano_sit_cadastral\", F.year('DT_SIT_CADASTRAL'))\n",
    "        df = df.withColumn( \"mes_sit_cadastral\", F.month('DT_SIT_CADASTRAL'))\n",
    "        \n",
    "        # Defina o dicionário de mapeamento\n",
    "        mapping = {1: 'NULA',2: 'ATIVA',3: 'SUSPENSA',4: 'INAPTA',8: 'BAIXADA'}\n",
    "        \n",
    "        # Use a função 'when' para criar a nova coluna 'NM_SIT_CADASTRAL'\n",
    "        df = df.withColumn(\"NM_SIT_CADASTRAL\",\n",
    "                           when(df[\"SIT_CADASTRAL\"].isin(list(mapping.keys())), df[\"SIT_CADASTRAL\"]).otherwise(None))\n",
    "        \n",
    "        # Substitua os valores na nova coluna com base no dicionário de mapeamento\n",
    "        for key, value in mapping.items():\n",
    "            df = df.withColumn(\"NM_SIT_CADASTRAL\", when(df[\"SIT_CADASTRAL\"] == key, value).otherwise(df[\"NM_SIT_CADASTRAL\"]))\n",
    "            \n",
    "        # Use uma expressão regular para validar os endereços de e-mail\n",
    "        email_pattern = r'^\\S+@\\S+\\.\\S+$'  # Padrão simples de endereço de e-mail\n",
    "        \n",
    "        # Use a função 'regexp_extract' para extrair endereços de e-mail válidos\n",
    "        df = df.withColumn(\"valid_email\", regexp_extract(col(\"EMAIL\"), email_pattern, 0))\n",
    "        \n",
    "        # Defina o dicionário de mapeamento\n",
    "        mapping = {1: 'MATRIZ',2: 'FILIAL'}\n",
    "        \n",
    "        # Use a função 'when' para criar a nova coluna 'NM_MATRIZ_FILIAL'\n",
    "        df = df.withColumn(\"NM_MATRIZ_FILIAL\",\n",
    "                           when(df[\"MATRIZ_FILIAL\"].isin(list(mapping.keys())), df[\"MATRIZ_FILIAL\"]).otherwise(None))\n",
    "        \n",
    "        # Substitua os valores na nova coluna com base no dicionário de mapeamento\n",
    "        for key, value in mapping.items():\n",
    "            df = df.withColumn(\"NM_MATRIZ_FILIAL\", when(df[\"MATRIZ_FILIAL\"] == key, value).otherwise(df[\"NM_MATRIZ_FILIAL\"]))\n",
    "            \n",
    "        # Criando a nova coluna \"ENDERECO_COMPLETO\"\n",
    "        df = df.withColumn(\"ENDERECO_COMPLETO\",\n",
    "                           concat_ws(\", \",\n",
    "                                     concat(df[\"TIPO_LOUGRADOURO\"], lit(\" \"), df[\"LOGRADOURO\"]),\n",
    "                                     \"NUMERO\",concat_ws(\" - \", \"MUNICIPIO\", \"UF\")))\n",
    "        \n",
    "        # Adicione a lógica de geocodificação aqui\n",
    "        df = df.withColumn(\"COORDENADAS\", geocode_udf(df[\"ENDERECO_COMPLETO\"]))\n",
    "        \n",
    "        df = df.withColumn(\"COORDENADAS\",\n",
    "                           when((col(\"COORDENADAS.latitude\").isNull()) & (col(\"COORDENADAS.longitude\").isNull()),\n",
    "                                geocode_udf(df[\"CEP\"])).otherwise(col(\"COORDENADAS\")))\n",
    "        \n",
    "        # Correção da coluna provedor\n",
    "        df = df.replace(dic_provedor, subset=['PROVEDOR'])\n",
    "\n",
    "        # Transformação das keys e values do dicionário em lowercase\n",
    "        dic_prov_lower = {k.lower(): str(v).lower() for k, v in dic_provedor.items()}\n",
    "\n",
    "        # Correção dos provedores na coluna EMAIL\n",
    "        replace_expr = reduce(\n",
    "            lambda a, b: regexp_replace(a, rf\"\\b{b[0]}\\b\", b[1]),\n",
    "            dic_prov_lower.items(),\n",
    "            F.col(\"valid_email\"))\n",
    "\n",
    "        df = df.withColumn(\"valid_email\", replace_expr)\n",
    "        df = df.withColumnRenamed(\"valid_email\", \"VALILD_EMAIL\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def process_empresas(self, df):\n",
    "        \"\"\"\n",
    "        Processa e enriquece o DataFrame de empresas com informações adicionais e transformações.\n",
    "\n",
    "        Parâmetros:\n",
    "            df (DataFrame): DataFrame inicial contendo informações de empresas.\n",
    "\n",
    "        Retorna:\n",
    "            DataFrame: DataFrame processado e enriquecido com novas colunas e informações.\n",
    "\n",
    "        Descrição:\n",
    "            - Lê dataframes adicionais relacionados a naturezas jurídicas e qualificações.\n",
    "            - Realiza renomeação de colunas para facilitar junções.\n",
    "            - Enriquece o dataframe com informações de naturezas jurídicas e qualificações.\n",
    "            - Processa a coluna 'NOME_EMPRESA' para extrair informações potenciais de CPF.\n",
    "            - Deriva uma nova coluna baseada no porte da empresa, usando um mapeamento predefinido.\n",
    "            - Determina a probabilidade de um valor ser um CPF válido com base em seu comprimento.\n",
    "            - Criptografa possíveis valores de CPF usando AES e os armazena em uma nova coluna 'CPF_CRIPTOGRAFADO', enquanto remove a coluna original 'CPF'.\n",
    "\n",
    "        Notas:\n",
    "            - Esta função faz uso intensivo das operações de DataFrame do PySpark.\n",
    "            - O valor de criptografia (secret_key) é gerado dinamicamente a cada chamada da função. Portanto, cada execução resultará em valores de 'CPF_CRIPTOGRAFADO' diferentes para os mesmos CPFs.\n",
    "            - O método AES usado aqui é 'ECB', que não é considerado seguro para muitos casos de uso devido à falta de vetor de inicialização (IV). A utilização deste modo deve ser revista se a segurança for uma preocupação.\n",
    "        \"\"\"\n",
    "        df_nat = self.read_data(schema_name='naturezas')\n",
    "        df_qual = self.read_data(schema_name='qualificacoes')\n",
    "        \n",
    "        df = df.withColumnRenamed(\"QUALIF_RESPONVAVEL\", \"COD_QUALIFICACAO\")\n",
    "        \n",
    "        df = df.join(broadcast(df_nat), \"COD_NAT_JURICA\", \"left\").drop(df.COD_NAT_JURICA)\n",
    "        df = df.join(broadcast(df_qual), \"COD_QUALIFICACAO\", \"left\").drop(df.COD_QUALIFICACAO)\n",
    "               \n",
    "        df = df.withColumn(\"CPF\", regexp_replace(\"NOME_EMPRESA\", \"[^0-9]\", \"\"))\n",
    "        df = df.withColumn(\"CPF\", when(col(\"CPF\") == \"\", None).otherwise(col(\"CPF\")))\n",
    "        df = df.withColumn('CPF_LEN', length('CPF'))\n",
    "        # Defina o dicionário de mapeamento\n",
    "        mapping = {0: 'NÃO INFORMADO',1: 'MICRO EMPRESA',3: ' EMPRESA DE PEQUENO PORTE',5: 'DEMAIS',8: 'BAIXADA'}\n",
    "        # Use a função 'when' para criar a nova coluna 'NM_SIT_CADASTRAL'\n",
    "        df = df.withColumn(\"NM_PORTE\",\n",
    "                           when(df[\"PORTE\"].isin(list(mapping.keys())), df[\"PORTE\"]).otherwise(None))\n",
    "        \n",
    "        # Substitua os valores na nova coluna com base no dicionário de mapeamento\n",
    "        for key, value in mapping.items():\n",
    "            df = df.withColumn(\"NM_PORTE\", when(df[\"PORTE\"] == key, value).otherwise(df[\"NM_PORTE\"]))\n",
    "\n",
    "        df = df.withColumn(\"PROBABILIDADE_DE_SER_CPF\", when(df[\"CPF_LEN\"] == 11, \"SIM\").otherwise(\"NAO\"))\n",
    "        \n",
    "        secret_key = secrets.token_urlsafe(24)\n",
    "        df = df.withColumn(\"CPF_CRIPTOGRAFADO\", expr(f\"base64(aes_encrypt(CPF, '{secret_key}', 'ECB', 'PKCS'))\")).drop(df.CPF)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def process_simples(self, df):\n",
    "        \"\"\"\n",
    "        Processa o DataFrame relacionado ao regime tributário SIMPLES das empresas.\n",
    "\n",
    "        Parâmetros:\n",
    "            df (DataFrame): DataFrame inicial contendo informações relacionadas ao regime tributário SIMPLES.\n",
    "\n",
    "        Retorna:\n",
    "            DataFrame: DataFrame processado com colunas de data convertidas e apenas as colunas relevantes selecionadas.\n",
    "\n",
    "        Descrição:\n",
    "            - Converte colunas que representam datas do formato \"yyyyMMdd\" para o tipo data.\n",
    "            - Seleciona apenas as colunas relevantes para o contexto, que são: 'CNPJ_BASICO', 'OPÇAO_PELO_MEI', 'DT_OPCAO_MEI', 'DT_EXCLUSAO_MEI', 'OPCAO_PELO_SIMPLES', 'DT_OPCAO_SIMPLES', e 'DT_EXCLUSAO_SIMPLES'.\n",
    "\n",
    "        Notas:\n",
    "            - Esta função assume que as colunas de data estão no formato \"yyyyMMdd\" e realiza a conversão para o tipo data.\n",
    "            - As colunas de datas que são processadas incluem: DATA_OPCAO_PELO_SIMPLES, DATA_EXCLUSAO_SIMPLES, DATA_EXCLUSAO_MEI e DATA_OPCAO_PELO_MEI.\n",
    "        \"\"\"\n",
    "        df = df.withColumn(\"DT_OPCAO_SIMPLES\", F.to_date(F.col('DATA_OPCAO_PELO_SIMPLES'), \"yyyyMMdd\"))\n",
    "        df = df.withColumn(\"DT_EXCLUSAO_SIMPLES\", to_date(F.col('DATA_EXCLUSAO_SIMPLES'), \"yyyyMMdd\"))\n",
    "        df = df.withColumn(\"DT_EXCLUSAO_MEI\", to_date(F.col('DATA_EXCLUSAO_MEI'), \"yyyyMMdd\"))\n",
    "        df = df.withColumn(\"DT_OPCAO_MEI\", to_date(F.col('DATA_OPCAO_PELO_MEI'), \"yyyyMMdd\"))\n",
    "        df = df.select('CNPJ_BASICO','OPÇAO_PELO_MEI','DT_OPCAO_MEI','DT_EXCLUSAO_MEI','OPCAO_PELO_SIMPLES','DT_OPCAO_SIMPLES','DT_EXCLUSAO_SIMPLES')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def save_data(self, df, path, num_partitions=1, file_format=\"parquet\"):\n",
    "        \"\"\"\n",
    "        Save the DataFrame to the specified path.\n",
    "        \n",
    "        :param df: DataFrame to be saved\n",
    "        :param path: Destination path\n",
    "        :param num_partitions: Number of partitions for saving data (default is 1)\n",
    "        :param file_format: File format to save the data (default is \"parquet\")\n",
    "        \"\"\"\n",
    "        \n",
    "        # Repartitioning the DataFrame based on user input\n",
    "        df = df.repartition(num_partitions)\n",
    "        \n",
    "        # Saving the DataFrame to the specified path and format\n",
    "        df.write.mode('overwrite').format(file_format).save(path)\n",
    "        \n",
    "    @staticmethod\n",
    "    def download_nomes(save_base_path=\"./output/nomes\"):        \n",
    "        \"\"\"\n",
    "        Baixa e extrai o arquivo nomes.csv.gz do dataset genero-nomes no Brasil.io.\n",
    "\n",
    "        Parâmetros:\n",
    "            save_base_path (str, opcional): Caminho base onde o arquivo será salvo. O padrão é './output/nomes'.\n",
    "\n",
    "        Descrição:\n",
    "            - Cria o diretório de salvamento se ele não existir.\n",
    "            - Baixa o arquivo nomes.csv.gz da URL especificada.\n",
    "            - Extrai o conteúdo do arquivo .gz.\n",
    "            - Remove o arquivo .gz original, mantendo apenas o arquivo CSV extraído.\n",
    "\n",
    "        Notas:\n",
    "            - Esta função usa a biblioteca `requests` para baixar o arquivo.\n",
    "            - A função verifica se a resposta do servidor é 200 (sucesso) antes de baixar o arquivo.\n",
    "            - O arquivo .gz é extraído usando a biblioteca `gzip`.\n",
    "        \"\"\"\n",
    "        # Certifique-se de que o diretório de salvamento exista\n",
    "        os.makedirs(save_base_path, exist_ok=True)\n",
    "        url = \"https://data.brasil.io/dataset/genero-nomes/nomes.csv.gz\"\n",
    "        # Derive o nome do arquivo da URL\n",
    "        file_name = os.path.basename(url)\n",
    "        file_path = os.path.join(save_base_path, file_name)\n",
    "        extracted_file_path = os.path.join(save_base_path, file_name[:-3])  # remove .gz\n",
    "\n",
    "        # Baixe o arquivo\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(file_path, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=128):\n",
    "                    file.write(chunk)\n",
    "        else:\n",
    "            print(f\"Failed to download {url}. Status code: {response.status_code}\")\n",
    "            return\n",
    "\n",
    "        # Extraia o arquivo\n",
    "        with gzip.open(file_path, 'rb') as f_in:\n",
    "            with open(extracted_file_path, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "        # Apague o arquivo .gz\n",
    "        os.remove(file_path)\n",
    "        \n",
    "    def process_mei(self, df, save_base_path=\"./output/nomes\", file_name=\"nomes.csv\"):\n",
    "        \"\"\"\n",
    "        Processa um DataFrame referente a MEIs, realiza joins com dados adicionais de naturezas jurídicas,\n",
    "        qualificações, e um conjunto de dados de nomes para extração e categorização de primeiro nome.\n",
    "\n",
    "        Parâmetros:\n",
    "            df (pyspark.sql.DataFrame): DataFrame inicial contendo dados sobre MEIs.\n",
    "            save_base_path (str, opcional): Caminho onde o arquivo com dados de nomes foi extraído. Padrão é './output/nomes'.\n",
    "            file_name (str, opcional): Nome do arquivo CSV contendo dados de nomes a ser lido. Padrão é 'nomes.csv'.\n",
    "\n",
    "        Retorna:\n",
    "            pyspark.sql.DataFrame: DataFrame processado após todas as transformações e joins.\n",
    "\n",
    "        Descrição:\n",
    "            1. Realiza join com DataFrames de 'naturezas' e 'qualificações'.\n",
    "            2. Extração e manipulação de dados de CPF.\n",
    "            3. Utiliza um dicionário para mapear e criar a coluna \"NM_PORTE\".\n",
    "            4. Criptografa a coluna de CPF.\n",
    "            5. Realiza filtragens baseado na probabilidade do nome ser um CPF válido.\n",
    "            6. Lê um conjunto de dados de nomes e realiza o explode na coluna 'alternative_names'.\n",
    "            7. Extrai o primeiro nome da coluna 'NOME_EMPRESA'.\n",
    "            8. Realiza o join com o conjunto de dados de nomes para categorizar o primeiro nome.\n",
    "            9. Retorna um DataFrame contendo informações relevantes após todas as transformações.\n",
    "        \"\"\"\n",
    "\n",
    "        df_nat = self.read_data(schema_name='naturezas')\n",
    "        df_qual = self.read_data(schema_name='qualificacoes')\n",
    "        \n",
    "        df = df.withColumnRenamed(\"QUALIF_RESPONVAVEL\", \"COD_QUALIFICACAO\")\n",
    "        \n",
    "        df = df.join(broadcast(df_nat), \"COD_NAT_JURICA\", \"left\").drop(df.COD_NAT_JURICA)\n",
    "        df = df.join(broadcast(df_qual), \"COD_QUALIFICACAO\", \"left\").drop(df.COD_QUALIFICACAO)\n",
    "               \n",
    "        df = df.withColumn(\"CPF\", regexp_replace(\"NOME_EMPRESA\", \"[^0-9]\", \"\"))\n",
    "        df = df.withColumn(\"CPF\", when(col(\"CPF\") == \"\", None).otherwise(col(\"CPF\")))\n",
    "        df = df.withColumn('CPF_LEN', length('CPF'))\n",
    "        # Defina o dicionário de mapeamento\n",
    "        mapping = {0: 'NÃO INFORMADO',1: 'MICRO EMPRESA',3: ' EMPRESA DE PEQUENO PORTE',5: 'DEMAIS',8: 'BAIXADA'}\n",
    "        # Use a função 'when' para criar a nova coluna 'NM_SIT_CADASTRAL'\n",
    "        df = df.withColumn(\"NM_PORTE\",\n",
    "                           when(df[\"PORTE\"].isin(list(mapping.keys())), df[\"PORTE\"]).otherwise(None))\n",
    "        \n",
    "        # Substitua os valores na nova coluna com base no dicionário de mapeamento\n",
    "        for key, value in mapping.items():\n",
    "            df = df.withColumn(\"NM_PORTE\", when(df[\"PORTE\"] == key, value).otherwise(df[\"NM_PORTE\"]))\n",
    "\n",
    "        df = df.withColumn(\"PROBABILIDADE_DE_SER_CPF\", when(df[\"CPF_LEN\"] == 11, \"SIM\").otherwise(\"NAO\"))\n",
    "        \n",
    "        secret_key = secrets.token_urlsafe(24)\n",
    "        df = df.withColumn(\"CPF_CRIPTOGRAFADO\", expr(f\"base64(aes_encrypt(CPF, '{secret_key}', 'ECB', 'PKCS'))\")).drop(df.CPF)\n",
    "        \n",
    "        # Caminho completo do arquivo\n",
    "        file_path = os.path.join(save_base_path, file_name)\n",
    "        \n",
    "        # Filtrar df_processed baseado na coluna PROBABILIDADE_DE_SER_CPF\n",
    "        df_filter = df.filter(col('PROBABILIDADE_DE_SER_CPF') == 'SIM').dropDuplicates(subset=['CPF_CRIPTOGRAFADO', 'NOME_EMPRESA'])\n",
    "\n",
    "        # Ler o arquivo CSV\n",
    "        df = self.spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "        # Explodir a coluna alternative_names para múltiplas linhas\n",
    "        df_expanded = df.withColumn(\"alternative_names\", explode(split(coalesce(col(\"alternative_names\"), col(\"first_name\")), \"\\\\|\")))\n",
    "\n",
    "        # Selecionar as colunas desejadas\n",
    "        df_result = df_expanded.select(\"alternative_names\", \"group_name\", \"ratio\", \"classification\").dropDuplicates(subset=['alternative_names'])\n",
    "\n",
    "        # Extrair o primeiro nome da coluna NOME_EMPRESA\n",
    "        df_filter = df_filter.withColumn(\"PRIMEIRO_NOME\", split(col(\"NOME_EMPRESA\"), \" \")[0])\n",
    "        \n",
    "        # Fazer o join entre df_filter e df_result\n",
    "        joined_df = df_filter.join(df_result, df_filter.PRIMEIRO_NOME == df_result.alternative_names, \"left\").dropDuplicates()\n",
    "        \n",
    "        joined_df = joined_df.select('CNPJ','NOME_EMPRESA','CAP_SOCIAL','NM_PORTE','NAT_JURICA','ENTE_FEDERATIVO','NM_QUALIFICACAO','CPF_CRIPTOGRAFADO','CPF_LEN',\n",
    "                    'PROBABILIDADE_DE_SER_CPF','PRIMEIRO_NOME',col('group_name').alias('GRUPO_NOME'), \n",
    "                    col('ratio').alias('PROBABILIDADE_CLASSIFICACAO'), col('classification').alias('CLASSIFICACAO')).dropDuplicates()\n",
    "        \n",
    "        return joined_df\n",
    "    \n",
    "    def process_socios(self, df, save_base_path=\"./output/nomes\", file_name=\"nomes.csv\"):\n",
    "        \"\"\"\n",
    "        Processa um DataFrame referente a sócios, realiza joins com dados adicionais de países, qualificações, \n",
    "        e um conjunto de dados de nomes para extração e categorização de primeiro nome.\n",
    "\n",
    "        Parâmetros:\n",
    "            df (pyspark.sql.DataFrame): DataFrame inicial contendo dados sobre sócios.\n",
    "            save_base_path (str, opcional): Caminho onde o arquivo com dados de nomes foi extraído. Padrão é './output/nomes'.\n",
    "            file_name (str, opcional): Nome do arquivo CSV contendo dados de nomes a ser lido. Padrão é 'nomes.csv'.\n",
    "\n",
    "        Retorna:\n",
    "            pyspark.sql.DataFrame: DataFrame processado após todas as transformações e joins.\n",
    "\n",
    "        Descrição:\n",
    "            1. Realiza join com DataFrames de 'países'.\n",
    "            2. Usa mapeamentos para criar colunas \"NM_FAIXA_ETARIA\" e \"NM_IDENTIFICADOR_SOCIO\".\n",
    "            3. Renomeia e realiza join com DataFrame de qualificações para obter descrições das qualificações.\n",
    "            4. Converte coluna de data \"DATA_ENTRADA_SOCIEDADE\" para o formato desejado.\n",
    "            5. Lê e processa um conjunto de dados de nomes, explodindo e selecionando colunas relevantes.\n",
    "            6. Extração do primeiro nome da coluna 'NOME_SOCIO_RAZAO_SOCIAL'.\n",
    "            7. Realiza o join entre o DataFrame processado e o conjunto de dados de nomes para categorizar o primeiro nome.\n",
    "            8. Retorna um DataFrame contendo informações relevantes após todas as transformações.\n",
    "        \"\"\"\n",
    "        \n",
    "        df_pais = self.read_data(schema_name='paises')\n",
    "        df = df.withColumnRenamed(\"PAIS\", \"COD_PAIS\")\n",
    "        df = df.join(broadcast(df_pais), \"COD_PAIS\", \"left\").drop(df.COD_PAIS)\n",
    "        \n",
    "        # Mapeamento de códigos para faixas etárias.\n",
    "        mapping = {\n",
    "            1: '0 a 12 anos',\n",
    "            2: '13 a 20 anos',\n",
    "            3: '21 a 30 anos',\n",
    "            4: '31 a 40 anos',\n",
    "            5: '41 a 50 anos',\n",
    "            6: '51 a 60 anos',\n",
    "            7: '61 a 70 anos',\n",
    "            8: '71 a 80 anos',\n",
    "            9: 'maiores de 80 anos',\n",
    "            0: 'NA'\n",
    "        }\n",
    "        \n",
    "                \n",
    "        # Mapeamento de códigos para faixas etárias.\n",
    "        id_socio = {\n",
    "            1: 'PESSOA JURIDICA',\n",
    "            2: 'PESSOA FISICA',\n",
    "            3: 'ESTRANGEIRO'}\n",
    "        \n",
    "        df_qual = self.read_data(schema_name='qualificacoes')\n",
    "        \n",
    "        \n",
    "        # Use a função 'when' para criar a nova coluna 'NM_SIT_CADASTRAL'\n",
    "        df = df.withColumn(\"NM_FAIXA_ETARIA\",\n",
    "                           when(df[\"FAIXA_ETARIA\"].isin(list(mapping.keys())), df[\"FAIXA_ETARIA\"]).otherwise(None))\n",
    "        \n",
    "        # Substitua os valores na nova coluna com base no dicionário de mapeamento\n",
    "        for key, value in mapping.items():\n",
    "            df = df.withColumn(\"NM_FAIXA_ETARIA\", when(df[\"FAIXA_ETARIA\"] == key, value).otherwise(df[\"NM_FAIXA_ETARIA\"]))\n",
    "            \n",
    "\n",
    "        # Use a função 'when' para criar a nova coluna 'NM_SIT_CADASTRAL'\n",
    "        df = df.withColumn(\"NM_IDENTIFICADOR_SOCIO\",\n",
    "                           when(df[\"IDENTIFICADOR_SOCIO\"].isin(list(id_socio.keys())), df[\"IDENTIFICADOR_SOCIO\"]).otherwise(None))\n",
    "        \n",
    "        # Substitua os valores na nova coluna com base no dicionário de mapeamento\n",
    "        for key, value in id_socio.items():\n",
    "            df = df.withColumn(\"NM_IDENTIFICADOR_SOCIO\", when(df[\"IDENTIFICADOR_SOCIO\"] == key, value).otherwise(df[\"NM_IDENTIFICADOR_SOCIO\"]))\n",
    "\n",
    "\n",
    "        df_qual = self.read_data(schema_name='qualificacoes')\n",
    "        # Renomeação e join com df_qual para obter descrições de qualificações.\n",
    "        df = df.withColumnRenamed(\"QUALIFICACAO_REPRESENTANTE_LEGAL\", \"COD_QUALIFICACAO\")\n",
    "        df = df.join(broadcast(df_qual), \"COD_QUALIFICACAO\", \"left\").drop(\"COD_QUALIFICACAO\")\n",
    "        df = df.withColumnRenamed(\"NM_QUALIFICACAO\", \"NM_QUALIFICACAO_REPRESENTANTE_LEGAL\")\n",
    "\n",
    "        df = df.withColumnRenamed(\"QUALIFICAÇAO_SOCIO\", \"COD_QUALIFICACAO\")\n",
    "        df = df.join(broadcast(df_qual), \"COD_QUALIFICACAO\", \"left\").drop(\"COD_QUALIFICACAO\")\n",
    "        df = df.withColumnRenamed(\"NM_QUALIFICACAO\", \"NM_QUALIFICAÇAO_SOCIO\")\n",
    "\n",
    "        # Conversão da coluna de data.\n",
    "        df = df.withColumn(\"DT_ENTRADA_SOCIEDADE\", to_date(col('DATA_ENTRADA_SOCIEDADE'), \"yyyyMMdd\")).drop(df.DATA_ENTRADA_SOCIEDADE)\n",
    "\n",
    "        # Leitura do arquivo CSV.\n",
    "        file_path = os.path.join(save_base_path, file_name)\n",
    "        df_csv = self.spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "        df_csv = df_csv.withColumn(\"alternative_name2\", explode(split(df_csv[\"alternative_names\"], \"\\|\")))\n",
    "        df_result = df_csv.select(\"alternative_name2\", \"group_name\", \"ratio\", \"classification\").dropDuplicates([\"alternative_name2\"])\n",
    "\n",
    "        # Extração do primeiro nome.\n",
    "        df = df.withColumn(\"PRIMEIRO_NOME\", split(col(\"NOME_SOCIO_RAZAO_SOCIAL\"), \" \")[0]).dropDuplicates()\n",
    "\n",
    "        # Join entre dataframes.\n",
    "        joined_df = df.join(df_result, df.PRIMEIRO_NOME == df_result.alternative_name2, \"left\").dropDuplicates()\n",
    "        \n",
    "        joined_df = joined_df.select('CNPJ_BASICO','NOME_SOCIO_RAZAO_SOCIAL','CNPJ_CPF_SOCIO','REPRESENTANTE_LEGAL',\n",
    "        'NOME_REPRESENTANTE','NM_PAIS','NM_FAIXA_ETARIA','NM_IDENTIFICADOR_SOCIO','NM_QUALIFICACAO_REPRESENTANTE_LEGAL',\n",
    "        'NM_QUALIFICAÇAO_SOCIO','DT_ENTRADA_SOCIEDADE','PRIMEIRO_NOME',col('ratio').alias('PROBABILIDADE_CLASSIFICACAO'),\n",
    "        col('classification').alias('CLASSIFICACAO')).dropDuplicates()\n",
    "\n",
    "        return joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2352f5-939e-461e-bfb3-55c5019359ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baixar o arquivo de nomes para realizar tratamento de sexo nas tabelas MEI e Socios\n",
    "#receitaLT_processor = ReceitaLT(spark)\n",
    "#receitaLT_processor.download_nomes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "74711e59-d4c7-4076-8559-069f3388c4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Detected encodings: {'./output\\\\Estabelecimentos\\\\Estabelecimentos1.csv': 'ascii', './output\\\\Estabelecimentos\\\\Estabelecimentos2.csv': 'ascii', './output\\\\Estabelecimentos\\\\Estabelecimentos3.csv': 'ascii', './output\\\\Estabelecimentos\\\\Estabelecimentos4.csv': 'ascii', './output\\\\Estabelecimentos\\\\Estabelecimentos5.csv': 'ascii', './output\\\\Estabelecimentos\\\\Estabelecimentos6.csv': 'ascii', './output\\\\Estabelecimentos\\\\Estabelecimentos8.csv': 'ascii'}\n",
      "INFO:__main__:Detected encodings: {'./output\\\\Paises\\\\Paises.csv': 'ISO-8859-1'}\n",
      "INFO:__main__:Detected encodings: {'./output\\\\Municipios\\\\Municipios.csv': 'ascii'}\n",
      "INFO:__main__:Detected encodings: {'./output\\\\Cnaes\\\\Cnaes.csv': 'ISO-8859-1'}\n",
      "INFO:__main__:Detected encodings: {'./output\\\\Motivos\\\\Motivos.csv': 'ascii'}\n"
     ]
    }
   ],
   "source": [
    "receitaLT_processor = ReceitaLT(spark)\n",
    "df = receitaLT_processor.read_data(schema_name='estabelecimentos').cache()\n",
    "df_estabelecimentos = receitaLT_processor.process_estabelecimentos(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "324b28fc-18cd-4e51-81e0-878f53c8858f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>CNPJ_BASICO</th><th>CNPJ_ORDEM</th><th>CNPJ_DV</th><th>MATRIZ_FILIAL</th><th>NOME_FANTASIA</th><th>SIT_CADASTRAL</th><th>DT_SIT_CADASTRAL</th><th>NOME_CIDADE_EXTERIOR</th><th>DT_INICIO_ATIVIDADE</th><th>CNAE_2</th><th>TIPO_LOUGRADOURO</th><th>LOGRADOURO</th><th>NUMERO</th><th>COMPLEMENTO</th><th>BAIRRO</th><th>CEP</th><th>UF</th><th>DDD1</th><th>TEL1</th><th>DDD2</th><th>TEL2</th><th>DDD_FAX</th><th>FAX</th><th>EMAIL</th><th>SIT_ESPECIAL</th><th>DT_SIT_ESPECIAL</th><th>NM_MOTIVO</th><th>CNAE</th><th>MUNICIPIO</th><th>NM_PAIS</th><th>PROVEDOR</th><th>ano_cadastro</th><th>mes_cadastro</th><th>ano_sit_cadastral</th><th>mes_sit_cadastral</th><th>NM_SIT_CADASTRAL</th><th>VALILD_EMAIL</th><th>NM_MATRIZ_FILIAL</th><th>ENDERECO_COMPLETO</th><th>COORDENADAS</th></tr>\n",
       "<tr><td>07396865</td><td>0001</td><td>68</td><td>1</td><td>NULL</td><td>8</td><td>2017-02-10</td><td>NULL</td><td>2005-05-18</td><td>1411801</td><td>RUA</td><td>TUCANEIRA</td><td>30</td><td>NULL</td><td>DOS LAGOS</td><td>89136000</td><td>SC</td><td>47</td><td>33851125</td><td>47</td><td>33851125</td><td>47</td><td>33851125</td><td>NULL</td><td>NULL</td><td>NULL</td><td>EXTINCAO POR ENCE...</td><td>Confec&ccedil;&atilde;o, sob me...</td><td>RODEIO</td><td>NULL</td><td>NULL</td><td>2005</td><td>5</td><td>2017</td><td>2</td><td>BAIXADA</td><td>NULL</td><td>MATRIZ</td><td>RUA TUCANEIRA, 30...</td><td>{-26.870523, -49....</td></tr>\n",
       "<tr><td>64904295</td><td>0018</td><td>51</td><td>2</td><td>NULL</td><td>8</td><td>2016-11-10</td><td>NULL</td><td>2005-04-29</td><td>4637199</td><td>AVENIDA</td><td>MENINO MARCELO</td><td>NULL</td><td>LOTE  2          ...</td><td>SERRARIA</td><td>57046000</td><td>AL</td><td>11</td><td>36491000</td><td>31</td><td>33880436</td><td>82</td><td>33118379</td><td>claudio.giglio@ca...</td><td>NULL</td><td>NULL</td><td>EXTINCAO POR ENCE...</td><td>Com&eacute;rcio atacadis...</td><td>MACEIO</td><td>NULL</td><td>CAMIL</td><td>2005</td><td>4</td><td>2016</td><td>11</td><td>BAIXADA</td><td>claudio.giglio@ca...</td><td>FILIAL</td><td>AVENIDA MENINO MA...</td><td>{-9.554351, -35.7...</td></tr>\n",
       "<tr><td>76016369</td><td>0003</td><td>16</td><td>2</td><td>NULL</td><td>3</td><td>2006-02-03</td><td>NULL</td><td>1985-12-12</td><td>NULL</td><td>RUA</td><td>DO COMERCIO</td><td>55</td><td>SALA 7 GALERIA</td><td>CENTRO</td><td>11010141</td><td>SP</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>PEDIDO DE BAIXA I...</td><td>Com&eacute;rcio atacadis...</td><td>SANTOS</td><td>NULL</td><td>NULL</td><td>1985</td><td>12</td><td>2006</td><td>2</td><td>SUSPENSA</td><td>NULL</td><td>FILIAL</td><td>RUA DO COMERCIO, ...</td><td>{-23.93253, -46.3...</td></tr>\n",
       "<tr><td>52302726</td><td>0001</td><td>82</td><td>1</td><td>NULL</td><td>4</td><td>2021-04-06</td><td>NULL</td><td>1983-02-23</td><td>NULL</td><td>RUA</td><td>GREGORIO LUCHIARI</td><td>496</td><td>NULL</td><td>SAO VITO</td><td>13472080</td><td>SP</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>OMISSAO DE DECLAR...</td><td>Com&eacute;rcio varejist...</td><td>AMERICANA</td><td>NULL</td><td>NULL</td><td>1983</td><td>2</td><td>2021</td><td>4</td><td>INAPTA</td><td>NULL</td><td>MATRIZ</td><td>RUA GREGORIO LUCH...</td><td>{-22.7283, -47.31...</td></tr>\n",
       "<tr><td>07396923</td><td>0001</td><td>53</td><td>1</td><td>NULL</td><td>8</td><td>2014-01-15</td><td>NULL</td><td>2005-05-16</td><td>NULL</td><td>RUA</td><td>DA MOOCA</td><td>3336</td><td>NULL</td><td>MOOCA</td><td>3165000</td><td>SP</td><td>011</td><td>69658088</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>EXTINCAO POR ENCE...</td><td>Padaria e confeit...</td><td>SAO PAULO</td><td>NULL</td><td>NULL</td><td>2005</td><td>5</td><td>2014</td><td>1</td><td>BAIXADA</td><td>NULL</td><td>MATRIZ</td><td>RUA DA MOOCA, 333...</td><td>{-23.557314, -46....</td></tr>\n",
       "<tr><td>03650261</td><td>0001</td><td>45</td><td>1</td><td>OTICA PERFEICAO</td><td>4</td><td>2019-03-22</td><td>NULL</td><td>1999-12-17</td><td>4783102</td><td>RUA</td><td>PREFEITO JOAO ORE...</td><td>541</td><td>LOJA 03</td><td>CENTRO</td><td>88495000</td><td>SC</td><td>048</td><td>2423953</td><td>NULL</td><td>NULL</td><td>48</td><td>2423953</td><td>NULL</td><td>NULL</td><td>NULL</td><td>OMISSAO DE DECLAR...</td><td>Com&eacute;rcio varejist...</td><td>GAROPABA</td><td>NULL</td><td>NULL</td><td>1999</td><td>12</td><td>2019</td><td>3</td><td>INAPTA</td><td>NULL</td><td>MATRIZ</td><td>RUA PREFEITO JOAO...</td><td>{-28.026684, -48....</td></tr>\n",
       "<tr><td>07396929</td><td>0001</td><td>20</td><td>1</td><td>NULL</td><td>3</td><td>2011-11-07</td><td>NULL</td><td>2005-05-04</td><td>6204000</td><td>RUA</td><td>FLORIANO PEIXOTO,</td><td>85</td><td>NULL</td><td>SANTA PAULA</td><td>9541350</td><td>SP</td><td>11</td><td>32281722</td><td>NULL</td><td>NULL</td><td>11</td><td>32281722</td><td>NULL</td><td>NULL</td><td>NULL</td><td>PEDIDO DE BAIXA I...</td><td>Desenvolvimento d...</td><td>SAO CAETANO DO SUL</td><td>NULL</td><td>NULL</td><td>2005</td><td>5</td><td>2011</td><td>11</td><td>SUSPENSA</td><td>NULL</td><td>MATRIZ</td><td>RUA FLORIANO PEIX...</td><td>{-23.618265, -46....</td></tr>\n",
       "<tr><td>25040718</td><td>0001</td><td>32</td><td>1</td><td>COOCULTURA LTDA</td><td>8</td><td>2005-05-23</td><td>NULL</td><td>1991-08-23</td><td>NULL</td><td>RUA</td><td>ROSULINO FERREIRA...</td><td>767</td><td>NULL</td><td>CENTRO</td><td>75902261</td><td>GO</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>EXTINCAO POR ENCE...</td><td>Bancos cooperativos</td><td>RIO VERDE</td><td>NULL</td><td>NULL</td><td>1991</td><td>8</td><td>2005</td><td>5</td><td>BAIXADA</td><td>NULL</td><td>MATRIZ</td><td>RUA ROSULINO FERR...</td><td>{NULL, NULL}</td></tr>\n",
       "<tr><td>07396936</td><td>0001</td><td>22</td><td>1</td><td>NULL</td><td>8</td><td>2005-12-19</td><td>NULL</td><td>2005-05-25</td><td>NULL</td><td>AVENIDA</td><td>DOM PEDRO II</td><td>1748</td><td>NULL</td><td>CARLOS PRATES</td><td>30710010</td><td>MG</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>EXTINCAO POR ENCE...</td><td>Com&eacute;rcio a varejo...</td><td>BELO HORIZONTE</td><td>NULL</td><td>NULL</td><td>2005</td><td>5</td><td>2005</td><td>12</td><td>BAIXADA</td><td>NULL</td><td>MATRIZ</td><td>AVENIDA DOM PEDRO...</td><td>{-18.754425, -44....</td></tr>\n",
       "<tr><td>07396943</td><td>0001</td><td>24</td><td>1</td><td>MEGA TELECOM</td><td>8</td><td>2017-02-17</td><td>NULL</td><td>2005-05-24</td><td>9512600</td><td>AVENIDA</td><td>PARANA</td><td>465</td><td>NULL</td><td>CENTRO</td><td>83800000</td><td>PR</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>EXTINCAO POR ENCE...</td><td>Com&eacute;rcio varejist...</td><td>MANDIRITUBA</td><td>NULL</td><td>NULL</td><td>2005</td><td>5</td><td>2017</td><td>2</td><td>BAIXADA</td><td>NULL</td><td>MATRIZ</td><td>AVENIDA PARANA, 4...</td><td>{-25.753084, -49....</td></tr>\n",
       "</table>\n",
       "only showing top 10 rows\n"
      ],
      "text/plain": [
       "+-----------+----------+-------+-------------+---------------+-------------+----------------+--------------------+-------------------+-------+----------------+--------------------+------+--------------------+-------------+--------+---+----+--------+----+--------+-------+--------+--------------------+------------+---------------+--------------------+--------------------+------------------+-------+--------+------------+------------+-----------------+-----------------+----------------+--------------------+----------------+--------------------+--------------------+\n",
       "|CNPJ_BASICO|CNPJ_ORDEM|CNPJ_DV|MATRIZ_FILIAL|  NOME_FANTASIA|SIT_CADASTRAL|DT_SIT_CADASTRAL|NOME_CIDADE_EXTERIOR|DT_INICIO_ATIVIDADE| CNAE_2|TIPO_LOUGRADOURO|          LOGRADOURO|NUMERO|         COMPLEMENTO|       BAIRRO|     CEP| UF|DDD1|    TEL1|DDD2|    TEL2|DDD_FAX|     FAX|               EMAIL|SIT_ESPECIAL|DT_SIT_ESPECIAL|           NM_MOTIVO|                CNAE|         MUNICIPIO|NM_PAIS|PROVEDOR|ano_cadastro|mes_cadastro|ano_sit_cadastral|mes_sit_cadastral|NM_SIT_CADASTRAL|        VALILD_EMAIL|NM_MATRIZ_FILIAL|   ENDERECO_COMPLETO|         COORDENADAS|\n",
       "+-----------+----------+-------+-------------+---------------+-------------+----------------+--------------------+-------------------+-------+----------------+--------------------+------+--------------------+-------------+--------+---+----+--------+----+--------+-------+--------+--------------------+------------+---------------+--------------------+--------------------+------------------+-------+--------+------------+------------+-----------------+-----------------+----------------+--------------------+----------------+--------------------+--------------------+\n",
       "|   07396865|      0001|     68|            1|           NULL|            8|      2017-02-10|                NULL|         2005-05-18|1411801|             RUA|           TUCANEIRA|    30|                NULL|    DOS LAGOS|89136000| SC|  47|33851125|  47|33851125|     47|33851125|                NULL|        NULL|           NULL|EXTINCAO POR ENCE...|Confecção, sob me...|            RODEIO|   NULL|    NULL|        2005|           5|             2017|                2|         BAIXADA|                NULL|          MATRIZ|RUA TUCANEIRA, 30...|{-26.870523, -49....|\n",
       "|   64904295|      0018|     51|            2|           NULL|            8|      2016-11-10|                NULL|         2005-04-29|4637199|         AVENIDA|      MENINO MARCELO|  NULL|LOTE  2          ...|     SERRARIA|57046000| AL|  11|36491000|  31|33880436|     82|33118379|claudio.giglio@ca...|        NULL|           NULL|EXTINCAO POR ENCE...|Comércio atacadis...|            MACEIO|   NULL|   CAMIL|        2005|           4|             2016|               11|         BAIXADA|claudio.giglio@ca...|          FILIAL|AVENIDA MENINO MA...|{-9.554351, -35.7...|\n",
       "|   76016369|      0003|     16|            2|           NULL|            3|      2006-02-03|                NULL|         1985-12-12|   NULL|             RUA|         DO COMERCIO|    55|      SALA 7 GALERIA|       CENTRO|11010141| SP|NULL|    NULL|NULL|    NULL|   NULL|    NULL|                NULL|        NULL|           NULL|PEDIDO DE BAIXA I...|Comércio atacadis...|            SANTOS|   NULL|    NULL|        1985|          12|             2006|                2|        SUSPENSA|                NULL|          FILIAL|RUA DO COMERCIO, ...|{-23.93253, -46.3...|\n",
       "|   52302726|      0001|     82|            1|           NULL|            4|      2021-04-06|                NULL|         1983-02-23|   NULL|             RUA|   GREGORIO LUCHIARI|   496|                NULL|     SAO VITO|13472080| SP|NULL|    NULL|NULL|    NULL|   NULL|    NULL|                NULL|        NULL|           NULL|OMISSAO DE DECLAR...|Comércio varejist...|         AMERICANA|   NULL|    NULL|        1983|           2|             2021|                4|          INAPTA|                NULL|          MATRIZ|RUA GREGORIO LUCH...|{-22.7283, -47.31...|\n",
       "|   07396923|      0001|     53|            1|           NULL|            8|      2014-01-15|                NULL|         2005-05-16|   NULL|             RUA|            DA MOOCA|  3336|                NULL|        MOOCA| 3165000| SP| 011|69658088|NULL|    NULL|   NULL|    NULL|                NULL|        NULL|           NULL|EXTINCAO POR ENCE...|Padaria e confeit...|         SAO PAULO|   NULL|    NULL|        2005|           5|             2014|                1|         BAIXADA|                NULL|          MATRIZ|RUA DA MOOCA, 333...|{-23.557314, -46....|\n",
       "|   03650261|      0001|     45|            1|OTICA PERFEICAO|            4|      2019-03-22|                NULL|         1999-12-17|4783102|             RUA|PREFEITO JOAO ORE...|   541|             LOJA 03|       CENTRO|88495000| SC| 048| 2423953|NULL|    NULL|     48| 2423953|                NULL|        NULL|           NULL|OMISSAO DE DECLAR...|Comércio varejist...|          GAROPABA|   NULL|    NULL|        1999|          12|             2019|                3|          INAPTA|                NULL|          MATRIZ|RUA PREFEITO JOAO...|{-28.026684, -48....|\n",
       "|   07396929|      0001|     20|            1|           NULL|            3|      2011-11-07|                NULL|         2005-05-04|6204000|             RUA|   FLORIANO PEIXOTO,|    85|                NULL|  SANTA PAULA| 9541350| SP|  11|32281722|NULL|    NULL|     11|32281722|                NULL|        NULL|           NULL|PEDIDO DE BAIXA I...|Desenvolvimento d...|SAO CAETANO DO SUL|   NULL|    NULL|        2005|           5|             2011|               11|        SUSPENSA|                NULL|          MATRIZ|RUA FLORIANO PEIX...|{-23.618265, -46....|\n",
       "|   25040718|      0001|     32|            1|COOCULTURA LTDA|            8|      2005-05-23|                NULL|         1991-08-23|   NULL|             RUA|ROSULINO FERREIRA...|   767|                NULL|       CENTRO|75902261| GO|NULL|    NULL|NULL|    NULL|   NULL|    NULL|                NULL|        NULL|           NULL|EXTINCAO POR ENCE...| Bancos cooperativos|         RIO VERDE|   NULL|    NULL|        1991|           8|             2005|                5|         BAIXADA|                NULL|          MATRIZ|RUA ROSULINO FERR...|        {NULL, NULL}|\n",
       "|   07396936|      0001|     22|            1|           NULL|            8|      2005-12-19|                NULL|         2005-05-25|   NULL|         AVENIDA|        DOM PEDRO II|  1748|                NULL|CARLOS PRATES|30710010| MG|NULL|    NULL|NULL|    NULL|   NULL|    NULL|                NULL|        NULL|           NULL|EXTINCAO POR ENCE...|Comércio a varejo...|    BELO HORIZONTE|   NULL|    NULL|        2005|           5|             2005|               12|         BAIXADA|                NULL|          MATRIZ|AVENIDA DOM PEDRO...|{-18.754425, -44....|\n",
       "|   07396943|      0001|     24|            1|   MEGA TELECOM|            8|      2017-02-17|                NULL|         2005-05-24|9512600|         AVENIDA|              PARANA|   465|                NULL|       CENTRO|83800000| PR|NULL|    NULL|NULL|    NULL|   NULL|    NULL|                NULL|        NULL|           NULL|EXTINCAO POR ENCE...|Comércio varejist...|       MANDIRITUBA|   NULL|    NULL|        2005|           5|             2017|                2|         BAIXADA|                NULL|          MATRIZ|AVENIDA PARANA, 4...|{-25.753084, -49....|\n",
       "+-----------+----------+-------+-------------+---------------+-------------+----------------+--------------------+-------------------+-------+----------------+--------------------+------+--------------------+-------------+--------+---+----+--------+----+--------+-------+--------+--------------------+------------+---------------+--------------------+--------------------+------------------+-------+--------+------------+------------+-----------------+-----------------+----------------+--------------------+----------------+--------------------+--------------------+\n",
       "only showing top 10 rows"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_estabelecimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2c1a338-7169-4d71-8295-3c5aaf2dfa88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Detected encodings: {'./output\\\\Empresas\\\\Empresas0.csv': 'ascii', './output\\\\Empresas\\\\Empresas1.csv': 'ascii', './output\\\\Empresas\\\\Empresas2.csv': 'ascii', './output\\\\Empresas\\\\Empresas3.csv': 'ascii', './output\\\\Empresas\\\\Empresas4.csv': 'ascii', './output\\\\Empresas\\\\Empresas5.csv': 'ascii', './output\\\\Empresas\\\\Empresas6.csv': 'ascii', './output\\\\Empresas\\\\Empresas7.csv': 'ascii', './output\\\\Empresas\\\\Empresas8.csv': 'ascii', './output\\\\Empresas\\\\Empresas9.csv': 'ascii'}\n",
      "INFO:__main__:Detected encodings: {'./output\\\\Naturezas\\\\Naturezas.csv': 'ISO-8859-1'}\n",
      "INFO:__main__:Detected encodings: {'./output\\\\Qualificacoes\\\\Qualificacoes.csv': 'ISO-8859-1'}\n"
     ]
    }
   ],
   "source": [
    "receitaLT_processor = ReceitaLT(spark) \n",
    "df = receitaLT_processor.read_data(schema_name='empresas').cache()\n",
    "df_empresas = receitaLT_processor.process_empresas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a171d8fb-7439-4b36-8ffb-ca8ce51ab1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>CNPJ</th><th>NOME_EMPRESA</th><th>CAP_SOCIAL</th><th>PORTE</th><th>ENTE_FEDERATIVO</th><th>NAT_JURICA</th><th>NM_QUALIFICACAO</th><th>CPF_LEN</th><th>NM_PORTE</th><th>PROBABILIDADE_DE_SER_CPF</th><th>CPF_CRIPTOGRAFADO</th></tr>\n",
       "<tr><td>41273594</td><td>OZINETE DELFINO C...</td><td>5000,00</td><td>01</td><td>NULL</td><td>Empres&aacute;rio (Indiv...</td><td>Empres&aacute;rio</td><td>11</td><td>MICRO EMPRESA</td><td>SIM</td><td>4Ik58r3iZo3gzXcpO...</td></tr>\n",
       "<tr><td>41273595</td><td>GILVAN PEREIRA XA...</td><td>3000,00</td><td>01</td><td>NULL</td><td>Empres&aacute;rio (Indiv...</td><td>Empres&aacute;rio</td><td>11</td><td>MICRO EMPRESA</td><td>SIM</td><td>4AbQJcrtoRbVxnK+m...</td></tr>\n",
       "<tr><td>41273596</td><td>RODRIGO JOSE FERR...</td><td>10000,00</td><td>01</td><td>NULL</td><td>Empres&aacute;rio (Indiv...</td><td>Empres&aacute;rio</td><td>11</td><td>MICRO EMPRESA</td><td>SIM</td><td>LDonCKXKM+ai3p7OW...</td></tr>\n",
       "<tr><td>41273597</td><td>PACHARRUS QUEIROZ...</td><td>5000,00</td><td>01</td><td>NULL</td><td>Empres&aacute;rio (Indiv...</td><td>Empres&aacute;rio</td><td>11</td><td>MICRO EMPRESA</td><td>SIM</td><td>+fJrzDdkb9It9VBn5...</td></tr>\n",
       "<tr><td>41273598</td><td>GLORIA VIANA DIAS...</td><td>1100,00</td><td>01</td><td>NULL</td><td>Empres&aacute;rio (Indiv...</td><td>Empres&aacute;rio</td><td>11</td><td>MICRO EMPRESA</td><td>SIM</td><td>PGaNBj87aOa+21sH0...</td></tr>\n",
       "<tr><td>41273599</td><td>ANA PAULA DA SILV...</td><td>2000,00</td><td>01</td><td>NULL</td><td>Empres&aacute;rio (Indiv...</td><td>Empres&aacute;rio</td><td>11</td><td>MICRO EMPRESA</td><td>SIM</td><td>Iyd1k5pms4WOw7iiB...</td></tr>\n",
       "<tr><td>41273600</td><td>41.273.600 AVANIL...</td><td>50000,00</td><td>01</td><td>NULL</td><td>Empres&aacute;rio (Indiv...</td><td>Empres&aacute;rio</td><td>8</td><td>MICRO EMPRESA</td><td>NAO</td><td>wRm/iwU6de9SSCFS3...</td></tr>\n",
       "<tr><td>41273601</td><td>GABRIELA HELENA F...</td><td>2000,00</td><td>01</td><td>NULL</td><td>Empres&aacute;rio (Indiv...</td><td>Empres&aacute;rio</td><td>11</td><td>MICRO EMPRESA</td><td>SIM</td><td>mZBkl89i1gLgdzZ3A...</td></tr>\n",
       "<tr><td>41273602</td><td>FABIO SOUZA DO RO...</td><td>15000,00</td><td>01</td><td>NULL</td><td>Empres&aacute;rio (Indiv...</td><td>Empres&aacute;rio</td><td>11</td><td>MICRO EMPRESA</td><td>SIM</td><td>3T+9AdNokHRBfnXjP...</td></tr>\n",
       "<tr><td>41273603</td><td>GRAFLINE ACESSORI...</td><td>10000,00</td><td>01</td><td>NULL</td><td>Sociedade Empres&aacute;...</td><td>S&oacute;cio-Administrador</td><td>NULL</td><td>MICRO EMPRESA</td><td>NAO</td><td>NULL</td></tr>\n",
       "</table>\n",
       "only showing top 10 rows\n"
      ],
      "text/plain": [
       "+--------+--------------------+----------+-----+---------------+--------------------+-------------------+-------+-------------+------------------------+--------------------+\n",
       "|    CNPJ|        NOME_EMPRESA|CAP_SOCIAL|PORTE|ENTE_FEDERATIVO|          NAT_JURICA|    NM_QUALIFICACAO|CPF_LEN|     NM_PORTE|PROBABILIDADE_DE_SER_CPF|   CPF_CRIPTOGRAFADO|\n",
       "+--------+--------------------+----------+-----+---------------+--------------------+-------------------+-------+-------------+------------------------+--------------------+\n",
       "|41273594|OZINETE DELFINO C...|   5000,00|   01|           NULL|Empresário (Indiv...|         Empresário|     11|MICRO EMPRESA|                     SIM|4Ik58r3iZo3gzXcpO...|\n",
       "|41273595|GILVAN PEREIRA XA...|   3000,00|   01|           NULL|Empresário (Indiv...|         Empresário|     11|MICRO EMPRESA|                     SIM|4AbQJcrtoRbVxnK+m...|\n",
       "|41273596|RODRIGO JOSE FERR...|  10000,00|   01|           NULL|Empresário (Indiv...|         Empresário|     11|MICRO EMPRESA|                     SIM|LDonCKXKM+ai3p7OW...|\n",
       "|41273597|PACHARRUS QUEIROZ...|   5000,00|   01|           NULL|Empresário (Indiv...|         Empresário|     11|MICRO EMPRESA|                     SIM|+fJrzDdkb9It9VBn5...|\n",
       "|41273598|GLORIA VIANA DIAS...|   1100,00|   01|           NULL|Empresário (Indiv...|         Empresário|     11|MICRO EMPRESA|                     SIM|PGaNBj87aOa+21sH0...|\n",
       "|41273599|ANA PAULA DA SILV...|   2000,00|   01|           NULL|Empresário (Indiv...|         Empresário|     11|MICRO EMPRESA|                     SIM|Iyd1k5pms4WOw7iiB...|\n",
       "|41273600|41.273.600 AVANIL...|  50000,00|   01|           NULL|Empresário (Indiv...|         Empresário|      8|MICRO EMPRESA|                     NAO|wRm/iwU6de9SSCFS3...|\n",
       "|41273601|GABRIELA HELENA F...|   2000,00|   01|           NULL|Empresário (Indiv...|         Empresário|     11|MICRO EMPRESA|                     SIM|mZBkl89i1gLgdzZ3A...|\n",
       "|41273602|FABIO SOUZA DO RO...|  15000,00|   01|           NULL|Empresário (Indiv...|         Empresário|     11|MICRO EMPRESA|                     SIM|3T+9AdNokHRBfnXjP...|\n",
       "|41273603|GRAFLINE ACESSORI...|  10000,00|   01|           NULL|Sociedade Empresá...|Sócio-Administrador|   NULL|MICRO EMPRESA|                     NAO|                NULL|\n",
       "+--------+--------------------+----------+-----+---------------+--------------------+-------------------+-------+-------------+------------------------+--------------------+\n",
       "only showing top 10 rows"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_empresas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85a79c6b-a29c-4bf4-b6cb-44f22fd7b11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Detected encodings: {'./output\\\\Empresas\\\\Empresas0.csv': 'ascii', './output\\\\Empresas\\\\Empresas1.csv': 'ascii', './output\\\\Empresas\\\\Empresas2.csv': 'ascii', './output\\\\Empresas\\\\Empresas3.csv': 'ascii', './output\\\\Empresas\\\\Empresas4.csv': 'ascii', './output\\\\Empresas\\\\Empresas5.csv': 'ascii', './output\\\\Empresas\\\\Empresas6.csv': 'ascii', './output\\\\Empresas\\\\Empresas7.csv': 'ascii', './output\\\\Empresas\\\\Empresas8.csv': 'ascii', './output\\\\Empresas\\\\Empresas9.csv': 'ascii'}\n",
      "INFO:__main__:Detected encodings: {'./output\\\\Naturezas\\\\Naturezas.csv': 'ISO-8859-1'}\n",
      "INFO:__main__:Detected encodings: {'./output\\\\Qualificacoes\\\\Qualificacoes.csv': 'ISO-8859-1'}\n"
     ]
    }
   ],
   "source": [
    "receitaLT_processor = ReceitaLT(spark) \n",
    "df = receitaLT_processor.read_data(schema_name='empresas').cache()\n",
    "df_mei = receitaLT_processor.process_mei(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bf5bb4c-e6d6-4949-ba90-ed120493a676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>CNPJ</th><th>NOME_EMPRESA</th><th>CAP_SOCIAL</th><th>NM_PORTE</th><th>NAT_JURICA</th><th>ENTE_FEDERATIVO</th><th>NM_QUALIFICACAO</th><th>CPF_CRIPTOGRAFADO</th><th>CPF_LEN</th><th>PROBABILIDADE_DE_SER_CPF</th><th>PRIMEIRO_NOME</th><th>GRUPO_NOME</th><th>PROBABILIDADE_CLASSIFICACAO</th><th>CLASSIFICACAO</th></tr>\n",
       "<tr><td>29339640</td><td>LUCIANO RODRIGUES...</td><td>500,00</td><td>MICRO EMPRESA</td><td>Empres&aacute;rio (Indiv...</td><td>NULL</td><td>Empres&aacute;rio</td><td>+++FNFRhxFzD2Tg4B...</td><td>11</td><td>SIM</td><td>LUCIANO</td><td>LUCIANO</td><td>1.0</td><td>M</td></tr>\n",
       "<tr><td>19929305</td><td>SERGIO VIEIRA DE ...</td><td>1,00</td><td>MICRO EMPRESA</td><td>Empres&aacute;rio (Indiv...</td><td>NULL</td><td>Empres&aacute;rio</td><td>+++eH9X+YVTMyHMhw...</td><td>11</td><td>SIM</td><td>SERGIO</td><td>SERGIO</td><td>1.0</td><td>M</td></tr>\n",
       "<tr><td>27383691</td><td>PAULO DOS ANJOS P...</td><td>4000,00</td><td>MICRO EMPRESA</td><td>Empres&aacute;rio (Indiv...</td><td>NULL</td><td>Empres&aacute;rio</td><td>++/GMgrJVv7h0HqIW...</td><td>11</td><td>SIM</td><td>PAULO</td><td>PAULO</td><td>0.9795640326975477</td><td>M</td></tr>\n",
       "<tr><td>17370146</td><td>CLERI DAGMAR DOS ...</td><td>5000,00</td><td>MICRO EMPRESA</td><td>Empres&aacute;rio (Indiv...</td><td>NULL</td><td>Empres&aacute;rio</td><td>++/TkElP39mAWtjp1...</td><td>11</td><td>SIM</td><td>CLERI</td><td>CLERI</td><td>0.831081081081081</td><td>F</td></tr>\n",
       "<tr><td>33458452</td><td>JOSY ANNE TELES D...</td><td>3000,00</td><td>MICRO EMPRESA</td><td>Empres&aacute;rio (Indiv...</td><td>NULL</td><td>Empres&aacute;rio</td><td>++/UmyuOCWeIL8yQh...</td><td>11</td><td>SIM</td><td>JOSY</td><td>JOSI</td><td>1.0</td><td>F</td></tr>\n",
       "<tr><td>46667092</td><td>KARINE SCUTTI LIM...</td><td>1000,00</td><td>MICRO EMPRESA</td><td>Empres&aacute;rio (Indiv...</td><td>NULL</td><td>Empres&aacute;rio</td><td>++/ljZ62erGkmtOPe...</td><td>11</td><td>SIM</td><td>KARINE</td><td>KARINE</td><td>0.9954933407185478</td><td>F</td></tr>\n",
       "<tr><td>47644879</td><td>WILIAN MOCELIN 08...</td><td>20000,00</td><td>MICRO EMPRESA</td><td>Empres&aacute;rio (Indiv...</td><td>NULL</td><td>Empres&aacute;rio</td><td>++0A1scn4okhBkWwG...</td><td>11</td><td>SIM</td><td>WILIAN</td><td>WILIAN</td><td>1.0</td><td>M</td></tr>\n",
       "<tr><td>35256263</td><td>DANYELA PEREIRA D...</td><td>6000,00</td><td>MICRO EMPRESA</td><td>Empres&aacute;rio (Indiv...</td><td>NULL</td><td>Empres&aacute;rio</td><td>++0W4cz6OTz/BGQLv...</td><td>11</td><td>SIM</td><td>DANYELA</td><td>DANIELA</td><td>1.0</td><td>F</td></tr>\n",
       "<tr><td>17112331</td><td>BIANCA MARQUES DE...</td><td>5000,00</td><td>MICRO EMPRESA</td><td>Empres&aacute;rio (Indiv...</td><td>NULL</td><td>Empres&aacute;rio</td><td>++0edu1yyC5Pjvc6V...</td><td>11</td><td>SIM</td><td>BIANCA</td><td>BIANCA</td><td>1.0</td><td>F</td></tr>\n",
       "<tr><td>26607559</td><td>ERMELINDO DE OLIV...</td><td>5000,00</td><td>MICRO EMPRESA</td><td>Empres&aacute;rio (Indiv...</td><td>NULL</td><td>Empres&aacute;rio</td><td>++0nVdIpHTN75mDGo...</td><td>11</td><td>SIM</td><td>ERMELINDO</td><td>ERMELINDO</td><td>1.0</td><td>M</td></tr>\n",
       "</table>\n",
       "only showing top 10 rows\n"
      ],
      "text/plain": [
       "DataFrame[CNPJ: string, NOME_EMPRESA: string, CAP_SOCIAL: string, NM_PORTE: string, NAT_JURICA: string, ENTE_FEDERATIVO: string, NM_QUALIFICACAO: string, CPF_CRIPTOGRAFADO: string, CPF_LEN: int, PROBABILIDADE_DE_SER_CPF: string, PRIMEIRO_NOME: string, GRUPO_NOME: string, PROBABILIDADE_CLASSIFICACAO: double, CLASSIFICACAO: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d5160ad-90d1-4fc3-80d6-70200ca8dff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Detected encodings: {'./output\\\\Socios\\\\Socios0.csv': 'ascii', './output\\\\Socios\\\\Socios1.csv': 'ascii', './output\\\\Socios\\\\Socios2.csv': 'ascii', './output\\\\Socios\\\\Socios3.csv': 'ascii', './output\\\\Socios\\\\Socios4.csv': 'ascii', './output\\\\Socios\\\\Socios5.csv': 'ascii', './output\\\\Socios\\\\Socios6.csv': 'ascii', './output\\\\Socios\\\\Socios7.csv': 'ascii', './output\\\\Socios\\\\Socios8.csv': 'ascii', './output\\\\Socios\\\\Socios9.csv': 'ascii'}\n",
      "INFO:__main__:Detected encodings: {'./output\\\\Paises\\\\Paises.csv': 'ISO-8859-1'}\n",
      "INFO:__main__:Detected encodings: {'./output\\\\Qualificacoes\\\\Qualificacoes.csv': 'ISO-8859-1'}\n",
      "INFO:__main__:Detected encodings: {'./output\\\\Qualificacoes\\\\Qualificacoes.csv': 'ISO-8859-1'}\n"
     ]
    }
   ],
   "source": [
    "receitaLT_processor = ReceitaLT(spark)\n",
    "df = receitaLT_processor.read_data(schema_name='socios').cache()\n",
    "df_socios = receitaLT_processor.process_socios(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ec66f6a-d738-489a-994b-981afd2cbe72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>CNPJ_BASICO</th><th>NOME_SOCIO_RAZAO_SOCIAL</th><th>CNPJ_CPF_SOCIO</th><th>REPRESENTANTE_LEGAL</th><th>NOME_REPRESENTANTE</th><th>NM_PAIS</th><th>NM_FAIXA_ETARIA</th><th>NM_IDENTIFICADOR_SOCIO</th><th>NM_QUALIFICACAO_REPRESENTANTE_LEGAL</th><th>NM_QUALIFICA&Ccedil;AO_SOCIO</th><th>DT_ENTRADA_SOCIEDADE</th><th>PRIMEIRO_NOME</th><th>PROBABILIDADE_CLASSIFICACAO</th><th>CLASSIFICACAO</th></tr>\n",
       "<tr><td>09218183</td><td>MARIA LEONOR VIAN...</td><td>***392041**</td><td>***807143**</td><td>MARCOS ALEXANDRE ...</td><td>PORTUGAL</td><td>61 a 70 anos</td><td>PESSOA FISICA</td><td>Procurador</td><td>S&oacute;cio Pessoa F&iacute;si...</td><td>2007-11-23</td><td>MARIA</td><td>1.0</td><td>F</td></tr>\n",
       "<tr><td>26252608</td><td>DANIEL ROSA BALSE...</td><td>***758757**</td><td>***309658**</td><td>FREDERICO ANTONIO...</td><td>ESTADOS UNIDOS</td><td>31 a 40 anos</td><td>PESSOA FISICA</td><td>Procurador</td><td>S&oacute;cio Pessoa F&iacute;si...</td><td>2016-09-28</td><td>DANIEL</td><td>1.0</td><td>M</td></tr>\n",
       "<tr><td>42262252</td><td>SUZANE MAYUMI IAM...</td><td>***231718**</td><td>***346238**</td><td>CAMILA MAKIKO IAM...</td><td>REINO UNIDO</td><td>41 a 50 anos</td><td>PESSOA FISICA</td><td>Procurador</td><td>S&oacute;cio Pessoa F&iacute;si...</td><td>2021-06-09</td><td>SUZANE</td><td>1.0</td><td>F</td></tr>\n",
       "<tr><td>07984125</td><td>VALTER JORGE PERE...</td><td>***703883**</td><td>***052553**</td><td>LUISA DE MARILAC ...</td><td>PORTUGAL</td><td>61 a 70 anos</td><td>PESSOA FISICA</td><td>Procurador</td><td>S&oacute;cio Pessoa F&iacute;si...</td><td>2006-05-09</td><td>VALTER</td><td>1.0</td><td>M</td></tr>\n",
       "<tr><td>08247240</td><td>CLAUDIO CONTI</td><td>***461244**</td><td>***907894**</td><td>ROBERTO CARLOS NE...</td><td>SUICA</td><td>61 a 70 anos</td><td>PESSOA FISICA</td><td>Procurador</td><td>S&oacute;cio Pessoa F&iacute;si...</td><td>2013-10-16</td><td>CLAUDIO</td><td>1.0</td><td>M</td></tr>\n",
       "<tr><td>29910911</td><td>MARCELO ALVES BAR...</td><td>***015417**</td><td>***369987**</td><td>JOAO CAMILO DE AS...</td><td>ESTADOS UNIDOS</td><td>51 a 60 anos</td><td>PESSOA FISICA</td><td>Procurador</td><td>S&oacute;cio Pessoa F&iacute;si...</td><td>2018-03-12</td><td>MARCELO</td><td>1.0</td><td>M</td></tr>\n",
       "<tr><td>39768913</td><td>ROBERT MITCHELL V...</td><td>***436518**</td><td>***194698**</td><td>RENATO PREVIATO ROJA</td><td>Pa&iacute;ses Baixos (Ho...</td><td>41 a 50 anos</td><td>PESSOA FISICA</td><td>Procurador</td><td>S&oacute;cio Pessoa F&iacute;si...</td><td>2020-11-12</td><td>ROBERT</td><td>1.0</td><td>M</td></tr>\n",
       "<tr><td>22677034</td><td>OLIVIER JEAN FRAN...</td><td>***597561**</td><td>***273891**</td><td>MARIA AURIA SOARE...</td><td>FRANCA</td><td>71 a 80 anos</td><td>PESSOA FISICA</td><td>Procurador</td><td>S&oacute;cio Pessoa F&iacute;si...</td><td>2015-06-18</td><td>OLIVIER</td><td>1.0</td><td>M</td></tr>\n",
       "<tr><td>02173216</td><td>ENZO JOSE PERSANO</td><td>***652719**</td><td>***550791**</td><td>STEFANO SCOVOLI</td><td>ARGENTINA</td><td>61 a 70 anos</td><td>PESSOA FISICA</td><td>Procurador</td><td>S&oacute;cio Pessoa F&iacute;si...</td><td>1999-01-10</td><td>ENZO</td><td>1.0</td><td>M</td></tr>\n",
       "<tr><td>60383031</td><td>RAFAEL SANZIO DE ...</td><td>***327807**</td><td>***228367**</td><td>SIMONE AVANY MEND...</td><td>ESTADOS UNIDOS</td><td>51 a 60 anos</td><td>PESSOA FISICA</td><td>Procurador</td><td>S&oacute;cio Pessoa F&iacute;si...</td><td>2022-04-20</td><td>RAFAEL</td><td>1.0</td><td>M</td></tr>\n",
       "</table>\n",
       "only showing top 10 rows\n"
      ],
      "text/plain": [
       "+-----------+-----------------------+--------------+-------------------+--------------------+--------------------+---------------+----------------------+-----------------------------------+---------------------+--------------------+-------------+---------------------------+-------------+\n",
       "|CNPJ_BASICO|NOME_SOCIO_RAZAO_SOCIAL|CNPJ_CPF_SOCIO|REPRESENTANTE_LEGAL|  NOME_REPRESENTANTE|             NM_PAIS|NM_FAIXA_ETARIA|NM_IDENTIFICADOR_SOCIO|NM_QUALIFICACAO_REPRESENTANTE_LEGAL|NM_QUALIFICAÇAO_SOCIO|DT_ENTRADA_SOCIEDADE|PRIMEIRO_NOME|PROBABILIDADE_CLASSIFICACAO|CLASSIFICACAO|\n",
       "+-----------+-----------------------+--------------+-------------------+--------------------+--------------------+---------------+----------------------+-----------------------------------+---------------------+--------------------+-------------+---------------------------+-------------+\n",
       "|   09218183|   MARIA LEONOR VIAN...|   ***392041**|        ***807143**|MARCOS ALEXANDRE ...|            PORTUGAL|   61 a 70 anos|         PESSOA FISICA|                         Procurador| Sócio Pessoa Físi...|          2007-11-23|        MARIA|                        1.0|            F|\n",
       "|   26252608|   DANIEL ROSA BALSE...|   ***758757**|        ***309658**|FREDERICO ANTONIO...|      ESTADOS UNIDOS|   31 a 40 anos|         PESSOA FISICA|                         Procurador| Sócio Pessoa Físi...|          2016-09-28|       DANIEL|                        1.0|            M|\n",
       "|   42262252|   SUZANE MAYUMI IAM...|   ***231718**|        ***346238**|CAMILA MAKIKO IAM...|         REINO UNIDO|   41 a 50 anos|         PESSOA FISICA|                         Procurador| Sócio Pessoa Físi...|          2021-06-09|       SUZANE|                        1.0|            F|\n",
       "|   07984125|   VALTER JORGE PERE...|   ***703883**|        ***052553**|LUISA DE MARILAC ...|            PORTUGAL|   61 a 70 anos|         PESSOA FISICA|                         Procurador| Sócio Pessoa Físi...|          2006-05-09|       VALTER|                        1.0|            M|\n",
       "|   08247240|          CLAUDIO CONTI|   ***461244**|        ***907894**|ROBERTO CARLOS NE...|               SUICA|   61 a 70 anos|         PESSOA FISICA|                         Procurador| Sócio Pessoa Físi...|          2013-10-16|      CLAUDIO|                        1.0|            M|\n",
       "|   29910911|   MARCELO ALVES BAR...|   ***015417**|        ***369987**|JOAO CAMILO DE AS...|      ESTADOS UNIDOS|   51 a 60 anos|         PESSOA FISICA|                         Procurador| Sócio Pessoa Físi...|          2018-03-12|      MARCELO|                        1.0|            M|\n",
       "|   39768913|   ROBERT MITCHELL V...|   ***436518**|        ***194698**|RENATO PREVIATO ROJA|Países Baixos (Ho...|   41 a 50 anos|         PESSOA FISICA|                         Procurador| Sócio Pessoa Físi...|          2020-11-12|       ROBERT|                        1.0|            M|\n",
       "|   22677034|   OLIVIER JEAN FRAN...|   ***597561**|        ***273891**|MARIA AURIA SOARE...|              FRANCA|   71 a 80 anos|         PESSOA FISICA|                         Procurador| Sócio Pessoa Físi...|          2015-06-18|      OLIVIER|                        1.0|            M|\n",
       "|   02173216|      ENZO JOSE PERSANO|   ***652719**|        ***550791**|     STEFANO SCOVOLI|           ARGENTINA|   61 a 70 anos|         PESSOA FISICA|                         Procurador| Sócio Pessoa Físi...|          1999-01-10|         ENZO|                        1.0|            M|\n",
       "|   60383031|   RAFAEL SANZIO DE ...|   ***327807**|        ***228367**|SIMONE AVANY MEND...|      ESTADOS UNIDOS|   51 a 60 anos|         PESSOA FISICA|                         Procurador| Sócio Pessoa Físi...|          2022-04-20|       RAFAEL|                        1.0|            M|\n",
       "+-----------+-----------------------+--------------+-------------------+--------------------+--------------------+---------------+----------------------+-----------------------------------+---------------------+--------------------+-------------+---------------------------+-------------+\n",
       "only showing top 10 rows"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_socios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea57b3d4-03c8-44ab-a92b-884074d8d160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Detected encodings: {'./output\\\\Simples\\\\Simples.csv': 'ascii'}\n"
     ]
    }
   ],
   "source": [
    "receitaLT_processor = ReceitaLT(spark)\n",
    "df = receitaLT_processor.read_data(schema_name='simples').cache()\n",
    "df_simples = receitaLT_processor.process_simples(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ed9852e-a0d3-40de-a529-d2483507ce1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>CNPJ_BASICO</th><th>OP&Ccedil;AO_PELO_MEI</th><th>DT_OPCAO_MEI</th><th>DT_EXCLUSAO_MEI</th><th>OPCAO_PELO_SIMPLES</th><th>DT_OPCAO_SIMPLES</th><th>DT_EXCLUSAO_SIMPLES</th></tr>\n",
       "<tr><td>00000000</td><td>N</td><td>2009-07-01</td><td>2009-07-01</td><td>N</td><td>2007-07-01</td><td>2007-07-01</td></tr>\n",
       "<tr><td>00000006</td><td>N</td><td>NULL</td><td>NULL</td><td>N</td><td>2018-01-01</td><td>2019-12-31</td></tr>\n",
       "<tr><td>00000008</td><td>N</td><td>NULL</td><td>NULL</td><td>N</td><td>2014-01-01</td><td>2021-12-31</td></tr>\n",
       "<tr><td>00000011</td><td>N</td><td>NULL</td><td>NULL</td><td>S</td><td>2007-07-01</td><td>NULL</td></tr>\n",
       "<tr><td>00000013</td><td>N</td><td>NULL</td><td>NULL</td><td>S</td><td>2009-01-01</td><td>NULL</td></tr>\n",
       "<tr><td>00000015</td><td>N</td><td>NULL</td><td>NULL</td><td>N</td><td>2007-07-01</td><td>2008-12-31</td></tr>\n",
       "<tr><td>00000030</td><td>N</td><td>NULL</td><td>NULL</td><td>N</td><td>2015-01-01</td><td>2015-12-31</td></tr>\n",
       "<tr><td>00000040</td><td>N</td><td>NULL</td><td>NULL</td><td>S</td><td>2007-07-01</td><td>NULL</td></tr>\n",
       "<tr><td>00000041</td><td>N</td><td>NULL</td><td>NULL</td><td>N</td><td>2007-07-01</td><td>2015-11-03</td></tr>\n",
       "<tr><td>00000056</td><td>N</td><td>NULL</td><td>NULL</td><td>N</td><td>2012-01-01</td><td>2014-12-03</td></tr>\n",
       "</table>\n",
       "only showing top 10 rows\n"
      ],
      "text/plain": [
       "+-----------+--------------+------------+---------------+------------------+----------------+-------------------+\n",
       "|CNPJ_BASICO|OPÇAO_PELO_MEI|DT_OPCAO_MEI|DT_EXCLUSAO_MEI|OPCAO_PELO_SIMPLES|DT_OPCAO_SIMPLES|DT_EXCLUSAO_SIMPLES|\n",
       "+-----------+--------------+------------+---------------+------------------+----------------+-------------------+\n",
       "|   00000000|             N|  2009-07-01|     2009-07-01|                 N|      2007-07-01|         2007-07-01|\n",
       "|   00000006|             N|        NULL|           NULL|                 N|      2018-01-01|         2019-12-31|\n",
       "|   00000008|             N|        NULL|           NULL|                 N|      2014-01-01|         2021-12-31|\n",
       "|   00000011|             N|        NULL|           NULL|                 S|      2007-07-01|               NULL|\n",
       "|   00000013|             N|        NULL|           NULL|                 S|      2009-01-01|               NULL|\n",
       "|   00000015|             N|        NULL|           NULL|                 N|      2007-07-01|         2008-12-31|\n",
       "|   00000030|             N|        NULL|           NULL|                 N|      2015-01-01|         2015-12-31|\n",
       "|   00000040|             N|        NULL|           NULL|                 S|      2007-07-01|               NULL|\n",
       "|   00000041|             N|        NULL|           NULL|                 N|      2007-07-01|         2015-11-03|\n",
       "|   00000056|             N|        NULL|           NULL|                 N|      2012-01-01|         2014-12-03|\n",
       "+-----------+--------------+------------+---------------+------------------+----------------+-------------------+\n",
       "only showing top 10 rows"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d33a54-3867-472b-87b3-d86740fb2d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\pedro\\Documents\\Curso de pos graduação de EST\\DADOS_CNPJ\\output\\tratado\\prefixo'\n",
    "receitaLT_processor.save_data(df_motivos, path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
