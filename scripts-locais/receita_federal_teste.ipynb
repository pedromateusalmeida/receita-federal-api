{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb5e9e91-2c23-47f6-9d8a-704b154caa51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ======================\n",
    "# Bibliotecas padrão para utilidades básicas e manipulação de arquivos:\n",
    "# ======================\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import zipfile\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ======================\n",
    "# Registros de logs:\n",
    "# ======================\n",
    "import logging\n",
    "\n",
    "# ======================\n",
    "# Requisições e manipulação de conteúdo web:\n",
    "# ======================\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ======================\n",
    "# Utilidades e manipulação de dados:\n",
    "# ======================\n",
    "from collections import Counter\n",
    "import chardet\n",
    "import string\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import shutil\n",
    "#import pycep_correios  # Descomente se necessário\n",
    "import brazilcep\n",
    "\n",
    "# ======================\n",
    "# Manipulação de datas:\n",
    "# ======================\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# ======================\n",
    "# Criptografia:\n",
    "# ======================\n",
    "import secrets\n",
    "import base64\n",
    "#from Crypto.Cipher import AES\n",
    "#from Crypto.Random import get_random_bytes\n",
    "\n",
    "# ======================\n",
    "# Multitarefas:\n",
    "# ======================\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# ======================\n",
    "# Spark:\n",
    "# ======================\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import (\n",
    "    regexp_replace,\n",
    "    when,\n",
    "    length,\n",
    "    to_date,\n",
    "    upper,\n",
    "    lower,\n",
    "    col,\n",
    "    udf,\n",
    "    split,\n",
    "    explode,\n",
    "    coalesce,\n",
    "    concat_ws,\n",
    "    concat,\n",
    "    lit,\n",
    "    broadcast,\n",
    "    regexp_extract,\n",
    "    expr\n",
    ")\n",
    "import pyspark.sql.functions as F\n",
    "from functools import reduce\n",
    "\n",
    "# ======================\n",
    "# Geolocalização:\n",
    "# ======================\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b83b4476-9d8c-4320-9b1c-6106a41a3a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a5add14-6a82-4bfb-893e-51926de1bd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bibliotecas padrão: Essas são bibliotecas padrão do Python para operações gerais e manipulação de arquivos.\n",
    "\n",
    "#Registros de logs: Para fazer registros de logs.\n",
    "\n",
    "#Requisições e manipulação de conteúdo web: Usado para fazer requisições web e manipular conteúdo HTML/XML.\n",
    "\n",
    "#Utilidades e manipulação de dados: Conjunto diversificado de bibliotecas para manipulação de dados, codificação de arquivos e outras utilidades.\n",
    "\n",
    "#Manipulação de datas: Para cálculos e manipulações relacionados a datas.\n",
    "\n",
    "#Criptografia: Utilitários e bibliotecas para lidar com criptografia.\n",
    "\n",
    "#Multitarefas: Permite a execução paralela de tarefas.\n",
    "\n",
    "#Spark: Todas as importações relacionadas ao PySpark.\n",
    "\n",
    "#Geolocalização: Bibliotecas para geolocalização e geocodificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62032f92-b64c-45ba-ad13-a96ae536d2c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\spark-3.5.0-bin-hadoop3\n",
      "C:\\Users\\pedro\\hadoop3.0\n",
      "C:\\Program Files\\Java\\jdk1.8.0_202\n"
     ]
    }
   ],
   "source": [
    "print(os.environ.get(\"SPARK_HOME\"))\n",
    "print(os.environ.get(\"HADOOP_HOME\"))\n",
    "print(os.environ.get(\"JAVA_HOME\"))\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c84a0-bdb1-4aca-9c85-87eb63788c04",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configuração do Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87126c24-d22e-4b01-8612-b6016c02c925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RAM: 32GB\n",
    "# CPU: 6 núcleos (12 threads)\n",
    "# Storage: SSD de 1TB\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .master(\"local[*]\")  # Utilizar todos os 12 threads disponíveis\n",
    "    .config(\"spark.driver.cores\", \"3\")  # Alocar metade dos núcleos para o driver\n",
    "    .config(\"spark.driver.memory\", \"12g\")  # Alocar 12GB para a memória do driver\n",
    "    .config(\"spark.default.parallelism\", \"32\")  # Paralelismo padrão baseado no número de threads\n",
    "    .config(\"spark.executor.cores\", \"2\")  # Como está em modo local, o executor pode usar metade dos núcleos\n",
    "    .config(\"spark.executor.instances\", \"3\")  # Em modo local, você geralmente tem apenas 1 instância de executor\n",
    "    .config(\"spark.executor.memory\", \"6g\")  # Alocar 16GB para a memória do executor\n",
    "    .config(\"spark.memory.fraction\", \"0.8\")  # Fraction da heap memory a ser usada para armazenamento e cache\n",
    "    .config(\"spark.memory.storageFraction\", \"0.5\")  # Fraction da memória de armazenamento que é reservada como memória não imune a evicção\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\")  # Habilitar memória off-heap\n",
    "    .config(\"spark.memory.offHeap.size\", \"2g\")  # Alocar 4GB para off-heap\n",
    "    .config(\"spark.executor.memoryOverhead\", \"1g\")  # Overhead de memória fora do heap para o executor. Se não for definido, Spark calculará um valor padrão\n",
    "    .config('spark.sql.repl.eagerEval.enabled', True)\n",
    "    .config(\"spark.sql.repl.eagerEval.maxNumRows\", 10)\n",
    "    .config(\"spark.shuffle.compress\", \"true\")   \n",
    "    .config(\"spark.storage.level\", \"MEMORY_AND_DISK\")   \n",
    "    .config(\"spark.rdd.compress\", \"true\")   \n",
    "    .appName('dataset_cnpj')\n",
    "    .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f18e528-a78a-4606-b2a2-e9f8d1c7b820",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "    .master(\"local[8]\")  # Use all 8 threads.\n",
    "    .config(\"spark.driver.cores\", \"3\")  # Use half the threads for the driver.\n",
    "    .config(\"spark.driver.memory\", \"8g\")  # Allocate 12GB to the driver to leave room for the OS and other processes.\n",
    "    .config(\"spark.default.parallelism\", \"16\")  # Default parallelism, you can adjust based on your data and tasks. Double the thread count is a good start.\n",
    "    .config(\"spark.executor.cores\", \"2\")  # Use 4 cores per executor.\n",
    "    .config(\"spark.executor.instances\", \"1\")  # Since it's local mode, only one executor instance.\n",
    "    .config(\"spark.executor.memory\", \"4g\")  # Assign 8GB for executor memory. Adjust based on your needs.\n",
    "    .config(\"spark.memory.fraction\", \"0.7\")  # Fraction of (heap space - 300MB) used for execution and storage. Adjust if needed.\n",
    "    .config(\"spark.memory.storageFraction\", \"0.5\")  # Amount of storage memory immune to eviction, expressed as a fraction of the size of the region set aside by spark.memory.fraction.\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\")  \n",
    "    .config(\"spark.memory.offHeap.size\", \"1g\")  # 3GB off-heap memory.\n",
    "    .config(\"spark.executor.memoryOverhead\", \"1g\")  # Overhead memory. You might need to adjust depending on your tasks.\n",
    "    .config('spark.sql.repl.eagerEval.enabled', True)\n",
    "    .config(\"spark.sql.repl.eagerEval.maxNumRows\", 10)\n",
    "    .appName('dataset_cnpj')\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fd92aef-8048-4c0e-9d72-fdf76dfd96a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>dataset_cnpj</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x29e7cc6d610>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee053cf7-d2fe-47f6-b844-c7659c9b316e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting python-magic\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Installing collected packages: python-magic\n",
      "Successfully installed python-magic-0.4.27\n"
     ]
    }
   ],
   "source": [
    "pip install python-magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20f2d2d6-083e-46fe-b08f-bf68650a459e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-magic-bin\n",
      "  Downloading python_magic_bin-0.4.14-py2.py3-none-win_amd64.whl (409 kB)\n",
      "     -------------------------------------- 409.3/409.3 kB 8.5 MB/s eta 0:00:00\n",
      "Installing collected packages: python-magic-bin\n",
      "Successfully installed python-magic-bin-0.4.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-magic-bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd7799c1-e915-4333-b50d-b867406edc8a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_22072/4068691271.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\PEDRO~1.ALM\\AppData\\Local\\Temp/ipykernel_22072/4068691271.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    pip install chardet\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install chardet\n",
    "pip install python-magic\n",
    "pip install python-magic-bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9965f590-1262-4760-b18f-cf270e1c335e",
   "metadata": {},
   "source": [
    "# Classe extração receita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89a99485-e01b-4782-8acb-1fa127a07660",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import os\n",
    "import chardet\n",
    "import logging\n",
    "import glob\n",
    "import secrets\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import regexp_replace, when,length,to_date,upper,lower,col,split,explode,coalesce,concat_ws,concat,lit,broadcast,regexp_extract,month,year,to_date\n",
    "from pyspark.sql.functions import broadcast,expr,udf\n",
    "#from geopy.geocoders import Nominatim\n",
    "#from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "   \n",
    "def geocode_address(address):\n",
    "    \"\"\"\n",
    "    Geocodifica um endereço, convertendo-o em coordenadas de latitude e longitude.\n",
    "\n",
    "    Parâmetros:\n",
    "        address (str): Endereço a ser geocodificado.\n",
    "\n",
    "    Retorna:\n",
    "        tuple: Um par contendo a latitude e a longitude do endereço fornecido. \n",
    "                Se o endereço não puder ser geocodificado, retorna (None, None).\n",
    "\n",
    "    Exemplo:\n",
    "        lat, lon = geocode_address(\"1600 Amphitheatre Parkway, Mountain View, CA\")\n",
    "\n",
    "    Notas:\n",
    "        - Usa o serviço Nominatim para a geocodificação.\n",
    "        - Incorpora um limitador de taxa para garantir que não excedamos os limites de requisições por segundo \n",
    "            do serviço.\n",
    "    \"\"\"\n",
    "    geolocator = Nominatim(user_agent=\"CNPJ_GEOLOCATION\")\n",
    "    geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n",
    "    location = geocode(address)\n",
    "    if location:\n",
    "        return (location.latitude, location.longitude)\n",
    "    else:\n",
    "        return (None, None)\n",
    "\n",
    "\n",
    "class ReceitaLT:\n",
    "    \"\"\"\n",
    "    A classe `ReceitaLT` facilita a manipulação e análise de dados da Receita Federal do Brasil.\n",
    "\n",
    "    Atributos:\n",
    "        spark (SparkSession): Sessão Spark para manipulação de dataframes.\n",
    "        logger (Logger): Logger para capturar e exibir logs.\n",
    "        \n",
    "    Atributos estáticos:\n",
    "        - estabelecimentos: Schema para dados de estabelecimentos.\n",
    "        - empresas: Schema para dados das empresas.\n",
    "        - municipios: Schema para municípios.\n",
    "        - cnaes: Schema para CNAEs.\n",
    "        - paises: Schema para países.\n",
    "        - qualificacoes: Schema para qualificações.\n",
    "        - socios: Schema para sócios.\n",
    "        - simples: Schema para opções do Simples Nacional.\n",
    "        - naturezas: Schema para naturezas jurídicas.\n",
    "        - motivos: Schema para motivos de situações cadastrais.\n",
    "        - dic_provedor: Dicionário para correção de nomes de provedores de email.\n",
    "        \n",
    "    Métodos:\n",
    "        detect_encoding(file_pattern_or_path, num_bytes=10000): Detecta a codificação do arquivo ou arquivos fornecidos.\n",
    "\n",
    "    Uso:\n",
    "        1. Instancie a classe com uma sessão Spark.\n",
    "        2. Utilize os schemas estáticos para leitura de arquivos.\n",
    "        3. Use o método `detect_encoding` para determinar a codificação de arquivos antes de lê-los.\n",
    "        \n",
    "    Exemplo:\n",
    "        from pyspark.sql import SparkSession\n",
    "        \n",
    "        spark_session = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "        receita_helper = ReceitaLT(spark_session)\n",
    "        encodings = receita_helper.detect_encoding(\"path/to/datafile.csv\")\n",
    "        df = spark_session.read.csv(\"path/to/datafile.csv\", schema=ReceitaLT.empresas, encoding=encodings[\"path/to/datafile.csv\"])\n",
    "    \"\"\"\n",
    "    def __init__(self, spark: SparkSession):\n",
    "        \"\"\"\n",
    "        Inicializa a classe ReceitaLT.\n",
    "        \n",
    "        Parâmetros:\n",
    "        spark (SparkSession): Uma sessão Spark ativa.\n",
    "        \"\"\"\n",
    "        self.spark = spark\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        logging.basicConfig(level=logging.INFO) \n",
    "        \n",
    "    # Definindo os schemas:\n",
    "    estabelecimentos = StructType([\n",
    "        StructField(\"CNPJ_BASICO\", StringType(), nullable=True),\n",
    "        StructField(\"CNPJ_ORDEM\", StringType(), nullable=True),\n",
    "        StructField(\"CNPJ_DV\", StringType(), nullable=True),\n",
    "        StructField(\"MATRIZ_FILIAL\", StringType(), nullable=True),\n",
    "        StructField(\"NOME_FANTASIA\", StringType(), nullable=True),\n",
    "        StructField(\"SIT_CADASTRAL\", IntegerType(), nullable=True),\n",
    "        StructField(\"DT_SIT_CADASTRAL\", StringType(), nullable=True),\n",
    "        StructField(\"MOTIVO_CADASTRAL\", StringType(), nullable=True),\n",
    "        StructField(\"NOME_CIDADE_EXTERIOR\", StringType(), nullable=True),\n",
    "        StructField(\"PAIS\", StringType(), nullable=True),\n",
    "        StructField(\"DT_INICIO_ATIVIDADE\", StringType(), nullable=True),\n",
    "        StructField(\"CNAE_1\", StringType(), nullable=True),\n",
    "        StructField(\"CNAE_2\", StringType(), nullable=True),\n",
    "        StructField(\"TIPO_LOUGRADOURO\", StringType(), nullable=True),\n",
    "        StructField(\"LOGRADOURO\", StringType(), nullable=True),\n",
    "        StructField(\"NUMERO\", IntegerType(), nullable=True),\n",
    "        StructField(\"COMPLEMENTO\", StringType(), nullable=True),\n",
    "        StructField(\"BAIRRO\", StringType(), nullable=True),\n",
    "        StructField(\"CEP\", IntegerType(), nullable=True),\n",
    "        StructField(\"UF\", StringType(), nullable=True),\n",
    "        StructField(\"MUNICIPIO\", StringType(), nullable=True),\n",
    "        StructField(\"DDD1\", StringType(), nullable=True),\n",
    "        StructField(\"TEL1\", StringType(), nullable=True),\n",
    "        StructField(\"DDD2\", StringType(), nullable=True),\n",
    "        StructField(\"TEL2\", StringType(), nullable=True),\n",
    "        StructField(\"DDD_FAX\", IntegerType(), nullable=True),\n",
    "        StructField(\"FAX\", IntegerType(), nullable=True),\n",
    "        StructField(\"EMAIL\", StringType(), nullable=True),\n",
    "        StructField(\"SIT_ESPECIAL\", StringType(), nullable=True),\n",
    "        StructField(\"DT_SIT_ESPECIAL\", StringType(), nullable=True)])\n",
    "\n",
    "    empresas = StructType([\n",
    "        StructField(\"CNPJ\", StringType(), nullable=True),\n",
    "        StructField(\"NOME_EMPRESA\", StringType(), nullable=True),\n",
    "        StructField(\"COD_NAT_JURICA\", StringType(), nullable=True),\n",
    "        StructField(\"QUALIF_RESPONVAVEL\", StringType(), nullable=True),\n",
    "        StructField(\"CAP_SOCIAL\", StringType(), nullable=True),\n",
    "        StructField(\"PORTE\", StringType(), nullable=True),\n",
    "        StructField(\"ENTE_FEDERATIVO\", StringType(), nullable=True)])\n",
    "\n",
    "    municipios = StructType([\n",
    "        StructField(\"ID_MUNICPIO\", StringType(), nullable=True),\n",
    "        StructField(\"MUNICIPIO\", StringType(), nullable=True)])\n",
    "\n",
    "    cnaes = StructType([\n",
    "        StructField(\"COD_CNAE\", StringType(), nullable=True),\n",
    "        StructField(\"CNAE\", StringType(), nullable=True)])\n",
    "    \n",
    "    paises = StructType([\n",
    "        StructField(\"COD_PAIS\", StringType(), nullable=True),\n",
    "        StructField(\"NM_PAIS\", StringType(), nullable=True)])\n",
    "    \n",
    "    qualificacoes = StructType([\n",
    "        StructField(\"COD_QUALIFICACAO\", StringType(), nullable=True),\n",
    "        StructField(\"NM_QUALIFICACAO\", StringType(), nullable=True)])\n",
    "\n",
    "    socios = StructType([\n",
    "        StructField(\"CNPJ_BASICO\", StringType(), nullable=True),\n",
    "        StructField(\"IDENTIFICADOR_SOCIO\", IntegerType(), nullable=True),\n",
    "        StructField(\"NOME_SOCIO_RAZAO_SOCIAL\", StringType(), nullable=True),\n",
    "        StructField(\"CNPJ_CPF_SOCIO\", StringType(), nullable=True),\n",
    "        StructField(\"QUALIFICAÇAO_SOCIO\", StringType(), nullable=True),\n",
    "        StructField(\"DATA_ENTRADA_SOCIEDADE\", StringType(), nullable=True),\n",
    "        StructField(\"PAIS\", StringType(), nullable=True),\n",
    "        StructField(\"REPRESENTANTE_LEGAL\", StringType(), nullable=True),\n",
    "        StructField(\"NOME_REPRESENTANTE\", StringType(), nullable=True),\n",
    "        StructField(\"QUALIFICACAO_REPRESENTANTE_LEGAL\", StringType(), nullable=True),\n",
    "        StructField(\"FAIXA_ETARIA\", StringType(), nullable=True)])\n",
    "\n",
    "    simples = StructType([\n",
    "        StructField(\"CNPJ_BASICO\", StringType(), nullable=True),\n",
    "        StructField(\"OPCAO_PELO_SIMPLES\", StringType(), nullable=True),\n",
    "        StructField(\"DATA_OPCAO_PELO_SIMPLES\", StringType(), nullable=True),\n",
    "        StructField(\"DATA_EXCLUSAO_SIMPLES\", StringType(), nullable=True),\n",
    "        StructField(\"OPÇAO_PELO_MEI\", StringType(), nullable=True),\n",
    "        StructField(\"DATA_OPCAO_PELO_MEI\", StringType(), nullable=True),\n",
    "        StructField(\"DATA_EXCLUSAO_MEI\", StringType(), nullable=True)])\n",
    "\n",
    "    naturezas = StructType([\n",
    "        StructField(\"COD_NAT_JURICA\", StringType(), nullable=True),\n",
    "        StructField(\"NAT_JURICA\", StringType(), nullable=True)])\n",
    "    \n",
    "    motivos = StructType([\n",
    "        StructField(\"COD_MOTIVO\", StringType(), nullable=True),\n",
    "        StructField(\"NM_MOTIVO\", StringType(), nullable=True)])\n",
    "    \n",
    "    dic_provedor = {'0UTLOOK': 'OUTLOOK', '123GMAIL': 'GMAIL', '12GMAIL': 'GMAIL', '19GMAIL': 'GMAIL', '1HOTMAIL': 'HOTMAIL', \n",
    "                    '2010HOTMAIL': 'HOTMAIL', '20GMAIL': 'GMAIL', '23GMAIL': 'GMAIL', '2GMAIL': 'GMAIL', '2HOTMAIL': 'HOTMAIL', \n",
    "                    '30GMAIL': 'GMAIL', '7GMAIL': 'GMAIL', 'ADV': 'ADV', 'AGMAIL': 'GMAIL', 'AHOO': 'YAHOO', 'AIL': 'AOL', 'ALUNO': 'ALUNO', \n",
    "                    'AOL': 'AOL', 'AUTLOOK': 'OUTLOOK', 'BB': 'BB', 'BOL': 'BOL', 'BOLL': 'BOL', 'BOOL': 'BOL', 'BRTURBO': 'OI', \n",
    "                    'CAIXA': 'CAIXA', 'CLICK21': 'CLICK21', 'CLOUD': 'ICLOUD', 'CRECI': 'CRECI', 'EDU': 'EDU', 'EMAIL': 'EMAIL', \n",
    "                    'FACEBOOK': 'FACEBOOK', 'FMAIL': 'GMAIL', 'G': 'GMAIL', 'G-MAIL': 'GMAIL', 'GAIL': 'GMAIL', 'GAMAIL': 'GMAIL', \n",
    "                    'GAMEIL': 'GMAIL', 'GAMIAL': 'GMAIL', 'GAMIL': 'GMAIL', 'GEMAIL': 'GMAIL', 'GGMAIL': 'GMAIL', 'GHMAIL': 'GMAIL', \n",
    "                    'GHOTMAIL': 'HOTMAIL', 'GIMAIL': 'GMAIL', 'GLOBO': 'GLOBO', 'GLOBOMAIL': 'LWMAIL', 'GMA': 'GMAIL', 'GMAAIL': 'GMAIL', \n",
    "                    'GMAI': 'GMAIL', 'GMAIAL': 'GMAIL', 'GMAII': 'GMAIL', 'GMAIIL': 'GMAIL', 'GMAIK': 'GMAIL', 'GMAIL': 'GMAIL', \n",
    "                    'GMAILC': 'GMAIL', 'GMAILGMAIL': 'GMAIL', 'GMAILL': 'GMAIL', 'GMAILMAIL': 'GMAIL', 'GMAILO': 'GMAIL', 'GMAIM': 'GMAIL', \n",
    "                    'GMAIO': 'GMAIL', 'GMAIOL': 'GMAIL', 'GMAIS': 'GMAIL', 'GMAISL': 'GMAIL', 'GMAIUL': 'GMAIL', 'GMAL': 'GMAIL', \n",
    "                    'GMALI': 'GMAIL', 'GMAOL': 'GMAIL', 'GMAQIL': 'GMAIL', 'GMASIL': 'GMAIL', 'GMAUIL': 'GMAIL', 'GMAUL': 'GMAIL',\n",
    "                    'GMEIL': 'GMAIL', 'GMIAL': 'GMAIL', 'GMIL': 'GMAIL', 'GML': 'GMAIL', 'GMMAIL': 'GMAIL', 'GMNAIL': 'GMAIL', \n",
    "                    'GMQIL': 'GMAIL', 'GMSIL': 'GMAIL', 'GNAIL': 'GMAIL', 'GNMAIL': 'GMAIL', 'GOMAIL': 'GMAIL', 'GOOGLEMAIL': 'GMAIL',\n",
    "                    'GOTMAIL': 'HOTMAIL', 'GTMAIL': 'GMAIL', 'H0TMAIL': 'HOTMAIL', 'HAHOO': 'YAHOO', 'HATMAIL': 'HOTMAIL', 'HAYOO': 'YAHOO', \n",
    "                    'HGMAIL': 'GMAIL', 'HHOTMAIL': 'HOTMAIL', 'HIOTMAIL': 'HOTMAIL', 'HITMAIL': 'HOTMAIL', 'HJOTMAIL': 'HOTMAIL', \n",
    "                    'HMAIL': 'HOTMAIL', 'HOITMAIL': 'HOTMAIL', 'HOLMAIL': 'HOTMAIL', 'HOLTMAIL': 'HOTMAIL', 'HOMAIL': 'HOTMAIL', \n",
    "                    'HOMTAIL': 'HOTMAIL', 'HOMTIAL': 'HOTMAIL', 'HOMTMAIL': 'HOTMAIL', 'HOOTMAIL': 'HOTMAIL', 'HOPTMAIL': 'HOTMAIL', \n",
    "                    'HORMAIL': 'HOTMAIL', 'HORTMAIL': 'HOTMAIL', 'HOT': 'HOTMAIL', 'HOTAIL': 'HOTMAIL', 'HOTAMAIL': 'HOTMAIL', \n",
    "                    'HOTAMIL': 'HOTMAIL', 'HOTEMAIL': 'HOTMAIL', 'HOTGMAIL': 'HOTMAIL', 'HOTIMAIL': 'HOTMAIL', 'HOTIMAL': 'HOTMAIL', \n",
    "                    'HOTLMAIL': 'HOTMAIL', 'HOTLOOK': 'OUTLOOK', 'HOTMA': 'HOTMAIL', 'HOTMAAIL': 'HOTMAIL', 'HOTMAI': 'HOTMAIL', \n",
    "                    'HOTMAIAL': 'HOTMAIL', 'HOTMAII': 'HOTMAIL', 'HOTMAIIL': 'HOTMAIL', 'HOTMAIL': 'HOTMAIL', 'HOTMAILC': 'HOTMAIL', \n",
    "                    'HOTMAILL': 'HOTMAIL', 'HOTMAILO': 'HOTMAIL', 'HOTMAIM': 'HOTMAIL', 'HOTMAIO': 'HOTMAIL', 'HOTMAIOL': 'HOTMAIL', \n",
    "                    'HOTMAIUL': 'HOTMAIL', 'HOTMAL': 'HOTMAIL', 'HOTMALI': 'HOTMAIL', 'HOTMAMIL': 'HOTMAIL', 'HOTMAOL': 'HOTMAIL', \n",
    "                    'HOTMAQIL': 'HOTMAIL', 'HOTMASIL': 'HOTMAIL', 'HOTMAUIL': 'HOTMAIL', 'HOTMAUL': 'HOTMAIL', 'HOTMEIL': 'HOTMAIL', \n",
    "                    'HOTMIAIL': 'HOTMAIL', 'HOTMIAL': 'HOTMAIL', 'HOTMIL': 'HOTMAIL', 'HOTMMAIL': 'HOTMAIL', 'HOTMNAIL': 'HOTMAIL',\n",
    "                    'HOTMQIL': 'HOTMAIL', 'HOTMSIL': 'HOTMAIL', 'HOTNAIL': 'HOTMAIL', 'HOTOMAIL': 'HOTMAIL', 'HOTRMAIL': 'HOTMAIL', \n",
    "                    'HOTTMAIL': 'HOTMAIL', 'HOTYMAIL': 'HOTMAIL', 'HOUTLOOK': 'OUTLOOK', 'HOYMAIL': 'HOTMAIL', 'HPTMAIL': 'HOTMAIL', \n",
    "                    'HTMAIL': 'HOTMAIL', 'HTOMAIL': 'HOTMAIL', 'HYAHOO': 'YAHOO', 'IAHOO': 'YAHOO', 'IBEST': 'IBEST', 'ICLAUD': 'ICLOUD', \n",
    "                    'ICLOD': 'ICLOUD', 'ICLOID': 'ICLOUD', 'ICLOOD': 'ICLOUD', 'ICLOU': 'ICLOUD', 'ICLOUD': 'ICLOUD', 'ICLOUDE': 'ICLOUD', \n",
    "                    'ICLOULD': 'ICLOUD', 'ICLOUND': 'ICLOUD', 'ICLUD': 'ICLOUD', 'ICLUOD': 'ICLOUD', 'ICOUD': 'ICLOUD', 'ICOULD': 'ICLOUD', \n",
    "                    'ID': 'IG', 'IG': 'IG', 'IGMAIL': 'GMAIL', 'IGUI': 'IG', 'IMAIL': 'GMAIL', 'INCLOUD': 'ICLOUD', 'ITELEFONICA': 'ITELEFONICA',\n",
    "                    'JMAIL': 'GMAIL', 'JOTMAIL': 'HOTMAIL', 'LIVE': 'LIVE', 'LWMAIL': 'LWMAIL', 'MAIL': 'MAIL', 'ME': 'ME', 'MSM': 'MSN', \n",
    "                    'MSN': 'MSN', 'NETSITE': 'NETSITE', 'OI': 'OI', 'OIMAIL': 'HOTMAIL', 'OITLOOK': 'OUTLOOK', 'OLTLOOK': 'OUTLOOK', \n",
    "                    'OOUTLOOK': 'OUTLOOK', 'OTLOOK': 'OUTLOOK', 'OTMAIL': 'HOTMAIL', 'OUL': 'UOL', 'OULOOK': 'OUTLOOK', 'OULTLOOK': 'OUTLOOK',\n",
    "                    'OULTOOK': 'OUTLOOK', 'OUTILOOK': 'OUTLOOK', 'OUTIOOK': 'OUTLOOK', 'OUTLLOK': 'OUTLOOK', 'OUTLLOOK': 'OUTLOOK', \n",
    "                    'OUTLOCK': 'OUTLOOK', 'OUTLOK': 'OUTLOOK', 'OUTLOKK': 'OUTLOOK', 'OUTLOOCK': 'OUTLOOK', 'OUTLOOK': 'OUTLOOK', \n",
    "                    'OUTLOOKL': 'OUTLOOK', 'OUTLOOL': 'OUTLOOK', 'OUTLOOOK': 'OUTLOOK', 'OUTLUK': 'OUTLOOK', 'OUTOLOOK': 'OUTLOOK',\n",
    "                    'OUTOOK': 'OUTLOOK', 'OUTOOLK': 'OUTLOOK', 'OUTTLOOK': 'OUTLOOK', 'OUTULOOK': 'OUTLOOK', 'POP': 'POP',\n",
    "                    'PROTON': 'PROTONMAIL', 'PROTONMAIL': 'PROTONMAIL', 'PUTLOOK': 'OUTLOOK', 'R7': 'R7', 'ROCKETMAIL': 'ROCKETMAIL', \n",
    "                    'ROCKTMAIL': 'ROCKETMAIL', 'ROTMAIL': 'HOTMAIL', 'SERCOMTEL': 'SERCOMTEL', 'SETELAGOASGML': 'GMAIL', \n",
    "                    'SUPERIG': 'SUPERIG', 'TAHOO': 'YAHOO', 'TERRA': 'TERRA', 'TERRRA': 'TERRA', 'TMAIL': 'GMAIL', \n",
    "                    'TVGLOBO': 'GLOBO', 'UAHOO': 'YAHOO', 'UAI': 'UAI', 'UFV': 'UFV', 'UNESP': 'UNESP', 'UNOCHAPECO': 'UNOCHAPECO', \n",
    "                    'UO': 'UOL', 'UOL': 'UOL', 'UOTLOOK': 'OUTLOOK', 'UPF': 'UPF', 'USP': 'USP', 'UTLOOK': 'OUTLOOK', 'VELOXMAIL': 'VELOXMAIL',\n",
    "                    'WINDOWSLIVE': 'WINDOWSLIVE', 'YAAHOO': 'YAHOO', 'YAGOO': 'YAHOO', 'YAHAOO': 'YAHOO', 'YAHHO': 'YAHOO', 'YAHHOO': 'YAHOO', \n",
    "                    'YAHO': 'YAHOO', 'YAHOO': 'YAHOO', 'YAHOOCOM': 'YAHOO', 'YAHOOL': 'YAHOO', 'YAHOOO': 'YAHOO', 'YAHOOU': 'YAHOO', \n",
    "                    'YANHOO': 'YAHOO', 'YAOO': 'YAHOO', 'YAOOL': 'YAHOO', 'YAROO': 'YAHOO', 'YHAOO': 'YAHOO', 'YHOO': 'YAHOO', 'YMAIL': 'YMAIL', \n",
    "                    'YOHOO': 'YAHOO', 'YOPMAIL': 'HOTMAIL', 'ZIPMAIL': 'ZIPMAIL', '_HOTMAIL': 'HOTMAIL',     'GMAUL': 'GMAIL','GMALE': 'GMAIL', \n",
    "                    'GMAILE': 'GMAIL', 'GMILE': 'GMAIL', 'HOTMEL': 'HOTMAIL', 'HOTMELL': 'HOTMAIL','HOTMEAL': 'HOTMAIL', 'OUTLOKES': 'OUTLOOK', \n",
    "                    'OTLOOKS': 'OUTLOOK', 'YAHU': 'YAHOO', 'YOHU': 'YAHOO', 'YAHUO': 'YAHOO', 'YAHEE': 'YAHOO', 'UOLL': 'UOL',\n",
    "                    'UOOL': 'UOL', 'UULL': 'UOL', 'ICLODUE': 'ICLOUD', 'ICLAWD': 'ICLOUD', 'ROCKEDMAIL': 'ROCKETMAIL', 'ROKETMAIL': 'ROCKETMAIL',\n",
    "                    'OUTLOKE': 'OUTLOOK', 'OUTLOOCKE': 'OUTLOOK', 'YAAHO': 'YAHOO', 'YAHOOE': 'YAHOO', 'YAHUE': 'YAHOO', 'HOTMILE': 'HOTMAIL', 'HOTMELE': 'HOTMAIL',\n",
    "                    'FACEBOKE': 'FACEBOOK', 'FACBOOK': 'FACEBOOK', 'FCEBOOK': 'FACEBOOK', 'BOLL': 'BOL', 'BOLLE': 'BOL', 'BULE': 'BOL', 'GLOBOE': 'GLOBO',\n",
    "                    'GLOBU': 'GLOBO', 'GMILE': 'GMAIL', 'MSNE': 'MSN', 'MSNN': 'MSN', 'ICLOOUD': 'ICLOUD', 'OUTLUKE': 'OUTLOOK', 'OUTLLOKE': 'OUTLOOK',\n",
    "                    'GMAUL': 'GMAIL','GMALE': 'GMAIL', 'GMAILE': 'GMAIL', 'GMILE': 'GMAIL', 'HOTMEL': 'HOTMAIL', 'HOTMELL': 'HOTMAIL',\n",
    "                    'HOTMEAL': 'HOTMAIL', 'OUTLOKES': 'OUTLOOK', 'OTLOOKS': 'OUTLOOK', 'YAHU': 'YAHOO', 'YOHU': 'YAHOO', 'YAHUO': 'YAHOO', \n",
    "                    'YAHEE': 'YAHOO', 'UOLL': 'UOL', 'UOOL': 'UOL', 'UULL': 'UOL', 'ICLODUE': 'ICLOUD', 'ICLAWD': 'ICLOUD',  'ROCKEDMAIL': 'ROCKETMAIL',\n",
    "                    'ROKETMAIL': 'ROCKETMAIL', 'OUTLOKE': 'OUTLOOK', 'OUTLOOCKE': 'OUTLOOK', 'YAAHO': 'YAHOO', 'YAHOOE': 'YAHOO', 'YAHUE': 'YAHOO', \n",
    "                    'HOTMILE': 'HOTMAIL', 'HOTMELE': 'HOTMAIL', 'FACEBOKE': 'FACEBOOK', 'FACBOOK': 'FACEBOOK', 'FCEBOOK': 'FACEBOOK', 'BOLL': 'BOL',\n",
    "                    'BOLLE': 'BOL', 'BULE': 'BOL', 'GLOBOE': 'GLOBO',  'GLOBU': 'GLOBO', 'GMILE': 'GMAIL', 'MSNE': 'MSN', 'MSNN': 'MSN', 'ICLOOUD': 'ICLOUD',\n",
    "                    'OUTLUKE': 'OUTLOOK', 'OUTLLOKE': 'OUTLOOK', 'PROTONMIAL': 'PROTONMAIL',  'PROTONMALE': 'PROTONMAIL', 'PROTOMAIL': 'PROTONMAIL', \n",
    "                    'OULOOKCOM': 'OUTLOOK', 'YAHCOM': 'YAHOO',  'YAHOCOM': 'YAHOO','GAMILCOM': 'GMAIL', 'GMALCOM': 'GMAIL',  'HOTMALCOM': 'HOTMAIL',  \n",
    "                    'HOTMILCOM': 'HOTMAIL', 'HOTMELCOM': 'HOTMAIL', 'ROCKMAIL': 'ROCKETMAIL', 'ROKMAIL': 'ROCKETMAIL', 'TERA': 'TERRA', 'TEERA': 'TERRA', \n",
    "                    'FACBOOKCOM': 'FACEBOOK', 'FACEBOOKCOM': 'FACEBOOK', 'ICLOWD': 'ICLOUD', 'ICLOUND': 'ICLOUD', 'UOOLCOM': 'UOL', 'UOLLCOM': 'UOL', \n",
    "                    'UOLCOMBR': 'UOL','LIVECOM': 'LIVE', 'LIVECOMBR': 'LIVE', 'GMAICOM': 'GMAIL',  'GMAILCOMBR': 'GMAIL',  'YAHOOBR': 'YAHOO', \n",
    "                    'YAHOOOCOMBR': 'YAHOO', 'YAHOOOCOM': 'YAHOO', 'ZIPMAILE': 'ZIPMAIL', 'ZIPMAILL': 'ZIPMAIL',  'IBESTT': 'IBEST', 'IBESTE': 'IBEST'}\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def detect_encoding(file_pattern_or_path, num_bytes=10000):\n",
    "        \"\"\"\n",
    "        Detecta a codificação do arquivo ou arquivos fornecidos.\n",
    "        \n",
    "        Parâmetros:\n",
    "            file_pattern_or_path (str): Caminho ou padrão do arquivo para detecção.\n",
    "            num_bytes (int, opcional): Número de bytes para ler para a detecção. Padrão é 10000.\n",
    "        \n",
    "        Retorna:\n",
    "            dict: Dicionário com caminho do arquivo como chave e codificação detectada como valor.\n",
    "        \"\"\"\n",
    "        files = glob.glob(file_pattern_or_path)\n",
    "        encodings = {}\n",
    "        for file_path in files:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                rawdata = f.read(num_bytes)\n",
    "                encodings[file_path] = chardet.detect(rawdata)[\"encoding\"]\n",
    "        return encodings\n",
    "\n",
    "\n",
    "    def read_data(self, schema_name, base_path=None):\n",
    "        \"\"\"\n",
    "        Lê dados de vários arquivos CSV de acordo com o esquema e caminho base fornecidos, consolidando-os \n",
    "        em um único DataFrame do Spark.\n",
    "\n",
    "        Parâmetros:\n",
    "            schema_name (str): Nome do esquema a ser usado para a leitura dos arquivos.\n",
    "                               Deve ser uma das chaves do dicionário `schemas`.\n",
    "\n",
    "            base_path (str, opcional): Caminho base dos arquivos CSV.\n",
    "                                       Se não for fornecido, ele tentará buscar da variável de ambiente 'BASE_PATH'.\n",
    "                                       Caso não encontre, o padrão \"./output\" será utilizado.\n",
    "\n",
    "        Retorna:\n",
    "            DataFrame: DataFrame do Spark contendo os dados consolidados dos arquivos CSV.\n",
    "\n",
    "        Exceções:\n",
    "            Pode lançar uma exceção se o arquivo não estiver presente no caminho especificado ou\n",
    "            se houver problemas de codificação ao ler o arquivo.\n",
    "\n",
    "        Exemplo:\n",
    "            receita_helper = ReceitaLT(spark_session)\n",
    "            df = receita_helper.read_data(\"estabelecimentos\", \"/path/to/csv/files\")\n",
    "\n",
    "        Notas:\n",
    "            - A função primeiro detecta a codificação dos arquivos antes de lê-los para garantir que \n",
    "              eles sejam lidos corretamente.\n",
    "            - A função lida com múltiplos arquivos CSV e os une em um único DataFrame.\n",
    "            - O formato de arquivo assumido é CSV com delimitador \";\", sem cabeçalho e com aspas para delimitar campos.\n",
    "        \"\"\"\n",
    "        schemas = {\n",
    "            \"estabelecimentos\": self.estabelecimentos,\n",
    "            \"empresas\": self.empresas,\n",
    "            \"municipios\": self.municipios,\n",
    "            \"cnaes\": self.cnaes,\n",
    "            \"socios\": self.socios,\n",
    "            \"simples\": self.simples,\n",
    "            \"naturezas\": self.naturezas,\n",
    "            \"qualificacoes\": self.qualificacoes,\n",
    "            \"motivos\": self.motivos,\n",
    "            \"paises\": self.paises}\n",
    "\n",
    "        # Se o base_path não for fornecido, pegar da variável de ambiente ou usar um padrão.\n",
    "        if not base_path:\n",
    "            base_path = os.environ.get('BASE_PATH', \"./output\")\n",
    "\n",
    "        if schema_name in ['estabelecimentos', 'empresas', 'socios']:\n",
    "            file_location_pattern = os.path.join(base_path, schema_name.capitalize(), '*.csv')\n",
    "        else:\n",
    "            file_location_pattern = os.path.join(base_path, schema_name.capitalize(), f\"{schema_name.capitalize()}.csv\")\n",
    "\n",
    "        # Detectar codificações\n",
    "        encodings = self.detect_encoding(file_location_pattern)\n",
    "        self.logger.info(f\"Detected encodings: {encodings}\")\n",
    "\n",
    "        # Agora, vamos ler cada arquivo com sua codificação correta e armazenar em uma lista de DataFrames\n",
    "        dfs = []\n",
    "        for file_location, encoding in encodings.items():\n",
    "            df = (self.spark.read.format(\"csv\")\n",
    "                  .option(\"sep\", \";\")\n",
    "                  .option(\"header\", \"false\")\n",
    "                  .option('quote', '\"')\n",
    "                  .option(\"escape\", '\"')\n",
    "                  .option(\"encoding\", encoding)\n",
    "                  .schema(schemas[schema_name])\n",
    "                  .load(file_location))\n",
    "            dfs.append(df)\n",
    "\n",
    "        # Unir todos os DataFrames em um único DataFrame\n",
    "        if dfs:\n",
    "            final_df = reduce(lambda a, b: a.union(b), dfs)\n",
    "        else:\n",
    "            final_df = self.spark.createDataFrame([], schemas[schema_name])\n",
    "\n",
    "        return final_df\n",
    "    \n",
    "\n",
    "    # Define the UDF\n",
    "    schema = StructType([\n",
    "        StructField(\"latitude\", FloatType(), nullable=True),\n",
    "        StructField(\"longitude\", FloatType(), nullable=True)\n",
    "    ])\n",
    "    \n",
    "    @udf(schema)\n",
    "    def geocode_udf(address):\n",
    "        \"\"\"\n",
    "        UDF do Spark para geocodificar um endereço dentro de um DataFrame.\n",
    "\n",
    "        Parâmetros:\n",
    "            address (str): Endereço a ser geocodificado.\n",
    "\n",
    "        Retorna:\n",
    "            dict: Dicionário contendo a 'latitude' e a 'longitude' do endereço fornecido.\n",
    "                  Se o endereço não puder ser geocodificado, os valores serão None.\n",
    "\n",
    "        Exemplo:\n",
    "            df.withColumn(\"location\", geocode_udf(df[\"address\"]))\n",
    "\n",
    "        Notas:\n",
    "            - Esta UDF encapsula a função `geocode_address`.\n",
    "            - Retorna um tipo de dado complexo (Struct) com dois campos: 'latitude' e 'longitude'.\n",
    "        \"\"\"\n",
    "        global geocode_address\n",
    "        lat, lon = geocode_address(address)\n",
    "        return {\"latitude\": lat, \"longitude\": lon}\n",
    "    \n",
    "    def process_estabelecimentos(self, df):\n",
    "        \n",
    "        \"\"\"\n",
    "        Processa e enriquece o DataFrame de estabelecimentos com informações adicionais e transformações.\n",
    "\n",
    "        Parâmetros:\n",
    "            df (DataFrame): DataFrame inicial contendo informações de estabelecimentos.\n",
    "\n",
    "        Retorna:\n",
    "            DataFrame: DataFrame processado e enriquecido com novas colunas e informações.\n",
    "\n",
    "        Descrição:\n",
    "            - Lê dataframes adicionais relacionados a países, municípios, cnaes e motivos.\n",
    "            - Realiza renomeações de colunas para facilitar junções.\n",
    "            - Enriquece o dataframe com informações de motivos, cnaes, municípios e países.\n",
    "            - Processa colunas de e-mail, separando provedores e corrigindo valores.\n",
    "            - Converte colunas de data de string para formato de data.\n",
    "            - Deriva colunas de ano e mês a partir de datas.\n",
    "            - Processa e deriva novas colunas com base em mapeamentos para situação cadastral e tipo de estabelecimento.\n",
    "            - Valida endereços de e-mail usando expressões regulares.\n",
    "            - Combina informações de endereço para formar uma coluna completa de endereço.\n",
    "            - Utiliza a função de geocodificação para obter coordenadas com base no endereço e, em caso de falha, com base no CEP.\n",
    "            - Realiza correções na coluna de provedor de e-mail usando um dicionário de mapeamento.\n",
    "\n",
    "        Notas:\n",
    "            - Esta função faz uso intensivo das operações de DataFrame do PySpark.\n",
    "            - Dependências: A função depende de outras funções e UDFs, como 'geocode_udf', bem como de variáveis de instância, como 'dic_provedor'.\n",
    "        \"\"\"\n",
    "        \n",
    "        df_pais = self.read_data(schema_name='paises')\n",
    "        df_mun = self.read_data(schema_name='municipios')\n",
    "        df_cnaes = self.read_data(schema_name='cnaes')\n",
    "        df_motivos = self.read_data(schema_name='motivos')\n",
    "        \n",
    "        df = df.withColumnRenamed(\"CNAE_1\", \"COD_CNAE\")\n",
    "        df = df.withColumnRenamed(\"MUNICIPIO\", \"ID_MUNICPIO\")\n",
    "        df = df.withColumnRenamed(\"PAIS\", \"COD_PAIS\")\n",
    "        df = df.withColumnRenamed(\"MOTIVO_CADASTRAL\", \"COD_MOTIVO\") \n",
    "        \n",
    "\n",
    "        df = df.join(broadcast(df_motivos), \"COD_MOTIVO\", \"left\").drop(df.COD_MOTIVO)\n",
    "        df = df.join(broadcast(df_cnaes), \"COD_CNAE\", \"left\").drop(df.COD_CNAE)\n",
    "        df = df.join(broadcast(df_mun), \"ID_MUNICPIO\", \"left\").drop(df.ID_MUNICPIO)\n",
    "        df = df.join(broadcast(df_pais), \"COD_PAIS\", \"left\").drop(df.COD_PAIS)\n",
    "        \n",
    "        dic_provedor = self.dic_provedor\n",
    "        # Tratamento da coluna provedor\n",
    "        df = df.withColumn(\"PROVEDOR\",  regexp_extract(\"EMAIL\", \"(?<=@)[^.]+(?=\\\\.)\", 0))\n",
    "        \n",
    "        # Colocando em caixa alta o provedor\n",
    "        df = df.withColumn(\"PROVEDOR\", upper(col(\"PROVEDOR\")))\n",
    "        \n",
    "        # Colocando em caixa baixa o email\n",
    "        df = df.withColumn(\"EMAIL\", lower(col(\"EMAIL\")))\n",
    "\n",
    "        # Convertendo colunas de data\n",
    "        df = df.withColumn(\"DT_SIT_CADASTRAL\", to_date(col('DT_SIT_CADASTRAL'), \"yyyyMMdd\"))\n",
    "        df = df.withColumn(\"DT_INICIO_ATIVIDADE\", to_date(col('DT_INICIO_ATIVIDADE'), \"yyyyMMdd\"))\n",
    "        df = df.withColumn(\"DT_SIT_ESPECIAL\", to_date(col('DT_SIT_ESPECIAL'), \"yyyyMMdd\"))\n",
    "        \n",
    "        df = df.withColumn( \"ano_cadastro\", year('DT_INICIO_ATIVIDADE'))\n",
    "        df = df.withColumn( \"mes_cadastro\", month('DT_INICIO_ATIVIDADE'))\n",
    "        df = df.withColumn( \"ano_sit_cadastral\", year('DT_SIT_CADASTRAL'))\n",
    "        df = df.withColumn( \"mes_sit_cadastral\", month('DT_SIT_CADASTRAL'))\n",
    "        \n",
    "        # Defina o dicionário de mapeamento\n",
    "        mapping = {1: 'NULA',2: 'ATIVA',3: 'SUSPENSA',4: 'INAPTA',8: 'BAIXADA'}\n",
    "        \n",
    "        # Use a função 'when' para criar a nova coluna 'NM_SIT_CADASTRAL'\n",
    "        df = df.withColumn(\"NM_SIT_CADASTRAL\",\n",
    "                           when(df[\"SIT_CADASTRAL\"].isin(list(mapping.keys())), df[\"SIT_CADASTRAL\"]).otherwise(None))\n",
    "        \n",
    "        # Substitua os valores na nova coluna com base no dicionário de mapeamento\n",
    "        for key, value in mapping.items():\n",
    "            df = df.withColumn(\"NM_SIT_CADASTRAL\", when(df[\"SIT_CADASTRAL\"] == key, value).otherwise(df[\"NM_SIT_CADASTRAL\"]))\n",
    "            \n",
    "        # Use uma expressão regular para validar os endereços de e-mail\n",
    "        email_pattern = r'^\\S+@\\S+\\.\\S+$'  # Padrão simples de endereço de e-mail\n",
    "        \n",
    "        # Use a função 'regexp_extract' para extrair endereços de e-mail válidos\n",
    "        df = df.withColumn(\"valid_email\", regexp_extract(col(\"EMAIL\"), email_pattern, 0))\n",
    "        \n",
    "        # Defina o dicionário de mapeamento\n",
    "        mapping = {1: 'MATRIZ',2: 'FILIAL'}\n",
    "        \n",
    "        # Use a função 'when' para criar a nova coluna 'NM_MATRIZ_FILIAL'\n",
    "        df = df.withColumn(\"NM_MATRIZ_FILIAL\",\n",
    "                           when(df[\"MATRIZ_FILIAL\"].isin(list(mapping.keys())), df[\"MATRIZ_FILIAL\"]).otherwise(None))\n",
    "        \n",
    "        # Substitua os valores na nova coluna com base no dicionário de mapeamento\n",
    "        for key, value in mapping.items():\n",
    "            df = df.withColumn(\"NM_MATRIZ_FILIAL\", when(df[\"MATRIZ_FILIAL\"] == key, value).otherwise(df[\"NM_MATRIZ_FILIAL\"]))\n",
    "            \n",
    "        # Criando a nova coluna \"ENDERECO_COMPLETO\"\n",
    "        df = df.withColumn(\"ENDERECO_COMPLETO\",\n",
    "                           concat_ws(\", \",\n",
    "                                     concat(df[\"TIPO_LOUGRADOURO\"], lit(\" \"), df[\"LOGRADOURO\"]),\n",
    "                                     \"NUMERO\",concat_ws(\" - \", \"MUNICIPIO\", \"UF\")))\n",
    "        \n",
    "        # Adicione a lógica de geocodificação aqui\n",
    "        df = df.withColumn(\"COORDENADAS\", ReceitaLT.geocode_udf(df[\"ENDERECO_COMPLETO\"]))\n",
    "        \n",
    "        df = df.withColumn(\"COORDENADAS\",\n",
    "                           when((col(\"COORDENADAS.latitude\").isNull()) & (col(\"COORDENADAS.longitude\").isNull()),\n",
    "                                ReceitaLT.geocode_udf(df[\"CEP\"])).otherwise(col(\"COORDENADAS\")))\n",
    "        \n",
    "        # Correção da coluna provedor\n",
    "        df = df.replace(dic_provedor, subset=['PROVEDOR'])\n",
    "\n",
    "        # Transformação das keys e values do dicionário em lowercase\n",
    "        dic_prov_lower = {k.lower(): str(v).lower() for k, v in dic_provedor.items()}\n",
    "\n",
    "        # Correção dos provedores na coluna EMAIL\n",
    "        replace_expr = reduce(\n",
    "            lambda a, b: regexp_replace(a, rf\"\\b{b[0]}\\b\", b[1]),\n",
    "            dic_prov_lower.items(),\n",
    "            col(\"valid_email\"))\n",
    "\n",
    "        df = df.withColumn(\"valid_email\", replace_expr)\n",
    "        df = df.withColumnRenamed(\"valid_email\", \"VALILD_EMAIL\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def process_empresas(self, df):\n",
    "        \"\"\"\n",
    "        Processa e enriquece o DataFrame de empresas com informações adicionais e transformações.\n",
    "\n",
    "        Parâmetros:\n",
    "            df (DataFrame): DataFrame inicial contendo informações de empresas.\n",
    "\n",
    "        Retorna:\n",
    "            DataFrame: DataFrame processado e enriquecido com novas colunas e informações.\n",
    "\n",
    "        Descrição:\n",
    "            - Lê dataframes adicionais relacionados a naturezas jurídicas e qualificações.\n",
    "            - Realiza renomeação de colunas para facilitar junções.\n",
    "            - Enriquece o dataframe com informações de naturezas jurídicas e qualificações.\n",
    "            - Processa a coluna 'NOME_EMPRESA' para extrair informações potenciais de CPF.\n",
    "            - Deriva uma nova coluna baseada no porte da empresa, usando um mapeamento predefinido.\n",
    "            - Determina a probabilidade de um valor ser um CPF válido com base em seu comprimento.\n",
    "            - Criptografa possíveis valores de CPF usando AES e os armazena em uma nova coluna 'CPF_CRIPTOGRAFADO', enquanto remove a coluna original 'CPF'.\n",
    "\n",
    "        Notas:\n",
    "            - Esta função faz uso intensivo das operações de DataFrame do PySpark.\n",
    "            - O valor de criptografia (secret_key) é gerado dinamicamente a cada chamada da função. Portanto, cada execução resultará em valores de 'CPF_CRIPTOGRAFADO' diferentes para os mesmos CPFs.\n",
    "            - O método AES usado aqui é 'ECB', que não é considerado seguro para muitos casos de uso devido à falta de vetor de inicialização (IV). A utilização deste modo deve ser revista se a segurança for uma preocupação.\n",
    "        \"\"\"\n",
    "        df_nat = self.read_data(schema_name='naturezas')\n",
    "        df_qual = self.read_data(schema_name='qualificacoes')\n",
    "        \n",
    "        df = df.withColumnRenamed(\"QUALIF_RESPONVAVEL\", \"COD_QUALIFICACAO\")\n",
    "        \n",
    "        df = df.join(broadcast(df_nat), \"COD_NAT_JURICA\", \"left\").drop(df.COD_NAT_JURICA)\n",
    "        df = df.join(broadcast(df_qual), \"COD_QUALIFICACAO\", \"left\").drop(df.COD_QUALIFICACAO)\n",
    "               \n",
    "        df = df.withColumn(\"CPF\", regexp_replace(\"NOME_EMPRESA\", \"[^0-9]\", \"\"))\n",
    "        df = df.withColumn(\"CPF\", when(col(\"CPF\") == \"\", None).otherwise(col(\"CPF\")))\n",
    "        df = df.withColumn('CPF_LEN', length('CPF'))\n",
    "        # Defina o dicionário de mapeamento\n",
    "        mapping = {0: 'NÃO INFORMADO',1: 'MICRO EMPRESA',3: ' EMPRESA DE PEQUENO PORTE',5: 'DEMAIS',8: 'BAIXADA'}\n",
    "        # Use a função 'when' para criar a nova coluna 'NM_SIT_CADASTRAL'\n",
    "        df = df.withColumn(\"NM_PORTE\",\n",
    "                           when(df[\"PORTE\"].isin(list(mapping.keys())), df[\"PORTE\"]).otherwise(None))\n",
    "        \n",
    "        # Substitua os valores na nova coluna com base no dicionário de mapeamento\n",
    "        for key, value in mapping.items():\n",
    "            df = df.withColumn(\"NM_PORTE\", when(df[\"PORTE\"] == key, value).otherwise(df[\"NM_PORTE\"]))\n",
    "\n",
    "        df = df.withColumn(\"PROBABILIDADE_DE_SER_CPF\", when(df[\"CPF_LEN\"] == 11, \"SIM\").otherwise(\"NAO\"))\n",
    "        \n",
    "        secret_key = secrets.token_urlsafe(24)\n",
    "        df = df.withColumn(\"CPF_CRIPTOGRAFADO\", expr(f\"base64(aes_encrypt(CPF, '{secret_key}', 'ECB', 'PKCS'))\")).drop(df.CPF)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def process_simples(self, df):\n",
    "        \"\"\"\n",
    "        Processa o DataFrame relacionado ao regime tributário SIMPLES das empresas.\n",
    "\n",
    "        Parâmetros:\n",
    "            df (DataFrame): DataFrame inicial contendo informações relacionadas ao regime tributário SIMPLES.\n",
    "\n",
    "        Retorna:\n",
    "            DataFrame: DataFrame processado com colunas de data convertidas e apenas as colunas relevantes selecionadas.\n",
    "\n",
    "        Descrição:\n",
    "            - Converte colunas que representam datas do formato \"yyyyMMdd\" para o tipo data.\n",
    "            - Seleciona apenas as colunas relevantes para o contexto, que são: 'CNPJ_BASICO', 'OPÇAO_PELO_MEI', 'DT_OPCAO_MEI', 'DT_EXCLUSAO_MEI', 'OPCAO_PELO_SIMPLES', 'DT_OPCAO_SIMPLES', e 'DT_EXCLUSAO_SIMPLES'.\n",
    "\n",
    "        Notas:\n",
    "            - Esta função assume que as colunas de data estão no formato \"yyyyMMdd\" e realiza a conversão para o tipo data.\n",
    "            - As colunas de datas que são processadas incluem: DATA_OPCAO_PELO_SIMPLES, DATA_EXCLUSAO_SIMPLES, DATA_EXCLUSAO_MEI e DATA_OPCAO_PELO_MEI.\n",
    "        \"\"\"\n",
    "        df = df.withColumn(\"DT_OPCAO_SIMPLES\", to_date,(col('DATA_OPCAO_PELO_SIMPLES'), \"yyyyMMdd\"))\n",
    "        df = df.withColumn(\"DT_EXCLUSAO_SIMPLES\", to_date(col('DATA_EXCLUSAO_SIMPLES'), \"yyyyMMdd\"))\n",
    "        df = df.withColumn(\"DT_EXCLUSAO_MEI\", to_date(col('DATA_EXCLUSAO_MEI'), \"yyyyMMdd\"))\n",
    "        df = df.withColumn(\"DT_OPCAO_MEI\", to_date(col('DATA_OPCAO_PELO_MEI'), \"yyyyMMdd\"))\n",
    "        df = df.select('CNPJ_BASICO','OPÇAO_PELO_MEI','DT_OPCAO_MEI','DT_EXCLUSAO_MEI','OPCAO_PELO_SIMPLES','DT_OPCAO_SIMPLES','DT_EXCLUSAO_SIMPLES')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def save_data(self, df, path, num_partitions=1, file_format=\"parquet\"):\n",
    "        \"\"\"\n",
    "        Save the DataFrame to the specified path.\n",
    "        \n",
    "        :param df: DataFrame to be saved\n",
    "        :param path: Destination path\n",
    "        :param num_partitions: Number of partitions for saving data (default is 1)\n",
    "        :param file_format: File format to save the data (default is \"parquet\")\n",
    "        \"\"\"\n",
    "        \n",
    "        # Repartitioning the DataFrame based on user input\n",
    "        df = df.repartition(num_partitions)\n",
    "        \n",
    "        # Saving the DataFrame to the specified path and format\n",
    "        df.write.mode('overwrite').format(file_format).save(path)\n",
    "        \n",
    "    @staticmethod\n",
    "    def download_nomes(save_base_path=\"./output/nomes\"):        \n",
    "        \"\"\"\n",
    "        Baixa e extrai o arquivo nomes.csv.gz do dataset genero-nomes no Brasil.io.\n",
    "\n",
    "        Parâmetros:\n",
    "            save_base_path (str, opcional): Caminho base onde o arquivo será salvo. O padrão é './output/nomes'.\n",
    "\n",
    "        Descrição:\n",
    "            - Cria o diretório de salvamento se ele não existir.\n",
    "            - Baixa o arquivo nomes.csv.gz da URL especificada.\n",
    "            - Extrai o conteúdo do arquivo .gz.\n",
    "            - Remove o arquivo .gz original, mantendo apenas o arquivo CSV extraído.\n",
    "\n",
    "        Notas:\n",
    "            - Esta função usa a biblioteca `requests` para baixar o arquivo.\n",
    "            - A função verifica se a resposta do servidor é 200 (sucesso) antes de baixar o arquivo.\n",
    "            - O arquivo .gz é extraído usando a biblioteca `gzip`.\n",
    "        \"\"\"\n",
    "        # Certifique-se de que o diretório de salvamento exista\n",
    "        os.makedirs(save_base_path, exist_ok=True)\n",
    "        url = \"https://data.brasil.io/dataset/genero-nomes/nomes.csv.gz\"\n",
    "        # Derive o nome do arquivo da URL\n",
    "        file_name = os.path.basename(url)\n",
    "        file_path = os.path.join(save_base_path, file_name)\n",
    "        extracted_file_path = os.path.join(save_base_path, file_name[:-3])  # remove .gz\n",
    "\n",
    "        # Baixe o arquivo\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(file_path, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=128):\n",
    "                    file.write(chunk)\n",
    "        else:\n",
    "            print(f\"Failed to download {url}. Status code: {response.status_code}\")\n",
    "            return\n",
    "\n",
    "        # Extraia o arquivo\n",
    "        with gzip.open(file_path, 'rb') as f_in:\n",
    "            with open(extracted_file_path, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "        # Apague o arquivo .gz\n",
    "        os.remove(file_path)\n",
    "        \n",
    "    def process_mei(self, df, save_base_path=\"./output/nomes\", file_name=\"nomes.csv\"):\n",
    "        \"\"\"\n",
    "        Processa um DataFrame referente a MEIs, realiza joins com dados adicionais de naturezas jurídicas,\n",
    "        qualificações, e um conjunto de dados de nomes para extração e categorização de primeiro nome.\n",
    "\n",
    "        Parâmetros:\n",
    "            df (pyspark.sql.DataFrame): DataFrame inicial contendo dados sobre MEIs.\n",
    "            save_base_path (str, opcional): Caminho onde o arquivo com dados de nomes foi extraído. Padrão é './output/nomes'.\n",
    "            file_name (str, opcional): Nome do arquivo CSV contendo dados de nomes a ser lido. Padrão é 'nomes.csv'.\n",
    "\n",
    "        Retorna:\n",
    "            pyspark.sql.DataFrame: DataFrame processado após todas as transformações e joins.\n",
    "\n",
    "        Descrição:\n",
    "            1. Realiza join com DataFrames de 'naturezas' e 'qualificações'.\n",
    "            2. Extração e manipulação de dados de CPF.\n",
    "            3. Utiliza um dicionário para mapear e criar a coluna \"NM_PORTE\".\n",
    "            4. Criptografa a coluna de CPF.\n",
    "            5. Realiza filtragens baseado na probabilidade do nome ser um CPF válido.\n",
    "            6. Lê um conjunto de dados de nomes e realiza o explode na coluna 'alternative_names'.\n",
    "            7. Extrai o primeiro nome da coluna 'NOME_EMPRESA'.\n",
    "            8. Realiza o join com o conjunto de dados de nomes para categorizar o primeiro nome.\n",
    "            9. Retorna um DataFrame contendo informações relevantes após todas as transformações.\n",
    "        \"\"\"\n",
    "\n",
    "        df_nat = self.read_data(schema_name='naturezas')\n",
    "        df_qual = self.read_data(schema_name='qualificacoes')\n",
    "        \n",
    "        df = df.withColumnRenamed(\"QUALIF_RESPONVAVEL\", \"COD_QUALIFICACAO\")\n",
    "        \n",
    "        df = df.join(broadcast(df_nat), \"COD_NAT_JURICA\", \"left\").drop(df.COD_NAT_JURICA)\n",
    "        df = df.join(broadcast(df_qual), \"COD_QUALIFICACAO\", \"left\").drop(df.COD_QUALIFICACAO)\n",
    "               \n",
    "        df = df.withColumn(\"CPF\", regexp_replace(\"NOME_EMPRESA\", \"[^0-9]\", \"\"))\n",
    "        df = df.withColumn(\"CPF\", when(col(\"CPF\") == \"\", None).otherwise(col(\"CPF\")))\n",
    "        df = df.withColumn('CPF_LEN', length('CPF'))\n",
    "        # Defina o dicionário de mapeamento\n",
    "        mapping = {0: 'NÃO INFORMADO',1: 'MICRO EMPRESA',3: ' EMPRESA DE PEQUENO PORTE',5: 'DEMAIS',8: 'BAIXADA'}\n",
    "        # Use a função 'when' para criar a nova coluna 'NM_SIT_CADASTRAL'\n",
    "        df = df.withColumn(\"NM_PORTE\",\n",
    "                           when(df[\"PORTE\"].isin(list(mapping.keys())), df[\"PORTE\"]).otherwise(None))\n",
    "        \n",
    "        # Substitua os valores na nova coluna com base no dicionário de mapeamento\n",
    "        for key, value in mapping.items():\n",
    "            df = df.withColumn(\"NM_PORTE\", when(df[\"PORTE\"] == key, value).otherwise(df[\"NM_PORTE\"]))\n",
    "\n",
    "        df = df.withColumn(\"PROBABILIDADE_DE_SER_CPF\", when(df[\"CPF_LEN\"] == 11, \"SIM\").otherwise(\"NAO\"))\n",
    "        \n",
    "        secret_key = secrets.token_urlsafe(24)\n",
    "        df = df.withColumn(\"CPF_CRIPTOGRAFADO\", expr(f\"base64(aes_encrypt(CPF, '{secret_key}', 'ECB', 'PKCS'))\")).drop(df.CPF)\n",
    "        \n",
    "        # Caminho completo do arquivo\n",
    "        file_path = os.path.join(save_base_path, file_name)\n",
    "        \n",
    "        # Filtrar df_processed baseado na coluna PROBABILIDADE_DE_SER_CPF\n",
    "        df_filter = df.filter(col('PROBABILIDADE_DE_SER_CPF') == 'SIM').dropDuplicates(subset=['CPF_CRIPTOGRAFADO', 'NOME_EMPRESA'])\n",
    "\n",
    "        # Ler o arquivo CSV\n",
    "        df = self.spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "        # Explodir a coluna alternative_names para múltiplas linhas\n",
    "        df_expanded = df.withColumn(\"alternative_names\", explode(split(coalesce(col(\"alternative_names\"), col(\"first_name\")), \"\\\\|\")))\n",
    "\n",
    "        # Selecionar as colunas desejadas\n",
    "        df_result = df_expanded.select(\"alternative_names\", \"group_name\", \"ratio\", \"classification\").dropDuplicates(subset=['alternative_names'])\n",
    "\n",
    "        # Extrair o primeiro nome da coluna NOME_EMPRESA\n",
    "        df_filter = df_filter.withColumn(\"PRIMEIRO_NOME\", split(col(\"NOME_EMPRESA\"), \" \")[0])\n",
    "        \n",
    "        # Fazer o join entre df_filter e df_result\n",
    "        joined_df = df_filter.join(df_result, df_filter.PRIMEIRO_NOME == df_result.alternative_names, \"left\").dropDuplicates()\n",
    "        \n",
    "        joined_df = joined_df.select('CNPJ','NOME_EMPRESA','CAP_SOCIAL','NM_PORTE','NAT_JURICA','ENTE_FEDERATIVO','NM_QUALIFICACAO','CPF_CRIPTOGRAFADO','CPF_LEN',\n",
    "                    'PROBABILIDADE_DE_SER_CPF','PRIMEIRO_NOME',col('group_name').alias('GRUPO_NOME'), \n",
    "                    col('ratio').alias('PROBABILIDADE_CLASSIFICACAO'), col('classification').alias('CLASSIFICACAO')).dropDuplicates()\n",
    "        \n",
    "        return joined_df\n",
    "    \n",
    "    def process_socios(self, df, save_base_path=\"./output/nomes\", file_name=\"nomes.csv\"):\n",
    "        \"\"\"\n",
    "        Processa um DataFrame referente a sócios, realiza joins com dados adicionais de países, qualificações, \n",
    "        e um conjunto de dados de nomes para extração e categorização de primeiro nome.\n",
    "\n",
    "        Parâmetros:\n",
    "            df (pyspark.sql.DataFrame): DataFrame inicial contendo dados sobre sócios.\n",
    "            save_base_path (str, opcional): Caminho onde o arquivo com dados de nomes foi extraído. Padrão é './output/nomes'.\n",
    "            file_name (str, opcional): Nome do arquivo CSV contendo dados de nomes a ser lido. Padrão é 'nomes.csv'.\n",
    "\n",
    "        Retorna:\n",
    "            pyspark.sql.DataFrame: DataFrame processado após todas as transformações e joins.\n",
    "\n",
    "        Descrição:\n",
    "            1. Realiza join com DataFrames de 'países'.\n",
    "            2. Usa mapeamentos para criar colunas \"NM_FAIXA_ETARIA\" e \"NM_IDENTIFICADOR_SOCIO\".\n",
    "            3. Renomeia e realiza join com DataFrame de qualificações para obter descrições das qualificações.\n",
    "            4. Converte coluna de data \"DATA_ENTRADA_SOCIEDADE\" para o formato desejado.\n",
    "            5. Lê e processa um conjunto de dados de nomes, explodindo e selecionando colunas relevantes.\n",
    "            6. Extração do primeiro nome da coluna 'NOME_SOCIO_RAZAO_SOCIAL'.\n",
    "            7. Realiza o join entre o DataFrame processado e o conjunto de dados de nomes para categorizar o primeiro nome.\n",
    "            8. Retorna um DataFrame contendo informações relevantes após todas as transformações.\n",
    "        \"\"\"\n",
    "        \n",
    "        df_pais = self.read_data(schema_name='paises')\n",
    "        df = df.withColumnRenamed(\"PAIS\", \"COD_PAIS\")\n",
    "        df = df.join(broadcast(df_pais), \"COD_PAIS\", \"left\").drop(df.COD_PAIS)\n",
    "        \n",
    "        # Mapeamento de códigos para faixas etárias.\n",
    "        mapping = {\n",
    "            1: '0 a 12 anos',\n",
    "            2: '13 a 20 anos',\n",
    "            3: '21 a 30 anos',\n",
    "            4: '31 a 40 anos',\n",
    "            5: '41 a 50 anos',\n",
    "            6: '51 a 60 anos',\n",
    "            7: '61 a 70 anos',\n",
    "            8: '71 a 80 anos',\n",
    "            9: 'maiores de 80 anos',\n",
    "            0: 'NA'\n",
    "        }\n",
    "        \n",
    "                \n",
    "        # Mapeamento de códigos para faixas etárias.\n",
    "        id_socio = {\n",
    "            1: 'PESSOA JURIDICA',\n",
    "            2: 'PESSOA FISICA',\n",
    "            3: 'ESTRANGEIRO'}\n",
    "        \n",
    "        df_qual = self.read_data(schema_name='qualificacoes')\n",
    "        \n",
    "        \n",
    "        # Use a função 'when' para criar a nova coluna 'NM_SIT_CADASTRAL'\n",
    "        df = df.withColumn(\"NM_FAIXA_ETARIA\",\n",
    "                           when(df[\"FAIXA_ETARIA\"].isin(list(mapping.keys())), df[\"FAIXA_ETARIA\"]).otherwise(None))\n",
    "        \n",
    "        # Substitua os valores na nova coluna com base no dicionário de mapeamento\n",
    "        for key, value in mapping.items():\n",
    "            df = df.withColumn(\"NM_FAIXA_ETARIA\", when(df[\"FAIXA_ETARIA\"] == key, value).otherwise(df[\"NM_FAIXA_ETARIA\"]))\n",
    "            \n",
    "\n",
    "        # Use a função 'when' para criar a nova coluna 'NM_SIT_CADASTRAL'\n",
    "        df = df.withColumn(\"NM_IDENTIFICADOR_SOCIO\",\n",
    "                           when(df[\"IDENTIFICADOR_SOCIO\"].isin(list(id_socio.keys())), df[\"IDENTIFICADOR_SOCIO\"]).otherwise(None))\n",
    "        \n",
    "        # Substitua os valores na nova coluna com base no dicionário de mapeamento\n",
    "        for key, value in id_socio.items():\n",
    "            df = df.withColumn(\"NM_IDENTIFICADOR_SOCIO\", when(df[\"IDENTIFICADOR_SOCIO\"] == key, value).otherwise(df[\"NM_IDENTIFICADOR_SOCIO\"]))\n",
    "\n",
    "\n",
    "        df_qual = self.read_data(schema_name='qualificacoes')\n",
    "        # Renomeação e join com df_qual para obter descrições de qualificações.\n",
    "        df = df.withColumnRenamed(\"QUALIFICACAO_REPRESENTANTE_LEGAL\", \"COD_QUALIFICACAO\")\n",
    "        df = df.join(broadcast(df_qual), \"COD_QUALIFICACAO\", \"left\").drop(\"COD_QUALIFICACAO\")\n",
    "        df = df.withColumnRenamed(\"NM_QUALIFICACAO\", \"NM_QUALIFICACAO_REPRESENTANTE_LEGAL\")\n",
    "\n",
    "        df = df.withColumnRenamed(\"QUALIFICAÇAO_SOCIO\", \"COD_QUALIFICACAO\")\n",
    "        df = df.join(broadcast(df_qual), \"COD_QUALIFICACAO\", \"left\").drop(\"COD_QUALIFICACAO\")\n",
    "        df = df.withColumnRenamed(\"NM_QUALIFICACAO\", \"NM_QUALIFICAÇAO_SOCIO\")\n",
    "\n",
    "        # Conversão da coluna de data.\n",
    "        df = df.withColumn(\"DT_ENTRADA_SOCIEDADE\", to_date(col('DATA_ENTRADA_SOCIEDADE'), \"yyyyMMdd\")).drop(df.DATA_ENTRADA_SOCIEDADE)\n",
    "\n",
    "        # Leitura do arquivo CSV.\n",
    "        file_path = os.path.join(save_base_path, file_name)\n",
    "        df_csv = self.spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "        df_csv = df_csv.withColumn(\"alternative_name2\", explode(split(df_csv[\"alternative_names\"], \"\\|\")))\n",
    "        df_result = df_csv.select(\"alternative_name2\", \"group_name\", \"ratio\", \"classification\").dropDuplicates([\"alternative_name2\"])\n",
    "\n",
    "        # Extração do primeiro nome.\n",
    "        df = df.withColumn(\"PRIMEIRO_NOME\", split(col(\"NOME_SOCIO_RAZAO_SOCIAL\"), \" \")[0]).dropDuplicates()\n",
    "\n",
    "        # Join entre dataframes.\n",
    "        joined_df = df.join(df_result, df.PRIMEIRO_NOME == df_result.alternative_name2, \"left\").dropDuplicates()\n",
    "        \n",
    "        joined_df = joined_df.select('CNPJ_BASICO','NOME_SOCIO_RAZAO_SOCIAL','CNPJ_CPF_SOCIO','REPRESENTANTE_LEGAL',\n",
    "        'NOME_REPRESENTANTE','NM_PAIS','NM_FAIXA_ETARIA','NM_IDENTIFICADOR_SOCIO','NM_QUALIFICACAO_REPRESENTANTE_LEGAL',\n",
    "        'NM_QUALIFICAÇAO_SOCIO','DT_ENTRADA_SOCIEDADE','PRIMEIRO_NOME',col('ratio').alias('PROBABILIDADE_CLASSIFICACAO'),\n",
    "        col('classification').alias('CLASSIFICACAO')).dropDuplicates()\n",
    "\n",
    "        return joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8607b9a-ac1a-4d97-9ef8-790b2f4a71e4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReceitaCNPJApi:\n",
    "    \"\"\"\n",
    "    Classe ReceitaCNPJApi:\n",
    "\n",
    "    Esta classe é responsável por interagir com a API de Dados Abertos da Receita Federal, facilitando \n",
    "    a obtenção de informações relacionadas a CNPJs, como empresas, sócios, municípios, entre outros. \n",
    "    A classe inclui funcionalidades para baixar, descompactar e salvar dados, além de utilitários \n",
    "    para manipular e consultar URLs.\n",
    "    \"\"\"\n",
    "\n",
    "    # URL base da API de dados abertos da Receita Federal.\n",
    "    BASE_URL = \"https://dadosabertos.rfb.gov.br/CNPJ\"\n",
    "\n",
    "    # Prefixos de arquivos que podem ser baixados da API.\n",
    "    FILE_PREFIXES = ['Estabelecimentos', 'Municipios', 'Simples', 'Empresas', 'Cnaes', 'Socios', 'Naturezas','Qualificacoes','Paises','Motivos']\n",
    "\n",
    "    # Número máximo de tentativas ao fazer uma solicitação para a API.\n",
    "    MAX_ATTEMPTS = 15\n",
    "\n",
    "    # Tempo de espera entre tentativas de solicitações (em segundos).\n",
    "    WAIT_TIME = 180\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inicializador da classe. Configura o logger para registrar atividades e erros.\n",
    "        \"\"\"\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_last_month():\n",
    "        \"\"\"\n",
    "        Retorna a data atual.\n",
    "        \"\"\"\n",
    "        return (datetime.today()).date()\n",
    "\n",
    "    def get_output_path_for_prefix(self, prefix):\n",
    "        \"\"\"\n",
    "        Gera o caminho completo de saída para armazenar os dados com base em um prefixo fornecido.\n",
    "\n",
    "        Args:\n",
    "        prefix (str): Prefixo do arquivo para o qual o caminho de saída será gerado.\n",
    "\n",
    "        Returns:\n",
    "        str: Caminho completo de saída para o prefixo especificado.\n",
    "        \"\"\"\n",
    "        return os.path.join(self.BASE_OUTPUT_PATH, prefix, f\"{prefix}.csv\")\n",
    "\n",
    "    def get_most_common_date(self):\n",
    "        \"\"\"\n",
    "        Consulta a API para obter a data mais comum em que os arquivos foram atualizados.\n",
    "        \"\"\"\n",
    "\n",
    "        # Etapa 1: Tentar consultar a API e obter uma resposta.\n",
    "        try:\n",
    "            response = requests.get(self.BASE_URL)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Etapa 2: Usar a biblioteca BeautifulSoup para analisar a resposta HTML.\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Etapa 3: Selecionar todos os elementos de data da resposta HTML.\n",
    "            # Assumindo que a data é o terceiro elemento filho da tag 'tr'.\n",
    "            date_elements = soup.select('tr > td:nth-child(3)')\n",
    "\n",
    "            # Etapa 4: Transformar cada elemento de data em um objeto datetime.\n",
    "            dates = [datetime.strptime(elem.get_text().strip(), '%Y-%m-%d %H:%M') for elem in date_elements if elem.get_text().strip() != '']\n",
    "\n",
    "            # Etapa 5: Contar as ocorrências de cada data e determinar a data mais comum.\n",
    "            most_common_date, _ = Counter([date.date() for date in dates]).most_common(1)[0]\n",
    "\n",
    "            # Etapa 6: Retornar a data mais comum.\n",
    "            return datetime.combine(most_common_date, datetime.min.time()).date()\n",
    "        except requests.RequestException as e:\n",
    "            # Etapa 7: Em caso de qualquer erro na consulta da API, registrar o erro e retornar None.\n",
    "            self.logger.error(f\"Failed to get most common date. Error: {e}\")\n",
    "            return None\n",
    "\n",
    "    def lista_urls_receita(self, *prefixes):\n",
    "        \"\"\"\n",
    "        Gera uma lista de URLs para download com base nos prefixos fornecidos. As URLs são \n",
    "        determinadas com base nos prefixos de arquivo conhecidos. Se nenhum prefixo for \n",
    "        fornecido, o método gerará URLs para todos os tipos de arquivos conhecidos.\n",
    "\n",
    "        Args:\n",
    "        *prefixes (str): Prefixos de arquivos para os quais as URLs serão geradas.\n",
    "\n",
    "        Returns:\n",
    "        list: Lista de URLs completas para os arquivos correspondentes aos prefixos.\n",
    "        \"\"\"\n",
    "        # Etapa 1: Inicializar a lista vazia para armazenar as URLs.\n",
    "        urls = []\n",
    "\n",
    "        # Etapa 2: Se nenhum prefixo for fornecido como argumento, use todos os prefixos de arquivo conhecidos.\n",
    "        if not prefixes:\n",
    "            prefixes = self.FILE_PREFIXES\n",
    "\n",
    "        # Etapa 3: Iterar sobre cada prefixo fornecido.\n",
    "        for prefix in prefixes:\n",
    "            # Etapa 3.1: Se o prefixo estiver na lista de prefixos especificados, \n",
    "            # adicione a URL correspondente à lista de URLs.\n",
    "            if prefix in ['Municipios', 'Cnaes', 'Naturezas', 'Simples','Qualificacoes','Paises','Motivos']:\n",
    "                urls.append(f\"{self.BASE_URL}/{prefix}.zip\")\n",
    "\n",
    "            # Etapa 3.2: Se o prefixo estiver na lista de prefixos de arquivo conhecidos, \n",
    "            # gere URLs para cada arquivo (de 0 a 9) e adicione-as à lista de URLs.\n",
    "            elif prefix in self.FILE_PREFIXES:\n",
    "                urls.extend([f\"{self.BASE_URL}/{prefix}{i}.zip\" for i in range(10)])\n",
    "\n",
    "            # Etapa 3.3: Se o prefixo fornecido não for reconhecido, \n",
    "            # imprima uma mensagem informando que o prefixo não é reconhecido.\n",
    "            else:\n",
    "                print(f\"Prefixo '{prefix}' não reconhecido!\")\n",
    "\n",
    "        # Etapa 4: Retorne a lista completa de URLs geradas.\n",
    "        return urls\n",
    "\n",
    "    def fetch_data(self, url, save_path, log_accumulator=None, max_attempts=15, wait_time=180):\n",
    "        \"\"\"\n",
    "        Baixa um arquivo da URL especificada e o salva no caminho de saída especificado. Em caso de \n",
    "        falha na tentativa de download, tentará novamente até o número máximo de tentativas ser atingido.\n",
    "\n",
    "        Args:\n",
    "        url (str): URL de onde o arquivo será baixado.\n",
    "        save_path (str): Caminho onde o arquivo baixado será salvo.\n",
    "        log_accumulator (list, optional): Um acumulador para armazenar mensagens de log. Padrão para None.\n",
    "        max_attempts (int, optional): Número máximo de tentativas de download. Padrão para 15.\n",
    "        wait_time (int, optional): Tempo de espera entre tentativas em segundos. Padrão para 150.\n",
    "\n",
    "        Raises:\n",
    "        Exception: Se o número máximo de tentativas for atingido sem sucesso.\n",
    "\n",
    "        Returns:\n",
    "        str: Caminho completo do arquivo baixado.\n",
    "        \"\"\"\n",
    "\n",
    "        # Etapa 1: Criar o diretório no caminho de salvamento, caso ele não exista.\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        # Etapa 2: Derivar o nome do arquivo da URL fornecida.\n",
    "        file_name = url.split('/')[-1]\n",
    "        file_path = os.path.join(save_path, file_name)\n",
    "\n",
    "        # Etapa 3: Definir os cabeçalhos da solicitação.\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "\n",
    "        # Etapa 4: Iniciar tentativas de download do arquivo.\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                # Etapa 4.1: Fazer uma solicitação GET para a URL.\n",
    "                response = requests.get(url, headers=headers)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                # Etapa 4.2: Escrever o conteúdo da resposta no arquivo.\n",
    "                with open(file_path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "                return file_path\n",
    "\n",
    "            except requests.RequestException as e:\n",
    "                # Etapa 4.3: Registrar falha na tentativa de download.\n",
    "                msg = f\"Tentativa {attempt + 1} de {max_attempts} falhou. Erro: {e}\"\n",
    "                if log_accumulator:\n",
    "                    log_accumulator.append([msg, \"ERRO NO REQUEST! RETRY SENDO FEITO\"])\n",
    "\n",
    "                # Etapa 4.4: Se for a última tentativa, registrar o erro persistente e lançar uma exceção.\n",
    "                if attempt == max_attempts - 1:\n",
    "                    if log_accumulator:\n",
    "                        log_accumulator.append([\"ERRO PERSISTENTE AO TENTAR BAIXAR O ARQUIVO\"])\n",
    "                    raise\n",
    "\n",
    "                # Etapa 4.5: Aguardar o tempo especificado antes de tentar novamente.\n",
    "                time.sleep(wait_time)\n",
    "\n",
    "                \n",
    "    def download_and_unzip(self, url, save_base_path=\"./temp\", output_base_path=\"./output\", headers=None, log_accumulator=None, data_update=True):\n",
    "        \"\"\"\n",
    "        Baixa um arquivo zip da URL fornecida, descompacta e salva no caminho especificado. Se os \n",
    "        dados da Receita Federal não foram atualizados nos últimos 30 dias e o parâmetro data_update \n",
    "        estiver ativado, uma mensagem de log será gerada.\n",
    "\n",
    "        Args:\n",
    "        url (str): URL de onde o arquivo zip será baixado.\n",
    "        save_base_path (str, optional): Caminho base onde o arquivo baixado será salvo. Padrão para \"./temp\".\n",
    "        output_base_path (str, optional): Caminho base onde os dados descompactados serão armazenados. Padrão para \"./output\".\n",
    "        headers (dict, optional): Cabeçalhos HTTP para serem usados no pedido. Padrão para None.\n",
    "        log_accumulator (list, optional): Uma acumulador para armazenar mensagens de log. Padrão para None.\n",
    "        data_update (bool, optional): Determina se a função deve verificar se os dados foram atualizados nos últimos 30 dias. Padrão para True.\n",
    "\n",
    "        Returns:\n",
    "        tuple: Um tuple contendo a URL e uma mensagem indicando \"Success\" ou a razão da falha.\n",
    "        \"\"\"\n",
    "\n",
    "        # Etapa 1: Identificar o nome do arquivo e seu prefixo a partir da URL fornecida.\n",
    "        file_name = url.split('/')[-1]\n",
    "        prefix = next((p for p in self.FILE_PREFIXES if file_name.startswith(p)), None)\n",
    "        if not prefix:\n",
    "            return (url, \"Failed to determine prefix\")\n",
    "\n",
    "        # Etapa 2: Configurar o caminho de salvamento e garantir que o diretório de salvamento exista.\n",
    "        save_path = os.path.join(save_base_path)\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        # Etapa 3: Obter a data mais comum de atualização dos arquivos da API e verificar se é válida.\n",
    "        data_atualizacao = self.get_most_common_date()\n",
    "        if data_atualizacao is None:\n",
    "            return (url, \"Failed to determine most common date\")\n",
    "\n",
    "        # Etapa 4: Obter a data atual.\n",
    "        data_atual = self.get_last_month()\n",
    "\n",
    "        # Etapa 5: Verificar se os dados da Receita Federal foram atualizados nos últimos 30 dias.\n",
    "        if not data_update or (data_update and (data_atual - data_atualizacao).days <= 30):\n",
    "            try:\n",
    "                # Etapa 5.1: Tentar baixar o arquivo zip da URL fornecida.\n",
    "                zip_file_path = self.fetch_data(url, save_path, headers)\n",
    "\n",
    "                # Etapa 5.2: Configurar o caminho de saída e garantir que o diretório de saída exista.\n",
    "                output_path = os.path.join(output_base_path)\n",
    "                os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "                # Etapa 5.3: Tentar descompactar o arquivo baixado.\n",
    "                self.unzip_files(zip_file_path, output_path)\n",
    "\n",
    "                # Etapa 5.4: Registrar a conclusão bem-sucedida da descompactação.\n",
    "                self.logger.info(f\"File {url} has been unzipped successfully to {output_path}\")\n",
    "                return (url, \"Success\")\n",
    "            except Exception as e:\n",
    "                # Etapa 5.5: Em caso de qualquer erro, registrar o erro e retornar a mensagem.\n",
    "                self.logger.error(f\"Failed to download and unzip {url}. Error: {e}\")\n",
    "                return (url, str(e))\n",
    "        else:\n",
    "            # Etapa 6: Se os dados não foram atualizados nos últimos 30 dias e data_update está ativado, registrar uma mensagem.\n",
    "            log_msg = \"Os dados da receita federal não foram atualizados nos últimos 30 dias\"\n",
    "            if log_accumulator:\n",
    "                log_accumulator.add([log_msg])\n",
    "            self.logger.info(log_msg)\n",
    "            return (url, log_msg)\n",
    "\n",
    "    def unzip_files(self, zip_file_path, output_base_path, log_accumulator=None):\n",
    "        \"\"\"\n",
    "        Descompacta o arquivo fornecido e salva no caminho especificado. Substitui arquivos existentes.\n",
    "\n",
    "        Args:\n",
    "        - zip_file_path (str): Caminho completo do arquivo zip que precisa ser descompactado.\n",
    "        - output_base_path (str): Caminho base onde o arquivo descompactado deve ser salvo.\n",
    "        - log_accumulator (list, optional): Uma lista que pode ser fornecida para acumular mensagens de log, útil para rastrear erros ou informações. Se não for fornecido, apenas os logs padrão serão usados.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Etapa 1: Extraia o nome do arquivo zip da rota fornecida.\n",
    "        zip_file_name = os.path.basename(zip_file_path)\n",
    "\n",
    "        # Etapa 2: Determine o prefixo do arquivo com base nos prefixos conhecidos.\n",
    "        prefix = next((p for p in self.FILE_PREFIXES if zip_file_name.startswith(p)), None)\n",
    "        if not prefix:\n",
    "            msg = f\"File {zip_file_path} does not match expected patterns.\"\n",
    "\n",
    "            # Etapa 2.1: Se o prefixo não for encontrado, adicione uma mensagem de erro ao acumulador de log (se fornecido) e retorne.\n",
    "            if log_accumulator:\n",
    "                log_accumulator.append(msg)\n",
    "            self.logger.error(msg)\n",
    "            return\n",
    "\n",
    "        # Etapa 3: Configurar o caminho de saída e garantir que o diretório de saída exista.\n",
    "        output_path = os.path.join(output_base_path, prefix)\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        # Etapa 4: Abrir o arquivo zip.\n",
    "        self.logger.info(f\"Trying to unzip {zip_file_path}\")\n",
    "        with zipfile.ZipFile(zip_file_path, \"r\") as z:\n",
    "\n",
    "            # Etapa 4.1: Pegue o nome do primeiro arquivo dentro do arquivo zip.\n",
    "            file_inside_zip = z.namelist()[0]\n",
    "\n",
    "            # Etapa 4.2: Extraia a parte numérica do nome do arquivo zip para nomear corretamente o arquivo csv resultante.\n",
    "            number_in_zip = ''.join(filter(str.isdigit, zip_file_name))\n",
    "            final_file_path = os.path.join(output_path, f\"{prefix}{number_in_zip}.csv\")\n",
    "\n",
    "            # Etapa 4.3: Se o arquivo csv já existir no caminho de destino, exclua-o para garantir que o novo arquivo não seja sobreposto.\n",
    "            if os.path.exists(final_file_path):\n",
    "                os.remove(final_file_path)\n",
    "\n",
    "            # Etapa 4.4: Descompacte o conteúdo do arquivo zip diretamente para o caminho de saída desejado.\n",
    "            with z.open(file_inside_zip) as zf, open(final_file_path, 'wb') as f_out:\n",
    "                shutil.copyfileobj(zf, f_out)\n",
    "\n",
    "        # Etapa 5: Exclua o arquivo zip original após a extração.\n",
    "        os.remove(zip_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61a87111-948a-435b-bd91-e43d2a5755cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class ReceitaCNPJApi:\n",
    "    \"\"\"\n",
    "    Classe ReceitaCNPJApi:\n",
    "\n",
    "    Esta classe é responsável por interagir com a API de Dados Abertos da Receita Federal, facilitando \n",
    "    a obtenção de informações relacionadas a CNPJs, como empresas, sócios, municípios, entre outros. \n",
    "    A classe inclui funcionalidades para baixar, descompactar e salvar dados, além de utilitários \n",
    "    para manipular e consultar URLs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prefixos de arquivos que podem ser baixados da API.\n",
    "    FILE_PREFIXES = [\n",
    "        'Estabelecimentos', 'Municipios', 'Simples', 'Empresas', \n",
    "        'Cnaes', 'Socios', 'Naturezas', 'Qualificacoes', 'Paises', 'Motivos'\n",
    "    ]\n",
    "\n",
    "    # Número máximo de tentativas ao fazer uma solicitação para a API.\n",
    "    MAX_ATTEMPTS = 15\n",
    "\n",
    "    # Tempo de espera entre tentativas de solicitações (em segundos).\n",
    "    WAIT_TIME = 180\n",
    "\n",
    "    # URL base da API de dados abertos da Receita Federal, sem incluir o ano/mês.\n",
    "    BASE_URL = \"https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj\"\n",
    "\n",
    "    def __init__(self, base_output_path: str = \"./output\", save_base_path: str = \"./temp\"):\n",
    "        \"\"\"\n",
    "        Inicializador da classe. Configura o logger para registrar atividades e erros e define os caminhos base \n",
    "        para salvar os arquivos descompactados e baixados.\n",
    "\n",
    "        Args:\n",
    "            base_output_path (str, optional): Caminho base para salvar os arquivos descompactados. Padrão para \"./output\".\n",
    "            save_base_path (str, optional): Caminho base para salvar os arquivos baixados. Padrão para \"./temp\".\n",
    "        \"\"\"\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "        self.BASE_OUTPUT_PATH = base_output_path\n",
    "        self.SAVE_BASE_PATH = save_base_path\n",
    "\n",
    "    @staticmethod\n",
    "    def get_current_date():\n",
    "        \"\"\"\n",
    "        Retorna a data atual.\n",
    "\n",
    "        Returns:\n",
    "            date: Data atual.\n",
    "        \"\"\"\n",
    "        return datetime.today().date()\n",
    "\n",
    "    def get_output_path_for_prefix(self, prefix):\n",
    "        \"\"\"\n",
    "        Gera o caminho completo de saída para armazenar os dados com base em um prefixo fornecido.\n",
    "\n",
    "        Args:\n",
    "            prefix (str): Prefixo do arquivo para o qual o caminho de saída será gerado.\n",
    "\n",
    "        Returns:\n",
    "            str: Caminho completo de saída para o prefixo especificado.\n",
    "        \"\"\"\n",
    "        return os.path.join(self.BASE_OUTPUT_PATH, prefix, f\"{prefix}.csv\")\n",
    "\n",
    "    def lista_urls_receita(self, year: int = None, month: int = None, *prefixes):\n",
    "        \"\"\"\n",
    "        Gera uma lista de URLs para download com base nos prefixos fornecidos e no período especificado. \n",
    "        Se nenhum prefixo for fornecido, o método gerará URLs para todos os tipos de arquivos conhecidos.\n",
    "\n",
    "        Args:\n",
    "            year (int, optional): Ano de atualização dos dados (ex: 2024). Se não fornecido, usa o ano atual.\n",
    "            month (int, optional): Mês de atualização dos dados (ex: 9 para setembro). Se não fornecido, usa o mês atual.\n",
    "            *prefixes (str): Prefixos de arquivos para os quais as URLs serão geradas.\n",
    "\n",
    "        Returns:\n",
    "            list: Lista de URLs completas para os arquivos correspondentes aos prefixos.\n",
    "        \"\"\"\n",
    "        # Determinar o ano e o mês a serem usados\n",
    "        if year is None or month is None:\n",
    "            current_date = self.get_current_date()\n",
    "            year = year or current_date.year\n",
    "            month = month or current_date.month\n",
    "\n",
    "        # Validar os parâmetros de ano e mês\n",
    "        if not (1 <= month <= 12):\n",
    "            raise ValueError(\"O mês deve estar entre 1 e 12.\")\n",
    "        if year < 0:\n",
    "            raise ValueError(\"O ano deve ser um valor positivo.\")\n",
    "\n",
    "        # Construir a URL completa com base no ano e mês\n",
    "        period = f\"{year:04d}-{month:02d}\"\n",
    "        full_base_url = f\"{self.BASE_URL}/{period}\"\n",
    "\n",
    "        self.logger.info(f\"Gerando URLs para o período: {period}\")\n",
    "\n",
    "        urls = []\n",
    "\n",
    "        # Se nenhum prefixo for fornecido, usar todos os prefixos conhecidos\n",
    "        if not prefixes:\n",
    "            prefixes = self.FILE_PREFIXES\n",
    "\n",
    "        for prefix in prefixes:\n",
    "            if prefix in ['Municipios', 'Cnaes', 'Naturezas', 'Simples', 'Qualificacoes', 'Paises', 'Motivos']:\n",
    "                urls.append(f\"{full_base_url}/{prefix}.zip\")\n",
    "            elif prefix in self.FILE_PREFIXES:\n",
    "                urls.extend([f\"{full_base_url}/{prefix}{i}.zip\" for i in range(10)])\n",
    "            else:\n",
    "                self.logger.warning(f\"Prefixo '{prefix}' não reconhecido!\")\n",
    "\n",
    "        self.logger.info(f\"{len(urls)} URLs geradas.\")\n",
    "        return urls\n",
    "\n",
    "    def fetch_data(self, url, log_accumulator=None, max_attempts=15, wait_time=180):\n",
    "        \"\"\"\n",
    "        Baixa um arquivo da URL especificada e o salva no caminho de saída especificado. Em caso de \n",
    "        falha na tentativa de download, tentará novamente até o número máximo de tentativas ser atingido.\n",
    "\n",
    "        Args:\n",
    "            url (str): URL de onde o arquivo será baixado.\n",
    "            log_accumulator (list, optional): Um acumulador para armazenar mensagens de log. Padrão para None.\n",
    "            max_attempts (int, optional): Número máximo de tentativas de download. Padrão para 15.\n",
    "            wait_time (int, optional): Tempo de espera entre tentativas em segundos. Padrão para 180.\n",
    "\n",
    "        Raises:\n",
    "            Exception: Se o número máximo de tentativas for atingido sem sucesso.\n",
    "\n",
    "        Returns:\n",
    "            str: Caminho completo do arquivo baixado.\n",
    "        \"\"\"\n",
    "        os.makedirs(self.SAVE_BASE_PATH, exist_ok=True)\n",
    "\n",
    "        file_name = url.split('/')[-1]\n",
    "        file_path = os.path.join(self.SAVE_BASE_PATH, file_name)\n",
    "\n",
    "        headers = {\n",
    "            \"User-Agent\": (\n",
    "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                \"Chrome/91.0.4472.124 Safari/537.36\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        for attempt in range(1, max_attempts + 1):\n",
    "            try:\n",
    "                self.logger.info(f\"Baixando {url} (Tentativa {attempt}/{max_attempts})\")\n",
    "                response = requests.get(url, headers=headers)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                with open(file_path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "                self.logger.info(f\"Download concluído: {file_path}\")\n",
    "                return file_path\n",
    "\n",
    "            except requests.RequestException as e:\n",
    "                msg = f\"Tentativa {attempt} de {max_attempts} falhou. Erro: {e}\"\n",
    "                self.logger.error(msg)\n",
    "                if log_accumulator is not None:\n",
    "                    log_accumulator.append([msg, \"ERRO NO REQUEST! RETRY SENDO FEITO\"])\n",
    "\n",
    "                if attempt == max_attempts:\n",
    "                    final_msg = \"Número máximo de tentativas atingido. Download falhou.\"\n",
    "                    self.logger.error(final_msg)\n",
    "                    if log_accumulator is not None:\n",
    "                        log_accumulator.append([final_msg])\n",
    "                    raise Exception(final_msg) from e\n",
    "\n",
    "                self.logger.info(f\"Aguardando {wait_time} segundos antes da próxima tentativa...\")\n",
    "                time.sleep(wait_time)\n",
    "\n",
    "    def download_and_unzip(self, url, output_base_path=None, log_accumulator=None):\n",
    "        \"\"\"\n",
    "        Baixa um arquivo zip da URL fornecida, descompacta e salva no caminho especificado.\n",
    "\n",
    "        Args:\n",
    "            url (str): URL de onde o arquivo zip será baixado.\n",
    "            output_base_path (str, optional): Caminho base onde os dados descompactados serão armazenados. \n",
    "                                              Se não for fornecido, usa o caminho definido no inicializador.\n",
    "            log_accumulator (list, optional): Um acumulador para armazenar mensagens de log. Padrão para None.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Um tuple contendo a URL e uma mensagem indicando \"Success\" ou a razão da falha.\n",
    "        \"\"\"\n",
    "        if output_base_path is None:\n",
    "            output_base_path = self.BASE_OUTPUT_PATH\n",
    "\n",
    "        try:\n",
    "            zip_file_path = self.fetch_data(url, log_accumulator)\n",
    "            self.unzip_files(zip_file_path, output_base_path, log_accumulator)\n",
    "            self.logger.info(f\"Arquivo {url} foi descompactado com sucesso para {output_base_path}\")\n",
    "            return (url, \"Success\")\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Falha ao baixar e descompactar {url}. Erro: {e}\"\n",
    "            self.logger.error(error_msg)\n",
    "            return (url, str(e))\n",
    "\n",
    "    def unzip_files(self, zip_file_path, output_base_path, log_accumulator=None):\n",
    "        \"\"\"\n",
    "        Descompacta o arquivo fornecido e salva no caminho especificado. Substitui arquivos existentes.\n",
    "\n",
    "        Args:\n",
    "            zip_file_path (str): Caminho completo do arquivo zip que precisa ser descompactado.\n",
    "            output_base_path (str): Caminho base onde o arquivo descompactado deve ser salvo.\n",
    "            log_accumulator (list, optional): Uma lista que pode ser fornecida para acumular mensagens de log, \n",
    "                                             útil para rastrear erros ou informações. Se não for fornecido, \n",
    "                                             apenas os logs padrão serão usados.\n",
    "        \"\"\"\n",
    "        zip_file_name = os.path.basename(zip_file_path)\n",
    "        prefix = next((p for p in self.FILE_PREFIXES if zip_file_name.startswith(p)), None)\n",
    "\n",
    "        if not prefix:\n",
    "            msg = f\"O arquivo {zip_file_path} não corresponde aos padrões esperados.\"\n",
    "            self.logger.error(msg)\n",
    "            if log_accumulator is not None:\n",
    "                log_accumulator.append([msg])\n",
    "            return\n",
    "\n",
    "        output_path = os.path.join(output_base_path, prefix)\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            self.logger.info(f\"Descompactando {zip_file_path} para {output_path}\")\n",
    "            with zipfile.ZipFile(zip_file_path, \"r\") as z:\n",
    "                file_inside_zip = z.namelist()[0]\n",
    "                number_in_zip = ''.join(filter(str.isdigit, zip_file_name))\n",
    "                final_file_path = os.path.join(output_path, f\"{prefix}{number_in_zip}.csv\")\n",
    "\n",
    "                if os.path.exists(final_file_path):\n",
    "                    os.remove(final_file_path)\n",
    "                    self.logger.info(f\"Arquivo existente {final_file_path} removido.\")\n",
    "\n",
    "                with z.open(file_inside_zip) as zf, open(final_file_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(zf, f_out)\n",
    "            self.logger.info(f\"Arquivo descompactado com sucesso: {final_file_path}\")\n",
    "        except zipfile.BadZipFile as e:\n",
    "            msg = f\"Erro ao descompactar {zip_file_path}: {e}\"\n",
    "            self.logger.error(msg)\n",
    "            if log_accumulator is not None:\n",
    "                log_accumulator.append([msg])\n",
    "            raise\n",
    "        finally:\n",
    "            if os.path.exists(zip_file_path):\n",
    "                os.remove(zip_file_path)\n",
    "                self.logger.info(f\"Arquivo zip {zip_file_path} removido após descompactação.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16f4142b-abd9-4f3d-8752-74ce706db17c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class ReceitaCNPJApi:\n",
    "    \"\"\"\n",
    "    Classe ReceitaCNPJApi:\n",
    "\n",
    "    Esta classe é responsável por interagir com a API de Dados Abertos da Receita Federal, facilitando \n",
    "    a obtenção de informações relacionadas a CNPJs, como empresas, sócios, municípios, entre outros. \n",
    "    A classe inclui funcionalidades para baixar, descompactar e salvar dados, além de utilitários \n",
    "    para manipular e consultar URLs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prefixos de arquivos que podem ser baixados da API.\n",
    "    FILE_PREFIXES = [\n",
    "        'Estabelecimentos', 'Municipios', 'Simples', 'Empresas', \n",
    "        'Cnaes', 'Socios', 'Naturezas', 'Qualificacoes', 'Paises', 'Motivos'\n",
    "    ]\n",
    "\n",
    "    # Número máximo de tentativas ao fazer uma solicitação para a API.\n",
    "    MAX_ATTEMPTS = 15\n",
    "\n",
    "    # Tempo de espera entre tentativas de solicitações (em segundos).\n",
    "    WAIT_TIME = 180\n",
    "\n",
    "    # URL base da API de dados abertos da Receita Federal, sem incluir o ano/mês.\n",
    "    BASE_URL = \"https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj\"\n",
    "\n",
    "    def __init__(self, base_output_path: str = \"./output\", save_base_path: str = \"./temp\"):\n",
    "        \"\"\"\n",
    "        Inicializador da classe. Configura o logger para registrar atividades e erros e define os caminhos base \n",
    "        para salvar os arquivos descompactados e baixados.\n",
    "\n",
    "        Args:\n",
    "            base_output_path (str, optional): Caminho base para salvar os arquivos descompactados. Padrão para \"./output\".\n",
    "            save_base_path (str, optional): Caminho base para salvar os arquivos baixados. Padrão para \"./temp\".\n",
    "        \"\"\"\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "        self.BASE_OUTPUT_PATH = base_output_path\n",
    "        self.SAVE_BASE_PATH = save_base_path\n",
    "\n",
    "    @staticmethod\n",
    "    def get_current_date():\n",
    "        \"\"\"\n",
    "        Retorna a data atual.\n",
    "\n",
    "        Returns:\n",
    "            date: Data atual.\n",
    "        \"\"\"\n",
    "        return datetime.today().date()\n",
    "\n",
    "    def get_output_path_for_prefix(self, prefix):\n",
    "        \"\"\"\n",
    "        Gera o caminho completo de saída para armazenar os dados com base em um prefixo fornecido.\n",
    "\n",
    "        Args:\n",
    "            prefix (str): Prefixo do arquivo para o qual o caminho de saída será gerado.\n",
    "\n",
    "        Returns:\n",
    "            str: Caminho completo de saída para o prefixo especificado.\n",
    "        \"\"\"\n",
    "        return os.path.join(self.BASE_OUTPUT_PATH, prefix, f\"{prefix}.csv\")\n",
    "\n",
    "    def lista_urls_receita(self, *prefixes, year: int = None, month: int = None):\n",
    "        \"\"\"\n",
    "        Gera uma lista de URLs para download com base nos prefixos fornecidos e no período especificado. \n",
    "        Se nenhum prefixo for fornecido, o método gerará URLs para todos os tipos de arquivos conhecidos.\n",
    "\n",
    "        Args:\n",
    "            *prefixes (str): Prefixos de arquivos para os quais as URLs serão geradas.\n",
    "            year (int, optional): Ano de atualização dos dados (ex: 2024). Se não fornecido, usa o ano atual.\n",
    "            month (int, optional): Mês de atualização dos dados (ex: 9 para setembro). Se não fornecido, usa o mês atual.\n",
    "\n",
    "        Returns:\n",
    "            list: Lista de URLs completas para os arquivos correspondentes aos prefixos.\n",
    "        \"\"\"\n",
    "        # Determinar o ano e o mês a serem usados\n",
    "        if year is None or month is None:\n",
    "            current_date = self.get_current_date()\n",
    "            year = year or current_date.year\n",
    "            month = month or current_date.month\n",
    "\n",
    "        # Validar os parâmetros de ano e mês\n",
    "        if not (1 <= month <= 12):\n",
    "            raise ValueError(\"O mês deve estar entre 1 e 12.\")\n",
    "        if year < 0:\n",
    "            raise ValueError(\"O ano deve ser um valor positivo.\")\n",
    "\n",
    "        # Construir a URL completa com base no ano e mês\n",
    "        period = f\"{year:04d}-{month:02d}\"\n",
    "        full_base_url = f\"{self.BASE_URL}/{period}\"\n",
    "\n",
    "        self.logger.info(f\"Gerando URLs para o período: {period}\")\n",
    "\n",
    "        urls = []\n",
    "\n",
    "        # Se nenhum prefixo for fornecido, usar todos os prefixos conhecidos\n",
    "        if not prefixes:\n",
    "            prefixes = self.FILE_PREFIXES\n",
    "\n",
    "        for prefix in prefixes:\n",
    "            if prefix in ['Municipios', 'Cnaes', 'Naturezas', 'Simples', 'Qualificacoes', 'Paises', 'Motivos']:\n",
    "                urls.append(f\"{full_base_url}/{prefix}.zip\")\n",
    "            elif prefix in self.FILE_PREFIXES:\n",
    "                urls.extend([f\"{full_base_url}/{prefix}{i}.zip\" for i in range(10)])\n",
    "            else:\n",
    "                self.logger.warning(f\"Prefixo '{prefix}' não reconhecido!\")\n",
    "\n",
    "        self.logger.info(f\"{len(urls)} URLs geradas.\")\n",
    "        return urls\n",
    "\n",
    "    def fetch_data(self, url, log_accumulator=None, max_attempts=15, wait_time=180):\n",
    "        \"\"\"\n",
    "        Baixa um arquivo da URL especificada e o salva no caminho de saída especificado. Em caso de \n",
    "        falha na tentativa de download, tentará novamente até o número máximo de tentativas ser atingido.\n",
    "\n",
    "        Args:\n",
    "            url (str): URL de onde o arquivo será baixado.\n",
    "            log_accumulator (list, optional): Um acumulador para armazenar mensagens de log. Padrão para None.\n",
    "            max_attempts (int, optional): Número máximo de tentativas de download. Padrão para 15.\n",
    "            wait_time (int, optional): Tempo de espera entre tentativas em segundos. Padrão para 180.\n",
    "\n",
    "        Raises:\n",
    "            Exception: Se o número máximo de tentativas for atingido sem sucesso.\n",
    "\n",
    "        Returns:\n",
    "            str: Caminho completo do arquivo baixado.\n",
    "        \"\"\"\n",
    "        os.makedirs(self.SAVE_BASE_PATH, exist_ok=True)\n",
    "\n",
    "        file_name = url.split('/')[-1]\n",
    "        file_path = os.path.join(self.SAVE_BASE_PATH, file_name)\n",
    "\n",
    "        headers = {\n",
    "            \"User-Agent\": (\n",
    "                \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                \"Chrome/91.0.4472.124 Safari/537.36\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        for attempt in range(1, max_attempts + 1):\n",
    "            try:\n",
    "                self.logger.info(f\"Baixando {url} (Tentativa {attempt}/{max_attempts})\")\n",
    "                response = requests.get(url, headers=headers)\n",
    "                response.raise_for_status()\n",
    "\n",
    "                with open(file_path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "                self.logger.info(f\"Download concluído: {file_path}\")\n",
    "                return file_path\n",
    "\n",
    "            except requests.RequestException as e:\n",
    "                msg = f\"Tentativa {attempt} de {max_attempts} falhou. Erro: {e}\"\n",
    "                self.logger.error(msg)\n",
    "                if log_accumulator is not None:\n",
    "                    log_accumulator.append([msg, \"ERRO NO REQUEST! RETRY SENDO FEITO\"])\n",
    "\n",
    "                if attempt == max_attempts:\n",
    "                    final_msg = \"Número máximo de tentativas atingido. Download falhou.\"\n",
    "                    self.logger.error(final_msg)\n",
    "                    if log_accumulator is not None:\n",
    "                        log_accumulator.append([final_msg])\n",
    "                    raise Exception(final_msg) from e\n",
    "\n",
    "                self.logger.info(f\"Aguardando {wait_time} segundos antes da próxima tentativa...\")\n",
    "                time.sleep(wait_time)\n",
    "\n",
    "    def download_and_unzip(self, url, output_base_path=None, log_accumulator=None):\n",
    "        \"\"\"\n",
    "        Baixa um arquivo zip da URL fornecida, descompacta e salva no caminho especificado.\n",
    "\n",
    "        Args:\n",
    "            url (str): URL de onde o arquivo zip será baixado.\n",
    "            output_base_path (str, optional): Caminho base onde os dados descompactados serão armazenados. \n",
    "                                              Se não for fornecido, usa o caminho definido no inicializador.\n",
    "            log_accumulator (list, optional): Um acumulador para armazenar mensagens de log. Padrão para None.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Um tuple contendo a URL e uma mensagem indicando \"Success\" ou a razão da falha.\n",
    "        \"\"\"\n",
    "        if output_base_path is None:\n",
    "            output_base_path = self.BASE_OUTPUT_PATH\n",
    "\n",
    "        try:\n",
    "            zip_file_path = self.fetch_data(url, log_accumulator)\n",
    "            self.unzip_files(zip_file_path, output_base_path, log_accumulator)\n",
    "            self.logger.info(f\"Arquivo {url} foi descompactado com sucesso para {output_base_path}\")\n",
    "            return (url, \"Success\")\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Falha ao baixar e descompactar {url}. Erro: {e}\"\n",
    "            self.logger.error(error_msg)\n",
    "            return (url, str(e))\n",
    "\n",
    "    def unzip_files(self, zip_file_path, output_base_path, log_accumulator=None):\n",
    "        \"\"\"\n",
    "        Descompacta o arquivo fornecido e salva no caminho especificado. Substitui arquivos existentes.\n",
    "\n",
    "        Args:\n",
    "            zip_file_path (str): Caminho completo do arquivo zip que precisa ser descompactado.\n",
    "            output_base_path (str): Caminho base onde o arquivo descompactado deve ser salvo.\n",
    "            log_accumulator (list, optional): Uma lista que pode ser fornecida para acumular mensagens de log, \n",
    "                                             útil para rastrear erros ou informações. Se não for fornecido, \n",
    "                                             apenas os logs padrão serão usados.\n",
    "        \"\"\"\n",
    "        zip_file_name = os.path.basename(zip_file_path)\n",
    "        prefix = next((p for p in self.FILE_PREFIXES if zip_file_name.startswith(p)), None)\n",
    "\n",
    "        if not prefix:\n",
    "            msg = f\"O arquivo {zip_file_path} não corresponde aos padrões esperados.\"\n",
    "            self.logger.error(msg)\n",
    "            if log_accumulator is not None:\n",
    "                log_accumulator.append([msg])\n",
    "            return\n",
    "\n",
    "        output_path = os.path.join(output_base_path, prefix)\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            self.logger.info(f\"Descompactando {zip_file_path} para {output_path}\")\n",
    "            with zipfile.ZipFile(zip_file_path, \"r\") as z:\n",
    "                file_inside_zip = z.namelist()[0]\n",
    "                number_in_zip = ''.join(filter(str.isdigit, zip_file_name))\n",
    "                final_file_path = os.path.join(output_path, f\"{prefix}{number_in_zip}.csv\")\n",
    "\n",
    "                if os.path.exists(final_file_path):\n",
    "                    os.remove(final_file_path)\n",
    "                    self.logger.info(f\"Arquivo existente {final_file_path} removido.\")\n",
    "\n",
    "                with z.open(file_inside_zip) as zf, open(final_file_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(zf, f_out)\n",
    "            self.logger.info(f\"Arquivo descompactado com sucesso: {final_file_path}\")\n",
    "        except zipfile.BadZipFile as e:\n",
    "            msg = f\"Erro ao descompactar {zip_file_path}: {e}\"\n",
    "            self.logger.error(msg)\n",
    "            if log_accumulator is not None:\n",
    "                log_accumulator.append([msg])\n",
    "            raise\n",
    "        finally:\n",
    "            if os.path.exists(zip_file_path):\n",
    "                os.remove(zip_file_path)\n",
    "                self.logger.info(f\"Arquivo zip {zip_file_path} removido após descompactação.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb7d6bf-8d1e-432a-8a6f-e5065b4e52ff",
   "metadata": {},
   "source": [
    "## Extrando de forma uni thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d97fbfb-abc8-4d6a-bb60-c0e4c212122f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Gerando URLs para o período: 2024-09\n",
      "INFO:__main__:37 URLs geradas.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos0.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos1.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos2.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos3.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos4.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos5.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos6.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos7.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos8.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos9.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Municipios.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Simples.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas0.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas1.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas2.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas3.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas4.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas5.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas6.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas7.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas8.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas9.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Cnaes.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Socios0.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Socios1.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Socios2.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Socios3.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Socios4.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Socios5.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Socios6.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Socios7.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Socios8.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Socios9.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Naturezas.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Qualificacoes.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Paises.zip\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Motivos.zip\n"
     ]
    }
   ],
   "source": [
    "# Inicializa a classe sem parâmetros\n",
    "api = ReceitaCNPJApi()\n",
    "\n",
    "# Especifica o ano e o mês desejados\n",
    "year = 2024\n",
    "month = 9\n",
    "\n",
    "# Gera a lista de URLs para todos os prefixos no período especificado\n",
    "urls = api.lista_urls_receita(year=year, month=month)\n",
    "\n",
    "for url in urls:\n",
    "    print(f\"Result for {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc67ddb7-74a9-4b92-9006-e82bd415f2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Gerando URLs para o período: 2024-09\n",
      "INFO:__main__:37 URLs geradas.\n",
      "INFO:__main__:Baixando https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos0.zip (Tentativa 1/15)\n"
     ]
    }
   ],
   "source": [
    "# Inicializa a classe sem parâmetros\n",
    "api = ReceitaCNPJApi()\n",
    "\n",
    "# Especifica o ano e o mês desejados\n",
    "year = 2024\n",
    "month = 9\n",
    "\n",
    "# Gera a lista de URLs para todos os prefixos no período especificado\n",
    "urls = api.lista_urls_receita(year=year, month=month)\n",
    "\n",
    "for url in urls:\n",
    "    result = api.download_and_unzip(url)\n",
    "    print(f\"Result for {url}: {result[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d104995-a1f8-489c-9824-5c2de9d6066e",
   "metadata": {},
   "source": [
    "## Usando multi thread do python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "425f4587-d39f-4fa3-86d8-8bddfa1bd64a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Gerando URLs para o período: 2024-09\n",
      "INFO:__main__:6 URLs geradas.\n",
      "INFO:__main__:Baixando https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Municipios.zip (Tentativa 1/15)\n",
      "INFO:__main__:Baixando https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Cnaes.zip (Tentativa 1/15)\n",
      "INFO:__main__:Baixando https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Naturezas.zip (Tentativa 1/15)\n",
      "INFO:__main__:Download concluído: ./temp\\Naturezas.zip\n",
      "INFO:__main__:Descompactando ./temp\\Naturezas.zip para ./output\\Naturezas\n",
      "INFO:__main__:Arquivo existente ./output\\Naturezas\\Naturezas.csv removido.\n",
      "INFO:__main__:Arquivo descompactado com sucesso: ./output\\Naturezas\\Naturezas.csv\n",
      "INFO:__main__:Arquivo zip ./temp\\Naturezas.zip removido após descompactação.\n",
      "INFO:__main__:Arquivo https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Naturezas.zip foi descompactado com sucesso para ./output\n",
      "INFO:__main__:Baixando https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Qualificacoes.zip (Tentativa 1/15)\n",
      "INFO:__main__:Download concluído: ./temp\\Cnaes.zip\n",
      "INFO:__main__:Descompactando ./temp\\Cnaes.zip para ./output\\Cnaes\n",
      "INFO:__main__:Arquivo existente ./output\\Cnaes\\Cnaes.csv removido.\n",
      "INFO:__main__:Arquivo descompactado com sucesso: ./output\\Cnaes\\Cnaes.csv\n",
      "INFO:__main__:Arquivo zip ./temp\\Cnaes.zip removido após descompactação.\n",
      "INFO:__main__:Arquivo https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Cnaes.zip foi descompactado com sucesso para ./output\n",
      "INFO:__main__:Baixando https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Paises.zip (Tentativa 1/15)\n",
      "INFO:__main__:Download concluído: ./temp\\Municipios.zip\n",
      "INFO:__main__:Descompactando ./temp\\Municipios.zip para ./output\\Municipios\n",
      "INFO:__main__:Arquivo existente ./output\\Municipios\\Municipios.csv removido.\n",
      "INFO:__main__:Arquivo descompactado com sucesso: ./output\\Municipios\\Municipios.csv\n",
      "INFO:__main__:Arquivo zip ./temp\\Municipios.zip removido após descompactação.\n",
      "INFO:__main__:Arquivo https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Municipios.zip foi descompactado com sucesso para ./output\n",
      "INFO:__main__:Baixando https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Motivos.zip (Tentativa 1/15)\n",
      "INFO:__main__:Download concluído: ./temp\\Qualificacoes.zip\n",
      "INFO:__main__:Descompactando ./temp\\Qualificacoes.zip para ./output\\Qualificacoes\n",
      "INFO:__main__:Arquivo existente ./output\\Qualificacoes\\Qualificacoes.csv removido.\n",
      "INFO:__main__:Arquivo descompactado com sucesso: ./output\\Qualificacoes\\Qualificacoes.csv\n",
      "INFO:__main__:Arquivo zip ./temp\\Qualificacoes.zip removido após descompactação.\n",
      "INFO:__main__:Arquivo https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Qualificacoes.zip foi descompactado com sucesso para ./output\n",
      "INFO:__main__:Download concluído: ./temp\\Paises.zip\n",
      "INFO:__main__:Descompactando ./temp\\Paises.zip para ./output\\Paises\n",
      "INFO:__main__:Arquivo existente ./output\\Paises\\Paises.csv removido.\n",
      "INFO:__main__:Arquivo descompactado com sucesso: ./output\\Paises\\Paises.csv\n",
      "INFO:__main__:Arquivo zip ./temp\\Paises.zip removido após descompactação.\n",
      "INFO:__main__:Arquivo https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Paises.zip foi descompactado com sucesso para ./output\n",
      "INFO:__main__:Download concluído: ./temp\\Motivos.zip\n",
      "INFO:__main__:Descompactando ./temp\\Motivos.zip para ./output\\Motivos\n",
      "INFO:__main__:Arquivo existente ./output\\Motivos\\Motivos.csv removido.\n",
      "INFO:__main__:Arquivo descompactado com sucesso: ./output\\Motivos\\Motivos.csv\n",
      "INFO:__main__:Arquivo zip ./temp\\Motivos.zip removido após descompactação.\n",
      "INFO:__main__:Arquivo https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Motivos.zip foi descompactado com sucesso para ./output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Municipios.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Cnaes.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Naturezas.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Qualificacoes.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Paises.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Motivos.zip: Success\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Inicializa a classe sem parâmetros\n",
    "api = ReceitaCNPJApi()\n",
    "\n",
    "# Especifica o ano e o mês desejados\n",
    "year = 2024\n",
    "month = 9\n",
    "\n",
    "# Gera a lista de URLs para múltiplos prefixos no período especificado\n",
    "urls = api.lista_urls_receita(\n",
    "    'Municipios', 'Cnaes', 'Naturezas', 'Qualificacoes', 'Paises', 'Motivos',\n",
    "    year=year,\n",
    "    month=month\n",
    ")\n",
    "\n",
    "# Função para processar cada URL\n",
    "def process_url(url):\n",
    "    return api.download_and_unzip(url)\n",
    "\n",
    "# Baixa e descompacta as URLs utilizando ThreadPoolExecutor para paralelizar\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    results = list(executor.map(process_url, urls))\n",
    "\n",
    "# Imprime os resultados\n",
    "for url, result in zip(urls, results):\n",
    "    print(f\"Result for {url}: {result[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a2d2f7d-3865-43a3-a6d6-9f5ed0c3cc12",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Gerando URLs para o período: 2024-09\n",
      "INFO:__main__:10 URLs geradas.\n",
      "INFO:__main__:Baixando https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas0.zip (Tentativa 1/15)\n",
      "INFO:__main__:Baixando https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas2.zip (Tentativa 1/15)\n",
      "INFO:__main__:Baixando https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas1.zip (Tentativa 1/15)\n",
      "INFO:__main__:Download concluído: ./temp\\Empresas1.zip\n",
      "INFO:__main__:Descompactando ./temp\\Empresas1.zip para ./output\\Empresas\n",
      "INFO:__main__:Arquivo existente ./output\\Empresas\\Empresas1.csv removido.\n",
      "INFO:__main__:Arquivo descompactado com sucesso: ./output\\Empresas\\Empresas1.csv\n",
      "INFO:__main__:Arquivo zip ./temp\\Empresas1.zip removido após descompactação.\n",
      "INFO:__main__:Arquivo https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas1.zip foi descompactado com sucesso para ./output\n",
      "INFO:__main__:Baixando https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas3.zip (Tentativa 1/15)\n",
      "INFO:__main__:Download concluído: ./temp\\Empresas2.zip\n",
      "INFO:__main__:Descompactando ./temp\\Empresas2.zip para ./output\\Empresas\n",
      "INFO:__main__:Arquivo existente ./output\\Empresas\\Empresas2.csv removido.\n",
      "INFO:__main__:Arquivo descompactado com sucesso: ./output\\Empresas\\Empresas2.csv\n",
      "INFO:__main__:Arquivo zip ./temp\\Empresas2.zip removido após descompactação.\n",
      "INFO:__main__:Arquivo https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas2.zip foi descompactado com sucesso para ./output\n",
      "INFO:__main__:Baixando https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas4.zip (Tentativa 1/15)\n",
      "INFO:__main__:Download concluído: ./temp\\Empresas3.zip\n",
      "INFO:__main__:Descompactando ./temp\\Empresas3.zip para ./output\\Empresas\n",
      "INFO:__main__:Arquivo existente ./output\\Empresas\\Empresas3.csv removido.\n",
      "INFO:__main__:Arquivo descompactado com sucesso: ./output\\Empresas\\Empresas3.csv\n",
      "INFO:__main__:Arquivo zip ./temp\\Empresas3.zip removido após descompactação.\n",
      "INFO:__main__:Arquivo https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas3.zip foi descompactado com sucesso para ./output\n",
      "INFO:__main__:Baixando https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas5.zip (Tentativa 1/15)\n",
      "INFO:__main__:Download concluído: ./temp\\Empresas4.zip\n",
      "INFO:__main__:Descompactando ./temp\\Empresas4.zip para ./output\\Empresas\n",
      "INFO:__main__:Arquivo existente ./output\\Empresas\\Empresas4.csv removido.\n",
      "INFO:__main__:Arquivo descompactado com sucesso: ./output\\Empresas\\Empresas4.csv\n",
      "INFO:__main__:Arquivo zip ./temp\\Empresas4.zip removido após descompactação.\n",
      "INFO:__main__:Arquivo https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas4.zip foi descompactado com sucesso para ./output\n",
      "INFO:__main__:Baixando https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas6.zip (Tentativa 1/15)\n",
      "INFO:__main__:Download concluído: ./temp\\Empresas5.zip\n",
      "INFO:__main__:Descompactando ./temp\\Empresas5.zip para ./output\\Empresas\n",
      "INFO:__main__:Arquivo existente ./output\\Empresas\\Empresas5.csv removido.\n",
      "INFO:__main__:Arquivo descompactado com sucesso: ./output\\Empresas\\Empresas5.csv\n",
      "INFO:__main__:Arquivo zip ./temp\\Empresas5.zip removido após descompactação.\n",
      "INFO:__main__:Arquivo https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas5.zip foi descompactado com sucesso para ./output\n",
      "INFO:__main__:Baixando https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas7.zip (Tentativa 1/15)\n",
      "INFO:__main__:Download concluído: ./temp\\Empresas6.zip\n",
      "INFO:__main__:Descompactando ./temp\\Empresas6.zip para ./output\\Empresas\n",
      "INFO:__main__:Arquivo existente ./output\\Empresas\\Empresas6.csv removido.\n",
      "INFO:__main__:Arquivo descompactado com sucesso: ./output\\Empresas\\Empresas6.csv\n",
      "INFO:__main__:Arquivo zip ./temp\\Empresas6.zip removido após descompactação.\n",
      "INFO:__main__:Arquivo https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas6.zip foi descompactado com sucesso para ./output\n",
      "INFO:__main__:Baixando https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas8.zip (Tentativa 1/15)\n",
      "INFO:__main__:Download concluído: ./temp\\Empresas7.zip\n",
      "INFO:__main__:Descompactando ./temp\\Empresas7.zip para ./output\\Empresas\n",
      "INFO:__main__:Arquivo existente ./output\\Empresas\\Empresas7.csv removido.\n",
      "INFO:__main__:Arquivo descompactado com sucesso: ./output\\Empresas\\Empresas7.csv\n",
      "INFO:__main__:Arquivo zip ./temp\\Empresas7.zip removido após descompactação.\n",
      "INFO:__main__:Arquivo https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas7.zip foi descompactado com sucesso para ./output\n",
      "INFO:__main__:Baixando https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas9.zip (Tentativa 1/15)\n",
      "INFO:__main__:Download concluído: ./temp\\Empresas8.zip\n",
      "INFO:__main__:Descompactando ./temp\\Empresas8.zip para ./output\\Empresas\n",
      "INFO:__main__:Arquivo existente ./output\\Empresas\\Empresas8.csv removido.\n",
      "INFO:__main__:Arquivo descompactado com sucesso: ./output\\Empresas\\Empresas8.csv\n",
      "INFO:__main__:Arquivo zip ./temp\\Empresas8.zip removido após descompactação.\n",
      "INFO:__main__:Arquivo https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas8.zip foi descompactado com sucesso para ./output\n",
      "INFO:__main__:Download concluído: ./temp\\Empresas0.zip\n",
      "INFO:__main__:Descompactando ./temp\\Empresas0.zip para ./output\\Empresas\n",
      "INFO:__main__:Arquivo existente ./output\\Empresas\\Empresas0.csv removido.\n",
      "INFO:__main__:Arquivo descompactado com sucesso: ./output\\Empresas\\Empresas0.csv\n",
      "INFO:__main__:Arquivo zip ./temp\\Empresas0.zip removido após descompactação.\n",
      "INFO:__main__:Arquivo https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas0.zip foi descompactado com sucesso para ./output\n",
      "INFO:__main__:Download concluído: ./temp\\Empresas9.zip\n",
      "INFO:__main__:Descompactando ./temp\\Empresas9.zip para ./output\\Empresas\n",
      "INFO:__main__:Arquivo existente ./output\\Empresas\\Empresas9.csv removido.\n",
      "INFO:__main__:Arquivo descompactado com sucesso: ./output\\Empresas\\Empresas9.csv\n",
      "INFO:__main__:Arquivo zip ./temp\\Empresas9.zip removido após descompactação.\n",
      "INFO:__main__:Arquivo https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas9.zip foi descompactado com sucesso para ./output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas0.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas1.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas2.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas3.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas4.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas5.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas6.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas7.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas8.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Empresas9.zip: Success\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Inicializa a classe sem parâmetros\n",
    "api = ReceitaCNPJApi()\n",
    "\n",
    "# Especifica o ano e o mês desejados\n",
    "year = 2024\n",
    "month = 9\n",
    "\n",
    "# Gera a lista de URLs apenas para o prefixo 'Empresas' no período especificado\n",
    "urls = api.lista_urls_receita(\n",
    "    'Empresas',\n",
    "    year=year,\n",
    "    month=month\n",
    ")\n",
    "\n",
    "# Função para processar cada URL\n",
    "def process_url(url):\n",
    "    return api.download_and_unzip(url)\n",
    "\n",
    "# Baixa e descompacta as URLs utilizando ThreadPoolExecutor para paralelizar\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    results = list(executor.map(process_url, urls))\n",
    "\n",
    "# Imprime os resultados\n",
    "for url, result in zip(urls, results):\n",
    "    print(f\"Result for {url}: {result[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "570113af-8663-4797-9868-3253087fcbdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Gerando URLs para o período: 2024-09\n",
      "INFO:__main__:10 URLs geradas.\n",
      "INFO:__main__:Baixando https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos0.zip (Tentativa 1/15)\n",
      "INFO:__main__:Baixando https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos2.zip (Tentativa 1/15)\n",
      "INFO:__main__:Baixando https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos1.zip (Tentativa 1/15)\n",
      "INFO:__main__:Download concluído: ./temp\\Estabelecimentos2.zip\n",
      "INFO:__main__:Descompactando ./temp\\Estabelecimentos2.zip para ./output\\Estabelecimentos\n",
      "INFO:__main__:Arquivo existente ./output\\Estabelecimentos\\Estabelecimentos2.csv removido.\n",
      "INFO:__main__:Download concluído: ./temp\\Estabelecimentos1.zip\n",
      "INFO:__main__:Descompactando ./temp\\Estabelecimentos1.zip para ./output\\Estabelecimentos\n",
      "INFO:__main__:Arquivo existente ./output\\Estabelecimentos\\Estabelecimentos1.csv removido.\n",
      "INFO:__main__:Arquivo descompactado com sucesso: ./output\\Estabelecimentos\\Estabelecimentos2.csv\n",
      "INFO:__main__:Arquivo zip ./temp\\Estabelecimentos2.zip removido após descompactação.\n",
      "INFO:__main__:Arquivo https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos2.zip foi descompactado com sucesso para ./output\n",
      "INFO:__main__:Baixando https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos3.zip (Tentativa 1/15)\n",
      "INFO:__main__:Arquivo descompactado com sucesso: ./output\\Estabelecimentos\\Estabelecimentos1.csv\n",
      "INFO:__main__:Arquivo zip ./temp\\Estabelecimentos1.zip removido após descompactação.\n",
      "INFO:__main__:Arquivo https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos1.zip foi descompactado com sucesso para ./output\n",
      "INFO:__main__:Baixando https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos4.zip (Tentativa 1/15)\n",
      "INFO:__main__:Download concluído: ./temp\\Estabelecimentos3.zip\n",
      "INFO:__main__:Descompactando ./temp\\Estabelecimentos3.zip para ./output\\Estabelecimentos\n",
      "INFO:__main__:Arquivo existente ./output\\Estabelecimentos\\Estabelecimentos3.csv removido.\n",
      "INFO:__main__:Arquivo descompactado com sucesso: ./output\\Estabelecimentos\\Estabelecimentos3.csv\n",
      "INFO:__main__:Arquivo zip ./temp\\Estabelecimentos3.zip removido após descompactação.\n",
      "INFO:__main__:Arquivo https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos3.zip foi descompactado com sucesso para ./output\n",
      "INFO:__main__:Baixando https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos5.zip (Tentativa 1/15)\n",
      "INFO:__main__:Download concluído: ./temp\\Estabelecimentos4.zip\n",
      "INFO:__main__:Descompactando ./temp\\Estabelecimentos4.zip para ./output\\Estabelecimentos\n",
      "INFO:__main__:Arquivo existente ./output\\Estabelecimentos\\Estabelecimentos4.csv removido.\n",
      "INFO:__main__:Arquivo descompactado com sucesso: ./output\\Estabelecimentos\\Estabelecimentos4.csv\n",
      "INFO:__main__:Arquivo zip ./temp\\Estabelecimentos4.zip removido após descompactação.\n",
      "INFO:__main__:Arquivo https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos4.zip foi descompactado com sucesso para ./output\n",
      "INFO:__main__:Baixando https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos6.zip (Tentativa 1/15)\n",
      "INFO:__main__:Download concluído: ./temp\\Estabelecimentos5.zip\n",
      "INFO:__main__:Descompactando ./temp\\Estabelecimentos5.zip para ./output\\Estabelecimentos\n",
      "INFO:__main__:Arquivo existente ./output\\Estabelecimentos\\Estabelecimentos5.csv removido.\n",
      "INFO:__main__:Arquivo descompactado com sucesso: ./output\\Estabelecimentos\\Estabelecimentos5.csv\n",
      "INFO:__main__:Arquivo zip ./temp\\Estabelecimentos5.zip removido após descompactação.\n",
      "INFO:__main__:Arquivo https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos5.zip foi descompactado com sucesso para ./output\n",
      "INFO:__main__:Baixando https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos7.zip (Tentativa 1/15)\n",
      "INFO:__main__:Download concluído: ./temp\\Estabelecimentos6.zip\n",
      "INFO:__main__:Descompactando ./temp\\Estabelecimentos6.zip para ./output\\Estabelecimentos\n",
      "INFO:__main__:Arquivo existente ./output\\Estabelecimentos\\Estabelecimentos6.csv removido.\n",
      "INFO:__main__:Arquivo descompactado com sucesso: ./output\\Estabelecimentos\\Estabelecimentos6.csv\n",
      "INFO:__main__:Arquivo zip ./temp\\Estabelecimentos6.zip removido após descompactação.\n",
      "INFO:__main__:Arquivo https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos6.zip foi descompactado com sucesso para ./output\n",
      "INFO:__main__:Baixando https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos8.zip (Tentativa 1/15)\n",
      "INFO:__main__:Download concluído: ./temp\\Estabelecimentos0.zip\n",
      "INFO:__main__:Descompactando ./temp\\Estabelecimentos0.zip para ./output\\Estabelecimentos\n",
      "INFO:__main__:Arquivo existente ./output\\Estabelecimentos\\Estabelecimentos0.csv removido.\n",
      "INFO:__main__:Arquivo descompactado com sucesso: ./output\\Estabelecimentos\\Estabelecimentos0.csv\n",
      "INFO:__main__:Arquivo zip ./temp\\Estabelecimentos0.zip removido após descompactação.\n",
      "INFO:__main__:Arquivo https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos0.zip foi descompactado com sucesso para ./output\n",
      "INFO:__main__:Baixando https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos9.zip (Tentativa 1/15)\n",
      "INFO:__main__:Download concluído: ./temp\\Estabelecimentos7.zip\n",
      "INFO:__main__:Descompactando ./temp\\Estabelecimentos7.zip para ./output\\Estabelecimentos\n",
      "INFO:__main__:Arquivo existente ./output\\Estabelecimentos\\Estabelecimentos7.csv removido.\n",
      "INFO:__main__:Arquivo descompactado com sucesso: ./output\\Estabelecimentos\\Estabelecimentos7.csv\n",
      "INFO:__main__:Arquivo zip ./temp\\Estabelecimentos7.zip removido após descompactação.\n",
      "INFO:__main__:Arquivo https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos7.zip foi descompactado com sucesso para ./output\n",
      "INFO:__main__:Download concluído: ./temp\\Estabelecimentos8.zip\n",
      "INFO:__main__:Descompactando ./temp\\Estabelecimentos8.zip para ./output\\Estabelecimentos\n",
      "INFO:__main__:Arquivo existente ./output\\Estabelecimentos\\Estabelecimentos8.csv removido.\n",
      "INFO:__main__:Download concluído: ./temp\\Estabelecimentos9.zip\n",
      "INFO:__main__:Descompactando ./temp\\Estabelecimentos9.zip para ./output\\Estabelecimentos\n",
      "INFO:__main__:Arquivo existente ./output\\Estabelecimentos\\Estabelecimentos9.csv removido.\n",
      "INFO:__main__:Arquivo descompactado com sucesso: ./output\\Estabelecimentos\\Estabelecimentos8.csv\n",
      "INFO:__main__:Arquivo zip ./temp\\Estabelecimentos8.zip removido após descompactação.\n",
      "INFO:__main__:Arquivo https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos8.zip foi descompactado com sucesso para ./output\n",
      "INFO:__main__:Arquivo descompactado com sucesso: ./output\\Estabelecimentos\\Estabelecimentos9.csv\n",
      "INFO:__main__:Arquivo zip ./temp\\Estabelecimentos9.zip removido após descompactação.\n",
      "INFO:__main__:Arquivo https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos9.zip foi descompactado com sucesso para ./output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos0.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos1.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos2.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos3.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos4.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos5.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos6.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos7.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos8.zip: Success\n",
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Estabelecimentos9.zip: Success\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Inicializa a classe sem parâmetros\n",
    "api = ReceitaCNPJApi()\n",
    "\n",
    "# Especifica o ano e o mês desejados\n",
    "year = 2024\n",
    "month = 9\n",
    "\n",
    "# Gera a lista de URLs apenas para o prefixo 'Empresas' no período especificado\n",
    "urls = api.lista_urls_receita(\n",
    "    'Estabelecimentos',\n",
    "    year=year,\n",
    "    month=month\n",
    ")\n",
    "\n",
    "# Função para processar cada URL\n",
    "def process_url(url):\n",
    "    return api.download_and_unzip(url)\n",
    "\n",
    "# Baixa e descompacta as URLs utilizando ThreadPoolExecutor para paralelizar\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    results = list(executor.map(process_url, urls))\n",
    "\n",
    "# Imprime os resultados\n",
    "for url, result in zip(urls, results):\n",
    "    print(f\"Result for {url}: {result[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78e85fe1-4b01-4667-b4f6-19ff7ec2d1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Gerando URLs para o período: 2024-09\n",
      "INFO:__main__:1 URLs geradas.\n",
      "INFO:__main__:Baixando https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Simples.zip (Tentativa 1/15)\n",
      "INFO:__main__:Download concluído: ./temp\\Simples.zip\n",
      "INFO:__main__:Descompactando ./temp\\Simples.zip para ./output\\Simples\n",
      "INFO:__main__:Arquivo existente ./output\\Simples\\Simples.csv removido.\n",
      "INFO:__main__:Arquivo descompactado com sucesso: ./output\\Simples\\Simples.csv\n",
      "INFO:__main__:Arquivo zip ./temp\\Simples.zip removido após descompactação.\n",
      "INFO:__main__:Arquivo https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Simples.zip foi descompactado com sucesso para ./output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for https://dadosabertos.rfb.gov.br/CNPJ/dados_abertos_cnpj/2024-09/Simples.zip: Success\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Inicializa a classe sem parâmetros\n",
    "api = ReceitaCNPJApi()\n",
    "\n",
    "# Especifica o ano e o mês desejados\n",
    "year = 2024\n",
    "month = 9\n",
    "\n",
    "# Gera a lista de URLs apenas para o prefixo 'Empresas' no período especificado\n",
    "urls = api.lista_urls_receita(\n",
    "    'Simples',\n",
    "    year=year,\n",
    "    month=month\n",
    ")\n",
    "\n",
    "# Função para processar cada URL\n",
    "def process_url(url):\n",
    "    return api.download_and_unzip(url)\n",
    "\n",
    "# Baixa e descompacta as URLs utilizando ThreadPoolExecutor para paralelizar\n",
    "with ThreadPoolExecutor(max_workers=1) as executor:\n",
    "    results = list(executor.map(process_url, urls))\n",
    "\n",
    "# Imprime os resultados\n",
    "for url, result in zip(urls, results):\n",
    "    print(f\"Result for {url}: {result[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80475928-6df4-452c-84c9-ff8168c462f0",
   "metadata": {},
   "source": [
    "## Paralelizando com Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f74f43-4e20-4150-9e83-b359bb308eb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "api = ReceitaCNPJApi()\n",
    "# Defina a função de download e descompactação\n",
    "def download_and_unzip_spark(url):\n",
    "    api = ReceitaCNPJApi()\n",
    "    return api.download_and_unzip(url)\n",
    "\n",
    "# Crie um RDD das URLs\n",
    "x = api.lista_urls_receita()\n",
    "urls = x[22:24]\n",
    "urls_rdd = spark.sparkContext.parallelize(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749f6875-0e88-4e3d-b81f-0549081666c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Faça o download e descompacte as URLs em paralelo usando o Spark\n",
    "results = urls_rdd.map(download_and_unzip_spark).collect()\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Result for {result[0]}: {result[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0a00b5-9c70-48e0-bc3d-9b19dc0ea4cb",
   "metadata": {},
   "source": [
    "# CLasse TL Receita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c5006b3-853a-4cbb-9c32-7334b16c0085",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ======================\n",
    "# Bibliotecas padrão para utilidades básicas e manipulação de arquivos:\n",
    "# ======================\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import zipfile\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ======================\n",
    "# Registros de logs:\n",
    "# ======================\n",
    "import logging\n",
    "\n",
    "# ======================\n",
    "# Requisições e manipulação de conteúdo web:\n",
    "# ======================\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ======================\n",
    "# Utilidades e manipulação de dados:\n",
    "# ======================\n",
    "from collections import Counter\n",
    "import chardet\n",
    "import string\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import shutil\n",
    "# import pycep_correios  # Descomente se necessário\n",
    "import brazilcep\n",
    "\n",
    "# ======================\n",
    "# Manipulação de datas:\n",
    "# ======================\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# ======================\n",
    "# Criptografia:\n",
    "# ======================\n",
    "import secrets\n",
    "import base64\n",
    "# from Crypto.Cipher import AES\n",
    "# from Crypto.Random import get_random_bytes\n",
    "\n",
    "# ======================\n",
    "# Multitarefas:\n",
    "# ======================\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# ======================\n",
    "# Spark:\n",
    "# ======================\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import (\n",
    "    regexp_replace,\n",
    "    when,\n",
    "    length,\n",
    "    to_date,\n",
    "    upper,\n",
    "    lower,\n",
    "    col,\n",
    "    udf,\n",
    "    split,\n",
    "    explode,\n",
    "    coalesce,\n",
    "    concat_ws,\n",
    "    concat,\n",
    "    lit,\n",
    "    broadcast,\n",
    "    regexp_extract,\n",
    "    expr\n",
    ")\n",
    "import pyspark.sql.functions as F\n",
    "from functools import reduce\n",
    "\n",
    "# ======================\n",
    "# Geolocalização:\n",
    "# ======================\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "# Configuração do Logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d51367a0-7222-42e4-8163-7312c98b93a3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import glob\n",
    "import chardet\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "import secrets\n",
    "from functools import reduce\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    udf, broadcast, col, upper, lower, when, regexp_extract, \n",
    "    regexp_replace, concat_ws, concat, lit, to_date, explode, \n",
    "    split, expr, length\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, FloatType\n",
    ")\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "# Configuração do Logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class ReceitaLT:\n",
    "    \"\"\"\n",
    "    A classe `ReceitaLT` facilita a manipulação e análise de dados da Receita Federal do Brasil.\n",
    "\n",
    "    Atributos:\n",
    "        spark (SparkSession): Sessão Spark para manipulação de dataframes.\n",
    "        logger (Logger): Logger para capturar e exibir logs.\n",
    "        \n",
    "    Atributos estáticos:\n",
    "        - estabelecimentos: Schema para dados de estabelecimentos.\n",
    "        - empresas: Schema para dados das empresas.\n",
    "        - municipios: Schema para municípios.\n",
    "        - cnaes: Schema para CNAEs.\n",
    "        - paises: Schema para países.\n",
    "        - qualificacoes: Schema para qualificações.\n",
    "        - socios: Schema para sócios.\n",
    "        - simples: Schema para opções do Simples Nacional.\n",
    "        - naturezas: Schema para naturezas jurídicas.\n",
    "        - motivos: Schema para motivos de situações cadastrais.\n",
    "        - dic_provedor: Dicionário para correção de nomes de provedores de email.\n",
    "        \n",
    "    Métodos:\n",
    "        detect_encoding(file_pattern_or_path, num_bytes=10000): Detecta a codificação do arquivo ou arquivos fornecidos.\n",
    "\n",
    "    Uso:\n",
    "        1. Instancie a classe com uma sessão Spark.\n",
    "        2. Utilize os schemas estáticos para leitura de arquivos.\n",
    "        3. Use o método `detect_encoding` para determinar a codificação de arquivos antes de lê-los.\n",
    "        \n",
    "    Exemplo:\n",
    "        from pyspark.sql import SparkSession\n",
    "        \n",
    "        spark_session = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "        receita_helper = ReceitaLT(spark_session)\n",
    "        encodings = receita_helper.detect_encoding(\"path/to/datafile.csv\")\n",
    "        df = spark_session.read.csv(\"path/to/datafile.csv\", schema=ReceitaLT.empresas, encoding=encodings[\"path/to/datafile.csv\"])\n",
    "    \"\"\"\n",
    "\n",
    "    # Definição dos Schemas\n",
    "    estabelecimentos = StructType([\n",
    "        StructField(\"CNPJ_BASICO\", StringType(), nullable=True),\n",
    "        StructField(\"CNPJ_ORDEM\", StringType(), nullable=True),\n",
    "        StructField(\"CNPJ_DV\", StringType(), nullable=True),\n",
    "        StructField(\"MATRIZ_FILIAL\", StringType(), nullable=True),\n",
    "        StructField(\"NOME_FANTASIA\", StringType(), nullable=True),\n",
    "        StructField(\"SIT_CADASTRAL\", IntegerType(), nullable=True),\n",
    "        StructField(\"DT_SIT_CADASTRAL\", StringType(), nullable=True),\n",
    "        StructField(\"MOTIVO_CADASTRAL\", StringType(), nullable=True),\n",
    "        StructField(\"NOME_CIDADE_EXTERIOR\", StringType(), nullable=True),\n",
    "        StructField(\"PAIS\", StringType(), nullable=True),\n",
    "        StructField(\"DT_INICIO_ATIVIDADE\", StringType(), nullable=True),\n",
    "        StructField(\"CNAE_1\", StringType(), nullable=True),\n",
    "        StructField(\"CNAE_2\", StringType(), nullable=True),\n",
    "        StructField(\"TIPO_LOUGRADOURO\", StringType(), nullable=True),\n",
    "        StructField(\"LOGRADOURO\", StringType(), nullable=True),\n",
    "        StructField(\"NUMERO\", IntegerType(), nullable=True),\n",
    "        StructField(\"COMPLEMENTO\", StringType(), nullable=True),\n",
    "        StructField(\"BAIRRO\", StringType(), nullable=True),\n",
    "        StructField(\"CEP\", IntegerType(), nullable=True),\n",
    "        StructField(\"UF\", StringType(), nullable=True),\n",
    "        StructField(\"MUNICIPIO\", StringType(), nullable=True),\n",
    "        StructField(\"DDD1\", StringType(), nullable=True),\n",
    "        StructField(\"TEL1\", StringType(), nullable=True),\n",
    "        StructField(\"DDD2\", StringType(), nullable=True),\n",
    "        StructField(\"TEL2\", StringType(), nullable=True),\n",
    "        StructField(\"DDD_FAX\", IntegerType(), nullable=True),\n",
    "        StructField(\"FAX\", IntegerType(), nullable=True),\n",
    "        StructField(\"EMAIL\", StringType(), nullable=True),\n",
    "        StructField(\"SIT_ESPECIAL\", StringType(), nullable=True),\n",
    "        StructField(\"DT_SIT_ESPECIAL\", StringType(), nullable=True)\n",
    "    ])\n",
    "\n",
    "    empresas = StructType([\n",
    "        StructField(\"CNPJ\", StringType(), nullable=True),\n",
    "        StructField(\"NOME_EMPRESA\", StringType(), nullable=True),\n",
    "        StructField(\"COD_NAT_JURICA\", StringType(), nullable=True),\n",
    "        StructField(\"QUALIF_RESPONVAVEL\", StringType(), nullable=True),\n",
    "        StructField(\"CAP_SOCIAL\", StringType(), nullable=True),\n",
    "        StructField(\"PORTE\", StringType(), nullable=True),\n",
    "        StructField(\"ENTE_FEDERATIVO\", StringType(), nullable=True)\n",
    "    ])\n",
    "\n",
    "    municipios = StructType([\n",
    "        StructField(\"ID_MUNICPIO\", StringType(), nullable=True),\n",
    "        StructField(\"MUNICIPIO\", StringType(), nullable=True)\n",
    "    ])\n",
    "\n",
    "    cnaes = StructType([\n",
    "        StructField(\"COD_CNAE\", StringType(), nullable=True),\n",
    "        StructField(\"CNAE\", StringType(), nullable=True)\n",
    "    ])\n",
    "    \n",
    "    paises = StructType([\n",
    "        StructField(\"COD_PAIS\", StringType(), nullable=True),\n",
    "        StructField(\"NM_PAIS\", StringType(), nullable=True)\n",
    "    ])\n",
    "    \n",
    "    qualificacoes = StructType([\n",
    "        StructField(\"COD_QUALIFICACAO\", StringType(), nullable=True),\n",
    "        StructField(\"NM_QUALIFICACAO\", StringType(), nullable=True)\n",
    "    ])\n",
    "\n",
    "    socios = StructType([\n",
    "        StructField(\"CNPJ_BASICO\", StringType(), nullable=True),\n",
    "        StructField(\"IDENTIFICADOR_SOCIO\", IntegerType(), nullable=True),\n",
    "        StructField(\"NOME_SOCIO_RAZAO_SOCIAL\", StringType(), nullable=True),\n",
    "        StructField(\"CNPJ_CPF_SOCIO\", StringType(), nullable=True),\n",
    "        StructField(\"QUALIFICAÇAO_SOCIO\", StringType(), nullable=True),\n",
    "        StructField(\"DATA_ENTRADA_SOCIEDADE\", StringType(), nullable=True),\n",
    "        StructField(\"PAIS\", StringType(), nullable=True),\n",
    "        StructField(\"REPRESENTANTE_LEGAL\", StringType(), nullable=True),\n",
    "        StructField(\"NOME_REPRESENTANTE\", StringType(), nullable=True),\n",
    "        StructField(\"QUALIFICACAO_REPRESENTANTE_LEGAL\", StringType(), nullable=True),\n",
    "        StructField(\"FAIXA_ETARIA\", StringType(), nullable=True)\n",
    "    ])\n",
    "\n",
    "    simples = StructType([\n",
    "        StructField(\"CNPJ_BASICO\", StringType(), nullable=True),\n",
    "        StructField(\"OPCAO_PELO_SIMPLES\", StringType(), nullable=True),\n",
    "        StructField(\"DATA_OPCAO_PELO_SIMPLES\", StringType(), nullable=True),\n",
    "        StructField(\"DATA_EXCLUSAO_SIMPLES\", StringType(), nullable=True),\n",
    "        StructField(\"OPÇAO_PELO_MEI\", StringType(), nullable=True),\n",
    "        StructField(\"DATA_OPCAO_PELO_MEI\", StringType(), nullable=True),\n",
    "        StructField(\"DATA_EXCLUSAO_MEI\", StringType(), nullable=True)\n",
    "    ])\n",
    "\n",
    "    naturezas = StructType([\n",
    "        StructField(\"COD_NAT_JURICA\", StringType(), nullable=True),\n",
    "        StructField(\"NAT_JURICA\", StringType(), nullable=True)\n",
    "    ])\n",
    "    \n",
    "    motivos = StructType([\n",
    "        StructField(\"COD_MOTIVO\", StringType(), nullable=True),\n",
    "        StructField(\"NM_MOTIVO\", StringType(), nullable=True)\n",
    "    ])\n",
    "\n",
    "    dic_provedor = {\n",
    "        # ... (seu dicionário completo aqui)\n",
    "        # Certifique-se de que o dicionário esteja correto e sem duplicatas\n",
    "    }\n",
    "\n",
    "    def __init__(self, spark: SparkSession):\n",
    "        \"\"\"\n",
    "        Inicializa a classe ReceitaLT.\n",
    "        \n",
    "        Parâmetros:\n",
    "        spark (SparkSession): Uma sessão Spark ativa.\n",
    "        \"\"\"\n",
    "        self.spark = spark\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        # A configuração do logger já foi feita no início do script\n",
    "\n",
    "    @staticmethod\n",
    "    def geocode_address(address):\n",
    "        \"\"\"\n",
    "        Geocodifica um endereço, convertendo-o em coordenadas de latitude e longitude.\n",
    "\n",
    "        Parâmetros:\n",
    "            address (str): Endereço a ser geocodificado.\n",
    "\n",
    "        Retorna:\n",
    "            tuple: Um par contendo a latitude e a longitude do endereço fornecido. \n",
    "                   Se o endereço não puder ser geocodificado, retorna (None, None).\n",
    "\n",
    "        Exemplo:\n",
    "            lat, lon = geocode_address(\"1600 Amphitheatre Parkway, Mountain View, CA\")\n",
    "\n",
    "        Notas:\n",
    "            - Usa o serviço Nominatim para a geocodificação.\n",
    "            - Incorpora um limitador de taxa para garantir que não excedamos os limites de requisições por segundo \n",
    "              do serviço.\n",
    "        \"\"\"\n",
    "        geolocator = Nominatim(user_agent=\"CNPJ_GEOLOCATION\")\n",
    "        geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n",
    "        location = geocode(address)\n",
    "        if location:\n",
    "            return (location.latitude, location.longitude)\n",
    "        else:\n",
    "            return (None, None)\n",
    "\n",
    "    @staticmethod\n",
    "    def geocode_udf(address):\n",
    "        \"\"\"\n",
    "        UDF do Spark para geocodificar um endereço dentro de um DataFrame.\n",
    "\n",
    "        Parâmetros:\n",
    "            address (str): Endereço a ser geocodificado.\n",
    "\n",
    "        Retorna:\n",
    "            dict: Dicionário contendo a 'latitude' e a 'longitude' do endereço fornecido.\n",
    "                  Se o endereço não puder ser geocodificado, os valores serão None.\n",
    "\n",
    "        Exemplo:\n",
    "            df.withColumn(\"location\", ReceitaLT.geocode_udf(df[\"address\"]))\n",
    "\n",
    "        Notas:\n",
    "            - Esta UDF encapsula a função `geocode_address`.\n",
    "            - Retorna um tipo de dado complexo (Struct) com dois campos: 'latitude' e 'longitude'.\n",
    "        \"\"\"\n",
    "        lat, lon = ReceitaLT.geocode_address(address)\n",
    "        return {\"latitude\": lat, \"longitude\": lon}\n",
    "\n",
    "    # Definição do Schema para a UDF\n",
    "    geocode_schema = StructType([\n",
    "        StructField(\"latitude\", FloatType(), nullable=True),\n",
    "        StructField(\"longitude\", FloatType(), nullable=True)\n",
    "    ])\n",
    "\n",
    "    # Criação do UDF com o schema definido\n",
    "    geocode_udf = udf(geocode_udf, geocode_schema)\n",
    "\n",
    "    @staticmethod\n",
    "    def detect_encoding(file_pattern_or_path, num_bytes=10000):\n",
    "        \"\"\"\n",
    "        Detecta a codificação do arquivo ou arquivos fornecidos.\n",
    "        \n",
    "        Parâmetros:\n",
    "            file_pattern_or_path (str): Caminho ou padrão do arquivo para detecção.\n",
    "            num_bytes (int, opcional): Número de bytes para ler para a detecção. Padrão é 10000.\n",
    "        \n",
    "        Retorna:\n",
    "            dict: Dicionário com caminho do arquivo como chave e codificação detectada como valor.\n",
    "        \"\"\"\n",
    "        files = glob.glob(file_pattern_or_path)\n",
    "        encodings = {}\n",
    "        for file_path in files:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                rawdata = f.read(num_bytes)\n",
    "                encodings[file_path] = chardet.detect(rawdata)[\"encoding\"]\n",
    "        return encodings\n",
    "\n",
    "    def read_data(self, schema_name, base_path=None):\n",
    "        \"\"\"\n",
    "        Lê dados de vários arquivos CSV de acordo com o esquema e caminho base fornecidos, consolidando-os \n",
    "        em um único DataFrame do Spark.\n",
    "\n",
    "        Parâmetros:\n",
    "            schema_name (str): Nome do esquema a ser usado para a leitura dos arquivos.\n",
    "                               Deve ser uma das chaves do dicionário `schemas`.\n",
    "\n",
    "            base_path (str, opcional): Caminho base dos arquivos CSV.\n",
    "                                       Se não for fornecido, ele tentará buscar da variável de ambiente 'BASE_PATH'.\n",
    "                                       Caso não encontre, o padrão \"./output\" será utilizado.\n",
    "\n",
    "        Retorna:\n",
    "            DataFrame: DataFrame do Spark contendo os dados consolidados dos arquivos CSV.\n",
    "\n",
    "        Exceções:\n",
    "            Pode lançar uma exceção se o arquivo não estiver presente no caminho especificado ou\n",
    "            se houver problemas de codificação ao ler o arquivo.\n",
    "\n",
    "        Exemplo:\n",
    "            receita_helper = ReceitaLT(spark_session)\n",
    "            df = receita_helper.read_data(\"estabelecimentos\", \"/path/to/csv/files\")\n",
    "\n",
    "        Notas:\n",
    "            - A função primeiro detecta a codificação dos arquivos antes de lê-los para garantir que \n",
    "              eles sejam lidos corretamente.\n",
    "            - A função lida com múltiplos arquivos CSV e os une em um único DataFrame.\n",
    "            - O formato de arquivo assumido é CSV com delimitador \";\", sem cabeçalho e com aspas para delimitar campos.\n",
    "        \"\"\"\n",
    "        schemas = {\n",
    "            \"estabelecimentos\": self.estabelecimentos,\n",
    "            \"empresas\": self.empresas,\n",
    "            \"municipios\": self.municipios,\n",
    "            \"cnaes\": self.cnaes,\n",
    "            \"socios\": self.socios,\n",
    "            \"simples\": self.simples,\n",
    "            \"naturezas\": self.naturezas,\n",
    "            \"qualificacoes\": self.qualificacoes,\n",
    "            \"motivos\": self.motivos,\n",
    "            \"paises\": self.paises\n",
    "        }\n",
    "\n",
    "        # Se o base_path não for fornecido, pegar da variável de ambiente ou usar um padrão.\n",
    "        if not base_path:\n",
    "            base_path = os.environ.get('BASE_PATH', \"./output\")\n",
    "\n",
    "        if schema_name not in schemas:\n",
    "            raise ValueError(f\"Schema '{schema_name}' não está definido.\")\n",
    "\n",
    "        # Definir o padrão do caminho dos arquivos\n",
    "        if schema_name in ['estabelecimentos', 'empresas', 'socios']:\n",
    "            file_location_pattern = os.path.join(base_path, schema_name.capitalize(), '*.csv')\n",
    "        else:\n",
    "            file_location_pattern = os.path.join(base_path, schema_name.capitalize(), f\"{schema_name.capitalize()}.csv\")\n",
    "\n",
    "        # Detectar codificações\n",
    "        encodings = self.detect_encoding(file_location_pattern)\n",
    "        self.logger.info(f\"Detected encodings: {encodings}\")\n",
    "\n",
    "        # Agora, vamos ler cada arquivo com sua codificação correta e armazenar em uma lista de DataFrames\n",
    "        dfs = []\n",
    "        for file_location, encoding in encodings.items():\n",
    "            try:\n",
    "                df = (self.spark.read.format(\"csv\")\n",
    "                      .option(\"sep\", \";\")\n",
    "                      .option(\"header\", \"false\")\n",
    "                      .option('quote', '\"')\n",
    "                      .option(\"escape\", '\"')\n",
    "                      .option(\"encoding\", encoding)\n",
    "                      .schema(schemas[schema_name])\n",
    "                      .load(file_location))\n",
    "                dfs.append(df)\n",
    "                self.logger.info(f\"Arquivo lido com sucesso: {file_location}\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Erro ao ler o arquivo {file_location}: {e}\")\n",
    "                raise e\n",
    "\n",
    "        # Unir todos os DataFrames em um único DataFrame\n",
    "        if dfs:\n",
    "            final_df = reduce(lambda a, b: a.union(b), dfs)\n",
    "        else:\n",
    "            final_df = self.spark.createDataFrame([], schemas[schema_name])\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def save_data(self, df, path, num_partitions=1, file_format=\"parquet\"):\n",
    "        \"\"\"\n",
    "        Salva o DataFrame no caminho especificado.\n",
    "        \n",
    "        Parâmetros:\n",
    "            df (DataFrame): DataFrame a ser salvo.\n",
    "            path (str): Caminho de destino.\n",
    "            num_partitions (int, opcional): Número de partições para salvar os dados (padrão é 1).\n",
    "            file_format (str, opcional): Formato do arquivo para salvar os dados (padrão é \"parquet\").\n",
    "        \"\"\"\n",
    "        # Reparticionar o DataFrame conforme a entrada do usuário\n",
    "        df = df.repartition(num_partitions)\n",
    "        \n",
    "        # Salvar o DataFrame no caminho e formato especificados\n",
    "        df.write.mode('overwrite').format(file_format).save(path)\n",
    "        self.logger.info(f\"DataFrame salvo com sucesso em {path} no formato {file_format}.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def download_nomes(save_base_path=\"./output/nomes\"):\n",
    "        \"\"\"\n",
    "        Baixa e extrai o arquivo nomes.csv.gz do dataset genero-nomes no Brasil.io.\n",
    "\n",
    "        Parâmetros:\n",
    "            save_base_path (str, opcional): Caminho base onde o arquivo será salvo. O padrão é './output/nomes'.\n",
    "\n",
    "        Descrição:\n",
    "            - Cria o diretório de salvamento se ele não existir.\n",
    "            - Baixa o arquivo nomes.csv.gz da URL especificada.\n",
    "            - Extrai o conteúdo do arquivo .gz.\n",
    "            - Remove o arquivo .gz original, mantendo apenas o arquivo CSV extraído.\n",
    "\n",
    "        Notas:\n",
    "            - Esta função usa a biblioteca `requests` para baixar o arquivo.\n",
    "            - A função verifica se a resposta do servidor é 200 (sucesso) antes de baixar o arquivo.\n",
    "            - O arquivo .gz é extraído usando a biblioteca `gzip`.\n",
    "        \"\"\"\n",
    "        import gzip  # Certifique-se de importar gzip\n",
    "        os.makedirs(save_base_path, exist_ok=True)\n",
    "        url = \"https://data.brasil.io/dataset/genero-nomes/nomes.csv.gz\"\n",
    "        file_name = os.path.basename(url)\n",
    "        file_path = os.path.join(save_base_path, file_name)\n",
    "        extracted_file_path = os.path.join(save_base_path, file_name[:-3])  # remove .gz\n",
    "\n",
    "        # Baixe o arquivo\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(file_path, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=128):\n",
    "                    file.write(chunk)\n",
    "            logger.info(f\"Arquivo baixado com sucesso: {file_path}\")\n",
    "        else:\n",
    "            logger.error(f\"Falha ao baixar {url}. Status code: {response.status_code}\")\n",
    "            return\n",
    "\n",
    "        # Extraia o arquivo\n",
    "        try:\n",
    "            with gzip.open(file_path, 'rb') as f_in:\n",
    "                with open(extracted_file_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "            logger.info(f\"Arquivo extraído com sucesso: {extracted_file_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erro ao extrair o arquivo {file_path}: {e}\")\n",
    "            raise e\n",
    "\n",
    "        # Apague o arquivo .gz\n",
    "        os.remove(file_path)\n",
    "        logger.info(f\"Arquivo .gz removido: {file_path}\")\n",
    "\n",
    "    def process_estabelecimentos(self, df):\n",
    "        \"\"\"\n",
    "        Processa e enriquece o DataFrame de estabelecimentos com informações adicionais e transformações.\n",
    "\n",
    "        Parâmetros:\n",
    "            df (DataFrame): DataFrame inicial contendo informações de estabelecimentos.\n",
    "\n",
    "        Retorna:\n",
    "            DataFrame: DataFrame processado e enriquecido com novas colunas e informações.\n",
    "\n",
    "        Descrição:\n",
    "            - Lê dataframes adicionais relacionados a países, municípios, cnaes e motivos.\n",
    "            - Realiza renomeações de colunas para facilitar junções.\n",
    "            - Enriquece o dataframe com informações de motivos, cnaes, municípios e países.\n",
    "            - Processa colunas de e-mail, separando provedores e corrigindo valores.\n",
    "            - Converte colunas de data de string para formato de data.\n",
    "            - Deriva colunas de ano e mês a partir de datas.\n",
    "            - Processa e deriva novas colunas com base em mapeamentos para situação cadastral e tipo de estabelecimento.\n",
    "            - Valida endereços de e-mail usando expressões regulares.\n",
    "            - Combina informações de endereço para formar uma coluna completa de endereço.\n",
    "            - Utiliza a função de geocodificação para obter coordenadas com base no endereço e, em caso de falha, com base no CEP.\n",
    "            - Realiza correções na coluna de provedor de e-mail usando um dicionário de mapeamento.\n",
    "\n",
    "        Notas:\n",
    "            - Esta função faz uso intensivo das operações de DataFrame do PySpark.\n",
    "            - Dependências: A função depende de outras funções e UDFs, como 'geocode_udf', bem como de variáveis de instância, como 'dic_provedor'.\n",
    "        \"\"\"\n",
    "        from pyspark.sql.functions import broadcast  # Certifique-se de importar broadcast\n",
    "\n",
    "        # Leitura dos DataFrames auxiliares\n",
    "        df_pais = self.read_data(schema_name='paises')\n",
    "        df_mun = self.read_data(schema_name='municipios')\n",
    "        df_cnaes = self.read_data(schema_name='cnaes')\n",
    "        df_motivos = self.read_data(schema_name='motivos')\n",
    "        \n",
    "        # Renomear colunas para facilitar as junções\n",
    "        df = df.withColumnRenamed(\"CNAE_1\", \"COD_CNAE\") \\\n",
    "               .withColumnRenamed(\"MUNICIPIO\", \"ID_MUNICPIO\") \\\n",
    "               .withColumnRenamed(\"PAIS\", \"COD_PAIS\") \\\n",
    "               .withColumnRenamed(\"MOTIVO_CADASTRAL\", \"COD_MOTIVO\") \n",
    "        \n",
    "        # Realizar junções com os DataFrames auxiliares\n",
    "        df = df.join(broadcast(df_motivos), \"COD_MOTIVO\", \"left\").drop(\"COD_MOTIVO\")\n",
    "        df = df.join(broadcast(df_cnaes), \"COD_CNAE\", \"left\").drop(\"COD_CNAE\")\n",
    "        df = df.join(broadcast(df_mun), \"ID_MUNICPIO\", \"left\").drop(\"ID_MUNICPIO\")\n",
    "        df = df.join(broadcast(df_pais), \"COD_PAIS\", \"left\").drop(\"COD_PAIS\")\n",
    "        \n",
    "        # Tratamento da coluna PROVEDOR\n",
    "        df = df.withColumn(\"PROVEDOR\", regexp_extract(\"EMAIL\", \"(?<=@)[^.]+(?=\\\\.)\", 0))\n",
    "        df = df.withColumn(\"PROVEDOR\", upper(col(\"PROVEDOR\")))\n",
    "        df = df.withColumn(\"EMAIL\", lower(col(\"EMAIL\")))\n",
    "\n",
    "        # Convertendo colunas de data\n",
    "        df = df.withColumn(\"DT_SIT_CADASTRAL\", to_date(col('DT_SIT_CADASTRAL'), \"yyyyMMdd\")) \\\n",
    "               .withColumn(\"DT_INICIO_ATIVIDADE\", to_date(col('DT_INICIO_ATIVIDADE'), \"yyyyMMdd\")) \\\n",
    "               .withColumn(\"DT_SIT_ESPECIAL\", to_date(col('DT_SIT_ESPECIAL'), \"yyyyMMdd\"))\n",
    "        \n",
    "        # Derivando colunas de ano e mês\n",
    "        df = df.withColumn(\"ano_cadastro\", F.year('DT_INICIO_ATIVIDADE')) \\\n",
    "               .withColumn(\"mes_cadastro\", F.month('DT_INICIO_ATIVIDADE')) \\\n",
    "               .withColumn(\"ano_sit_cadastral\", F.year('DT_SIT_CADASTRAL')) \\\n",
    "               .withColumn(\"mes_sit_cadastral\", F.month('DT_SIT_CADASTRAL'))\n",
    "        \n",
    "        # Mapeamento para Situação Cadastral\n",
    "        mapping_sit_cadastral = {1: 'NULA', 2: 'ATIVA', 3: 'SUSPENSA', 4: 'INAPTA', 8: 'BAIXADA'}\n",
    "        \n",
    "        # Criando a coluna NM_SIT_CADASTRAL\n",
    "        df = df.withColumn(\"NM_SIT_CADASTRAL\", \n",
    "                           when(col(\"SIT_CADASTRAL\").isin(list(mapping_sit_cadastral.keys())), col(\"SIT_CADASTRAL\")).otherwise(None))\n",
    "        \n",
    "        for key, value in mapping_sit_cadastral.items():\n",
    "            df = df.withColumn(\"NM_SIT_CADASTRAL\", \n",
    "                               when(col(\"SIT_CADASTRAL\") == key, value).otherwise(col(\"NM_SIT_CADASTRAL\")))\n",
    "        \n",
    "        # Validação de e-mails\n",
    "        email_pattern = r'^\\S+@\\S+\\.\\S+$'  # Padrão simples de endereço de e-mail\n",
    "        df = df.withColumn(\"valid_email\", regexp_extract(col(\"EMAIL\"), email_pattern, 0))\n",
    "        \n",
    "        # Mapeamento para Tipo de Estabelecimento\n",
    "        mapping_tipo = {1: 'MATRIZ', 2: 'FILIAL'}\n",
    "        \n",
    "        df = df.withColumn(\"NM_MATRIZ_FILIAL\", \n",
    "                           when(col(\"MATRIZ_FILIAL\").isin(list(mapping_tipo.keys())), col(\"MATRIZ_FILIAL\")).otherwise(None))\n",
    "        \n",
    "        for key, value in mapping_tipo.items():\n",
    "            df = df.withColumn(\"NM_MATRIZ_FILIAL\", \n",
    "                               when(col(\"MATRIZ_FILIAL\") == key, value).otherwise(col(\"NM_MATRIZ_FILIAL\")))\n",
    "        \n",
    "        # Criando a coluna ENDERECO_COMPLETO\n",
    "        df = df.withColumn(\"ENDERECO_COMPLETO\",\n",
    "                           concat_ws(\", \",\n",
    "                                     concat(col(\"TIPO_LOUGRADOURO\"), lit(\" \"), col(\"LOGRADOURO\")),\n",
    "                                     col(\"NUMERO\"),\n",
    "                                     concat_ws(\" - \", col(\"MUNICIPIO\"), col(\"UF\"))))\n",
    "        \n",
    "        # Adicionando a lógica de geocodificação\n",
    "        df = df.withColumn(\"COORDENADAS\", ReceitaLT.geocode_udf(col(\"ENDERECO_COMPLETO\")))\n",
    "        \n",
    "        # Caso a geocodificação falhe, tentar com o CEP\n",
    "        df = df.withColumn(\"COORDENADAS\",\n",
    "                           when((col(\"COORDENADAS.latitude\").isNull()) & (col(\"COORDENADAS.longitude\").isNull()),\n",
    "                                ReceitaLT.geocode_udf(col(\"CEP\"))).otherwise(col(\"COORDENADAS\")))\n",
    "        \n",
    "        # Correção da coluna PROVEDOR\n",
    "        df = df.replace(self.dic_provedor, subset=['PROVEDOR'])\n",
    "        \n",
    "        # Transformação das keys e values do dicionário em lowercase\n",
    "        dic_prov_lower = {k.lower(): str(v).lower() for k, v in self.dic_provedor.items()}\n",
    "        \n",
    "        # Correção dos provedores na coluna EMAIL\n",
    "        replace_expr = reduce(\n",
    "            lambda a, b: regexp_replace(a, rf\"\\b{b[0]}\\b\", b[1]),\n",
    "            dic_prov_lower.items(),\n",
    "            col(\"valid_email\")\n",
    "        )\n",
    "        \n",
    "        df = df.withColumn(\"valid_email\", replace_expr) \\\n",
    "               .withColumnRenamed(\"valid_email\", \"VALID_EMAIL\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def process_empresas(self, df):\n",
    "        \"\"\"\n",
    "        Processa e enriquece o DataFrame de empresas com informações adicionais e transformações.\n",
    "\n",
    "        Parâmetros:\n",
    "            df (DataFrame): DataFrame inicial contendo informações de empresas.\n",
    "\n",
    "        Retorna:\n",
    "            DataFrame: DataFrame processado e enriquecido com novas colunas e informações.\n",
    "\n",
    "        Descrição:\n",
    "            - Lê dataframes adicionais relacionados a naturezas jurídicas e qualificações.\n",
    "            - Realiza renomeação de colunas para facilitar junções.\n",
    "            - Enriquece o dataframe com informações de naturezas jurídicas e qualificações.\n",
    "            - Processa a coluna 'NOME_EMPRESA' para extrair informações potenciais de CPF.\n",
    "            - Deriva uma nova coluna baseada no porte da empresa, usando um mapeamento predefinido.\n",
    "            - Determina a probabilidade de um valor ser um CPF válido com base em seu comprimento.\n",
    "            - Criptografa possíveis valores de CPF usando AES e os armazena em uma nova coluna 'CPF_CRIPTOGRAFADO', enquanto remove a coluna original 'CPF'.\n",
    "\n",
    "        Notas:\n",
    "            - Esta função faz uso intensivo das operações de DataFrame do PySpark.\n",
    "            - O valor de criptografia (secret_key) é gerado dinamicamente a cada chamada da função. Portanto, cada execução resultará em valores de 'CPF_CRIPTOGRAFADO' diferentes para os mesmos CPFs.\n",
    "            - O método AES usado aqui é 'ECB', que não é considerado seguro para muitos casos de uso devido à falta de vetor de inicialização (IV). A utilização deste modo deve ser revista se a segurança for uma preocupação.\n",
    "        \"\"\"\n",
    "        from pyspark.sql.functions import expr\n",
    "\n",
    "        # Leitura dos DataFrames auxiliares\n",
    "        df_nat = self.read_data(schema_name='naturezas')\n",
    "        df_qual = self.read_data(schema_name='qualificacoes')\n",
    "        \n",
    "        # Renomear coluna para facilitar a junção\n",
    "        df = df.withColumnRenamed(\"QUALIF_RESPONVAVEL\", \"COD_QUALIFICACAO\")\n",
    "        \n",
    "        # Realizar junções com os DataFrames auxiliares\n",
    "        df = df.join(broadcast(df_nat), \"COD_NAT_JURICA\", \"left\").drop(\"COD_NAT_JURICA\")\n",
    "        df = df.join(broadcast(df_qual), \"COD_QUALIFICACAO\", \"left\").drop(\"COD_QUALIFICACAO\")\n",
    "               \n",
    "        # Extração e tratamento da coluna CPF\n",
    "        df = df.withColumn(\"CPF\", regexp_replace(col(\"NOME_EMPRESA\"), \"[^0-9]\", \"\"))\n",
    "        df = df.withColumn(\"CPF\", when(col(\"CPF\") == \"\", None).otherwise(col(\"CPF\")))\n",
    "        df = df.withColumn('CPF_LEN', length('CPF'))\n",
    "        \n",
    "        # Mapeamento para Porte da Empresa\n",
    "        mapping_porte = {\n",
    "            0: 'NÃO INFORMADO',\n",
    "            1: 'MICRO EMPRESA',\n",
    "            3: 'EMPRESA DE PEQUENO PORTE',\n",
    "            5: 'DEMAIS',\n",
    "            8: 'BAIXADA'\n",
    "        }\n",
    "\n",
    "        # Criando a coluna NM_PORTE\n",
    "        df = df.withColumn(\"NM_PORTE\", \n",
    "                           when(col(\"PORTE\").isin(list(mapping_porte.keys())), col(\"PORTE\")).otherwise(None))\n",
    "        \n",
    "        for key, value in mapping_porte.items():\n",
    "            df = df.withColumn(\"NM_PORTE\", \n",
    "                               when(col(\"PORTE\") == key, value).otherwise(col(\"NM_PORTE\")))\n",
    "        \n",
    "        # Determinando a probabilidade de ser um CPF válido\n",
    "        df = df.withColumn(\"PROBABILIDADE_DE_SER_CPF\", \n",
    "                           when(col(\"CPF_LEN\") == 11, \"SIM\").otherwise(\"NAO\"))\n",
    "        \n",
    "        # Gerando uma chave secreta para criptografia\n",
    "        secret_key = secrets.token_urlsafe(24)\n",
    "        df = df.withColumn(\"CPF_CRIPTOGRAFADO\", \n",
    "                           expr(f\"base64(aes_encrypt(CPF, '{secret_key}', 'ECB', 'PKCS'))\")) \\\n",
    "               .drop(\"CPF\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def process_simples(self, df):\n",
    "        \"\"\"\n",
    "        Processa o DataFrame relacionado ao regime tributário SIMPLES das empresas.\n",
    "\n",
    "        Parâmetros:\n",
    "            df (pyspark.sql.DataFrame): DataFrame inicial contendo informações relacionadas ao regime tributário SIMPLES.\n",
    "\n",
    "        Retorna:\n",
    "            pyspark.sql.DataFrame: DataFrame processado com colunas de data convertidas e apenas as colunas relevantes selecionadas.\n",
    "\n",
    "        Descrição:\n",
    "            - Converte colunas que representam datas do formato \"yyyyMMdd\" para o tipo data.\n",
    "            - Seleciona apenas as colunas relevantes para o contexto, que são: 'CNPJ_BASICO', 'OPÇAO_PELO_MEI', 'DT_OPCAO_MEI', 'DT_EXCLUSAO_MEI', 'OPCAO_PELO_SIMPLES', 'DT_OPCAO_SIMPLES', e 'DT_EXCLUSAO_SIMPLES'.\n",
    "\n",
    "        Notas:\n",
    "            - Esta função assume que as colunas de data estão no formato \"yyyyMMdd\" e realiza a conversão para o tipo data.\n",
    "            - As colunas de datas que são processadas incluem: DATA_OPCAO_PELO_SIMPLES, DATA_EXCLUSAO_SIMPLES, DATA_EXCLUSAO_MEI e DATA_OPCAO_PELO_MEI.\n",
    "        \"\"\"\n",
    "        from pyspark.sql.functions import F\n",
    "\n",
    "        df = df.withColumn(\"DT_OPCAO_SIMPLES\", F.to_date(col('DATA_OPCAO_PELO_SIMPLES'), \"yyyyMMdd\")) \\\n",
    "               .withColumn(\"DT_EXCLUSAO_SIMPLES\", to_date(col('DATA_EXCLUSAO_SIMPLES'), \"yyyyMMdd\")) \\\n",
    "               .withColumn(\"DT_EXCLUSAO_MEI\", to_date(col('DATA_EXCLUSAO_MEI'), \"yyyyMMdd\")) \\\n",
    "               .withColumn(\"DT_OPCAO_MEI\", to_date(col('DATA_OPCAO_PELO_MEI'), \"yyyyMMdd\")) \\\n",
    "               .select('CNPJ_BASICO','OPÇAO_PELO_MEI','DT_OPCAO_MEI','DT_EXCLUSAO_MEI',\n",
    "                       'OPCAO_PELO_SIMPLES','DT_OPCAO_SIMPLES','DT_EXCLUSAO_SIMPLES')\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def process_mei(self, df, save_base_path=\"./output/nomes\", file_name=\"nomes.csv\"):\n",
    "        \"\"\"\n",
    "        Processa um DataFrame referente a MEIs, realiza joins com dados adicionais de naturezas jurídicas,\n",
    "        qualificações, e um conjunto de dados de nomes para extração e categorização de primeiro nome.\n",
    "\n",
    "        Parâmetros:\n",
    "            df (pyspark.sql.DataFrame): DataFrame inicial contendo dados sobre MEIs.\n",
    "            save_base_path (str, opcional): Caminho onde o arquivo com dados de nomes foi extraído. Padrão é './output/nomes'.\n",
    "            file_name (str, opcional): Nome do arquivo CSV contendo dados de nomes a ser lido. Padrão é 'nomes.csv'.\n",
    "\n",
    "        Retorna:\n",
    "            pyspark.sql.DataFrame: DataFrame processado após todas as transformações e joins.\n",
    "\n",
    "        Descrição:\n",
    "            1. Realiza join com DataFrames de 'naturezas' e 'qualificações'.\n",
    "            2. Extração e manipulação de dados de CPF.\n",
    "            3. Utiliza um dicionário para mapear e criar a coluna \"NM_PORTE\".\n",
    "            4. Criptografa a coluna de CPF.\n",
    "            5. Realiza filtragens baseado na probabilidade do nome ser um CPF válido.\n",
    "            6. Lê um conjunto de dados de nomes e realiza o explode na coluna 'alternative_names'.\n",
    "            7. Extrai o primeiro nome da coluna 'NOME_EMPRESA'.\n",
    "            8. Realiza o join com o conjunto de dados de nomes para categorizar o primeiro nome.\n",
    "            9. Retorna um DataFrame contendo informações relevantes após todas as transformações.\n",
    "        \"\"\"\n",
    "        from pyspark.sql.functions import explode, split\n",
    "\n",
    "        # Leitura dos DataFrames auxiliares\n",
    "        df_nat = self.read_data(schema_name='naturezas')\n",
    "        df_qual = self.read_data(schema_name='qualificacoes')\n",
    "        \n",
    "        # Renomear coluna para facilitar a junção\n",
    "        df = df.withColumnRenamed(\"QUALIF_RESPONVAVEL\", \"COD_QUALIFICACAO\")\n",
    "        \n",
    "        # Realizar junções com os DataFrames auxiliares\n",
    "        df = df.join(broadcast(df_nat), \"COD_NAT_JURICA\", \"left\").drop(\"COD_NAT_JURICA\")\n",
    "        df = df.join(broadcast(df_qual), \"COD_QUALIFICACAO\", \"left\").drop(\"COD_QUALIFICACAO\")\n",
    "               \n",
    "        # Extração e tratamento da coluna CPF\n",
    "        df = df.withColumn(\"CPF\", regexp_replace(col(\"NOME_EMPRESA\"), \"[^0-9]\", \"\"))\n",
    "        df = df.withColumn(\"CPF\", when(col(\"CPF\") == \"\", None).otherwise(col(\"CPF\")))\n",
    "        df = df.withColumn('CPF_LEN', length('CPF'))\n",
    "        \n",
    "        # Mapeamento para Porte da Empresa\n",
    "        mapping_porte = {\n",
    "            0: 'NÃO INFORMADO',\n",
    "            1: 'MICRO EMPRESA',\n",
    "            3: 'EMPRESA DE PEQUENO PORTE',\n",
    "            5: 'DEMAIS',\n",
    "            8: 'BAIXADA'\n",
    "        }\n",
    "\n",
    "        # Criando a coluna NM_PORTE\n",
    "        df = df.withColumn(\"NM_PORTE\", \n",
    "                           when(col(\"PORTE\").isin(list(mapping_porte.keys())), col(\"PORTE\")).otherwise(None))\n",
    "        \n",
    "        for key, value in mapping_porte.items():\n",
    "            df = df.withColumn(\"NM_PORTE\", \n",
    "                               when(col(\"PORTE\") == key, value).otherwise(col(\"NM_PORTE\")))\n",
    "        \n",
    "        # Determinando a probabilidade de ser um CPF válido\n",
    "        df = df.withColumn(\"PROBABILIDADE_DE_SER_CPF\", \n",
    "                           when(col(\"CPF_LEN\") == 11, \"SIM\").otherwise(\"NAO\"))\n",
    "        \n",
    "        # Gerando uma chave secreta para criptografia\n",
    "        secret_key = secrets.token_urlsafe(24)\n",
    "        df = df.withColumn(\"CPF_CRIPTOGRAFADO\", \n",
    "                           expr(f\"base64(aes_encrypt(CPF, '{secret_key}', 'ECB', 'PKCS'))\")) \\\n",
    "               .drop(\"CPF\")\n",
    "        \n",
    "        # Filtrar df_processado baseado na coluna PROBABILIDADE_DE_SER_CPF\n",
    "        df_filter = df.filter(col('PROBABILIDADE_DE_SER_CPF') == 'SIM') \\\n",
    "                      .dropDuplicates(subset=['CPF_CRIPTOGRAFADO', 'NOME_EMPRESA'])\n",
    "        \n",
    "        # Caminho completo do arquivo de nomes\n",
    "        file_path = os.path.join(save_base_path, file_name)\n",
    "        \n",
    "        # Ler o arquivo CSV de nomes\n",
    "        df_csv = self.spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "        \n",
    "        # Explodir a coluna alternative_names para múltiplas linhas\n",
    "        df_csv = df_csv.withColumn(\"alternative_names\", explode(split(coalesce(col(\"alternative_names\"), col(\"first_name\")), \"\\\\|\")))\n",
    "        \n",
    "        # Selecionar as colunas desejadas\n",
    "        df_result = df_csv.select(\"alternative_names\", \"group_name\", \"ratio\", \"classification\") \\\n",
    "                          .dropDuplicates(subset=['alternative_names'])\n",
    "        \n",
    "        # Extrair o primeiro nome da coluna NOME_EMPRESA\n",
    "        df_filter = df_filter.withColumn(\"PRIMEIRO_NOME\", split(col(\"NOME_EMPRESA\"), \" \")[0])\n",
    "        \n",
    "        # Fazer o join entre df_filter e df_result\n",
    "        joined_df = df_filter.join(df_result, df_filter.PRIMEIRO_NOME == df_result.alternative_names, \"left\").dropDuplicates()\n",
    "        \n",
    "        # Selecionar as colunas finais\n",
    "        joined_df = joined_df.select(\n",
    "            'CNPJ', 'NOME_EMPRESA', 'CAP_SOCIAL', 'NM_PORTE', 'NAT_JURICA', \n",
    "            'ENTE_FEDERATIVO', 'NM_QUALIFICACAO', 'CPF_CRIPTOGRAFADO', 'CPF_LEN',\n",
    "            'PROBABILIDADE_DE_SER_CPF', 'PRIMEIRO_NOME',\n",
    "            col('group_name').alias('GRUPO_NOME'), \n",
    "            col('ratio').alias('PROBABILIDADE_CLASSIFICACAO'), \n",
    "            col('classification').alias('CLASSIFICACAO')\n",
    "        ).dropDuplicates()\n",
    "        \n",
    "        return joined_df\n",
    "\n",
    "    def process_socios(self, df, save_base_path=\"./output/nomes\", file_name=\"nomes.csv\"):\n",
    "        \"\"\"\n",
    "        Processa um DataFrame referente a sócios, realiza joins com dados adicionais de países, qualificações, \n",
    "        e um conjunto de dados de nomes para extração e categorização de primeiro nome.\n",
    "\n",
    "        Parâmetros:\n",
    "            df (pyspark.sql.DataFrame): DataFrame inicial contendo dados sobre sócios.\n",
    "            save_base_path (str, opcional): Caminho onde o arquivo com dados de nomes foi extraído. Padrão é './output/nomes'.\n",
    "            file_name (str, opcional): Nome do arquivo CSV contendo dados de nomes a ser lido. Padrão é 'nomes.csv'.\n",
    "\n",
    "        Retorna:\n",
    "            pyspark.sql.DataFrame: DataFrame processado após todas as transformações e joins.\n",
    "\n",
    "        Descrição:\n",
    "            1. Realiza join com DataFrames de 'países'.\n",
    "            2. Usa mapeamentos para criar colunas \"NM_FAIXA_ETARIA\" e \"NM_IDENTIFICADOR_SOCIO\".\n",
    "            3. Renomeia e realiza join com DataFrame de qualificações para obter descrições das qualificações.\n",
    "            4. Converte coluna de data \"DATA_ENTRADA_SOCIEDADE\" para o formato desejado.\n",
    "            5. Lê um conjunto de dados de nomes e realiza o explode na coluna 'alternative_names'.\n",
    "            6. Extrai o primeiro nome da coluna 'NOME_SOCIO_RAZAO_SOCIAL'.\n",
    "            7. Realiza o join entre o DataFrame processado e o conjunto de dados de nomes para categorizar o primeiro nome.\n",
    "            8. Retorna um DataFrame contendo informações relevantes após todas as transformações.\n",
    "        \"\"\"\n",
    "        from pyspark.sql.functions import explode, split\n",
    "\n",
    "        # Leitura dos DataFrames auxiliares\n",
    "        df_pais = self.read_data(schema_name='paises')\n",
    "        df = df.withColumnRenamed(\"PAIS\", \"COD_PAIS\")\n",
    "        df = df.join(broadcast(df_pais), \"COD_PAIS\", \"left\").drop(\"COD_PAIS\")\n",
    "        \n",
    "        # Mapeamento de códigos para faixas etárias.\n",
    "        mapping_faixa_etaria = {\n",
    "            1: '0 a 12 anos',\n",
    "            2: '13 a 20 anos',\n",
    "            3: '21 a 30 anos',\n",
    "            4: '31 a 40 anos',\n",
    "            5: '41 a 50 anos',\n",
    "            6: '51 a 60 anos',\n",
    "            7: '61 a 70 anos',\n",
    "            8: '71 a 80 anos',\n",
    "            9: 'maiores de 80 anos',\n",
    "            0: 'NA'\n",
    "        }\n",
    "        \n",
    "        # Mapeamento de códigos para identificador de sócio.\n",
    "        mapping_id_socio = {\n",
    "            1: 'PESSOA JURIDICA',\n",
    "            2: 'PESSOA FISICA',\n",
    "            3: 'ESTRANGEIRO'\n",
    "        }\n",
    "        \n",
    "        # Leitura do DataFrame de qualificações\n",
    "        df_qual = self.read_data(schema_name='qualificacoes')\n",
    "        \n",
    "        # Criando a coluna NM_FAIXA_ETARIA\n",
    "        df = df.withColumn(\"NM_FAIXA_ETARIA\", \n",
    "                           when(col(\"FAIXA_ETARIA\").isin(list(mapping_faixa_etaria.keys())), col(\"FAIXA_ETARIA\")).otherwise(None))\n",
    "        \n",
    "        for key, value in mapping_faixa_etaria.items():\n",
    "            df = df.withColumn(\"NM_FAIXA_ETARIA\", \n",
    "                               when(col(\"FAIXA_ETARIA\") == key, value).otherwise(col(\"NM_FAIXA_ETARIA\")))\n",
    "        \n",
    "        # Criando a coluna NM_IDENTIFICADOR_SOCIO\n",
    "        df = df.withColumn(\"NM_IDENTIFICADOR_SOCIO\", \n",
    "                           when(col(\"IDENTIFICADOR_SOCIO\").isin(list(mapping_id_socio.keys())), col(\"IDENTIFICADOR_SOCIO\")).otherwise(None))\n",
    "        \n",
    "        for key, value in mapping_id_socio.items():\n",
    "            df = df.withColumn(\"NM_IDENTIFICADOR_SOCIO\", \n",
    "                               when(col(\"IDENTIFICADOR_SOCIO\") == key, value).otherwise(col(\"NM_IDENTIFICADOR_SOCIO\")))\n",
    "        \n",
    "        # Renomear e juntar com qualificações para obter descrições\n",
    "        df = df.withColumnRenamed(\"QUALIFICACAO_REPRESENTANTE_LEGAL\", \"COD_QUALIFICACAO\")\n",
    "        df = df.join(broadcast(df_qual), \"COD_QUALIFICACAO\", \"left\").drop(\"COD_QUALIFICACAO\") \\\n",
    "               .withColumnRenamed(\"NM_QUALIFICACAO\", \"NM_QUALIFICACAO_REPRESENTANTE_LEGAL\")\n",
    "        \n",
    "        df = df.withColumnRenamed(\"QUALIFICAÇAO_SOCIO\", \"COD_QUALIFICACAO\")\n",
    "        df = df.join(broadcast(df_qual), \"COD_QUALIFICACAO\", \"left\").drop(\"COD_QUALIFICACAO\") \\\n",
    "               .withColumnRenamed(\"NM_QUALIFICACAO\", \"NM_QUALIFICAÇAO_SOCIO\")\n",
    "        \n",
    "        # Conversão da coluna de data\n",
    "        df = df.withColumn(\"DT_ENTRADA_SOCIEDADE\", to_date(col('DATA_ENTRADA_SOCIEDADE'), \"yyyyMMdd\")) \\\n",
    "               .drop(\"DATA_ENTRADA_SOCIEDADE\")\n",
    "        \n",
    "        # Leitura do arquivo CSV de nomes\n",
    "        file_path = os.path.join(save_base_path, file_name)\n",
    "        df_csv = self.spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "        \n",
    "        # Explodir a coluna alternative_names para múltiplas linhas\n",
    "        df_csv = df_csv.withColumn(\"alternative_name2\", explode(split(col(\"alternative_names\"), \"\\|\")))\n",
    "        \n",
    "        # Selecionar as colunas desejadas\n",
    "        df_result = df_csv.select(\"alternative_name2\", \"group_name\", \"ratio\", \"classification\") \\\n",
    "                          .dropDuplicates([\"alternative_name2\"])\n",
    "        \n",
    "        # Extrair o primeiro nome da coluna NOME_SOCIO_RAZAO_SOCIAL\n",
    "        df = df.withColumn(\"PRIMEIRO_NOME\", split(col(\"NOME_SOCIO_RAZAO_SOCIAL\"), \" \")[0].alias(\"PRIMEIRO_NOME\"))\n",
    "        \n",
    "        # Fazer o join entre df_filter e df_result\n",
    "        joined_df = df.join(df_result, df.PRIMEIRO_NOME == df_result.alternative_name2, \"left\").dropDuplicates()\n",
    "        \n",
    "        # Selecionar as colunas finais\n",
    "        joined_df = joined_df.select(\n",
    "            'CNPJ_BASICO', 'NOME_SOCIO_RAZAO_SOCIAL', 'CNPJ_CPF_SOCIO', 'REPRESENTANTE_LEGAL',\n",
    "            'NOME_REPRESENTANTE', 'NM_PAIS', 'NM_FAIXA_ETARIA', 'NM_IDENTIFICADOR_SOCIO',\n",
    "            'NM_QUALIFICACAO_REPRESENTANTE_LEGAL', 'NM_QUALIFICAÇAO_SOCIO', 'DT_ENTRADA_SOCIEDADE',\n",
    "            'PRIMEIRO_NOME', \n",
    "            col('ratio').alias('PROBABILIDADE_CLASSIFICACAO'),\n",
    "            col('classification').alias('CLASSIFICACAO')\n",
    "        ).dropDuplicates()\n",
    "        \n",
    "        return joined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9389dc76-3978-4682-9fcc-3b97f670601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReceitaLT:\n",
    "    \"\"\"\n",
    "    A classe `ReceitaLT` facilita a manipulação e análise de dados da Receita Federal do Brasil.\n",
    "\n",
    "    Atributos:\n",
    "        spark (SparkSession): Sessão Spark para manipulação de dataframes.\n",
    "        logger (Logger): Logger para capturar e exibir logs.\n",
    "\n",
    "    Atributos estáticos:\n",
    "        - estabelecimentos: Schema para dados de estabelecimentos.\n",
    "        - empresas: Schema para dados das empresas.\n",
    "        - municipios: Schema para municípios.\n",
    "        - cnaes: Schema para CNAEs.\n",
    "        - paises: Schema para países.\n",
    "        - qualificacoes: Schema para qualificações.\n",
    "        - socios: Schema para sócios.\n",
    "        - simples: Schema para opções do Simples Nacional.\n",
    "        - naturezas: Schema para naturezas jurídicas.\n",
    "        - motivos: Schema para motivos de situações cadastrais.\n",
    "        - dic_provedor: Dicionário para correção de nomes de provedores de email.\n",
    "\n",
    "    Métodos:\n",
    "        detect_encoding(file_pattern_or_path, num_bytes=10000): Detecta a codificação do arquivo ou arquivos fornecidos.\n",
    "\n",
    "    Uso:\n",
    "        1. Instancie a classe com uma sessão Spark.\n",
    "        2. Utilize os schemas estáticos para leitura de arquivos.\n",
    "        3. Use o método `detect_encoding` para determinar a codificação de arquivos antes de lê-los.\n",
    "\n",
    "    Exemplo:\n",
    "        from pyspark.sql import SparkSession\n",
    "\n",
    "        spark_session = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "        receita_helper = ReceitaLT(spark_session)\n",
    "        encodings = receita_helper.detect_encoding(\"path/to/datafile.csv\")\n",
    "        df = spark_session.read.csv(\"path/to/datafile.csv\", schema=ReceitaLT.empresas, encoding=encodings[\"path/to/datafile.csv\"])\n",
    "    \"\"\"\n",
    "\n",
    "    # Definição dos Schemas\n",
    "    estabelecimentos = StructType([\n",
    "        StructField(\"CNPJ_BASICO\", StringType(), nullable=True),\n",
    "        StructField(\"CNPJ_ORDEM\", StringType(), nullable=True),\n",
    "        StructField(\"CNPJ_DV\", StringType(), nullable=True),\n",
    "        StructField(\"MATRIZ_FILIAL\", StringType(), nullable=True),\n",
    "        StructField(\"NOME_FANTASIA\", StringType(), nullable=True),\n",
    "        StructField(\"SIT_CADASTRAL\", IntegerType(), nullable=True),\n",
    "        StructField(\"DT_SIT_CADASTRAL\", StringType(), nullable=True),\n",
    "        StructField(\"MOTIVO_CADASTRAL\", StringType(), nullable=True),\n",
    "        StructField(\"NOME_CIDADE_EXTERIOR\", StringType(), nullable=True),\n",
    "        StructField(\"PAIS\", StringType(), nullable=True),\n",
    "        StructField(\"DT_INICIO_ATIVIDADE\", StringType(), nullable=True),\n",
    "        StructField(\"CNAE_1\", StringType(), nullable=True),\n",
    "        StructField(\"CNAE_2\", StringType(), nullable=True),\n",
    "        StructField(\"TIPO_LOUGRADOURO\", StringType(), nullable=True),\n",
    "        StructField(\"LOGRADOURO\", StringType(), nullable=True),\n",
    "        StructField(\"NUMERO\", IntegerType(), nullable=True),\n",
    "        StructField(\"COMPLEMENTO\", StringType(), nullable=True),\n",
    "        StructField(\"BAIRRO\", StringType(), nullable=True),\n",
    "        StructField(\"CEP\", IntegerType(), nullable=True),\n",
    "        StructField(\"UF\", StringType(), nullable=True),\n",
    "        StructField(\"MUNICIPIO\", StringType(), nullable=True),\n",
    "        StructField(\"DDD1\", StringType(), nullable=True),\n",
    "        StructField(\"TEL1\", StringType(), nullable=True),\n",
    "        StructField(\"DDD2\", StringType(), nullable=True),\n",
    "        StructField(\"TEL2\", StringType(), nullable=True),\n",
    "        StructField(\"DDD_FAX\", IntegerType(), nullable=True),\n",
    "        StructField(\"FAX\", IntegerType(), nullable=True),\n",
    "        StructField(\"EMAIL\", StringType(), nullable=True),\n",
    "        StructField(\"SIT_ESPECIAL\", StringType(), nullable=True),\n",
    "        StructField(\"DT_SIT_ESPECIAL\", StringType(), nullable=True)\n",
    "    ])\n",
    "\n",
    "    empresas = StructType([\n",
    "        StructField(\"CNPJ\", StringType(), nullable=True),\n",
    "        StructField(\"NOME_EMPRESA\", StringType(), nullable=True),\n",
    "        StructField(\"COD_NAT_JURICA\", StringType(), nullable=True),\n",
    "        StructField(\"QUALIF_RESPONVAVEL\", StringType(), nullable=True),\n",
    "        StructField(\"CAP_SOCIAL\", StringType(), nullable=True),\n",
    "        StructField(\"PORTE\", StringType(), nullable=True),\n",
    "        StructField(\"ENTE_FEDERATIVO\", StringType(), nullable=True)\n",
    "    ])\n",
    "\n",
    "    municipios = StructType([\n",
    "        StructField(\"ID_MUNICPIO\", StringType(), nullable=True),\n",
    "        StructField(\"MUNICIPIO\", StringType(), nullable=True)\n",
    "    ])\n",
    "\n",
    "    cnaes = StructType([\n",
    "        StructField(\"COD_CNAE\", StringType(), nullable=True),\n",
    "        StructField(\"CNAE\", StringType(), nullable=True)\n",
    "    ])\n",
    "    \n",
    "    paises = StructType([\n",
    "        StructField(\"COD_PAIS\", StringType(), nullable=True),\n",
    "        StructField(\"NM_PAIS\", StringType(), nullable=True)\n",
    "    ])\n",
    "    \n",
    "    qualificacoes = StructType([\n",
    "        StructField(\"COD_QUALIFICACAO\", StringType(), nullable=True),\n",
    "        StructField(\"NM_QUALIFICACAO\", StringType(), nullable=True)\n",
    "    ])\n",
    "\n",
    "    socios = StructType([\n",
    "        StructField(\"CNPJ_BASICO\", StringType(), nullable=True),\n",
    "        StructField(\"IDENTIFICADOR_SOCIO\", IntegerType(), nullable=True),\n",
    "        StructField(\"NOME_SOCIO_RAZAO_SOCIAL\", StringType(), nullable=True),\n",
    "        StructField(\"CNPJ_CPF_SOCIO\", StringType(), nullable=True),\n",
    "        StructField(\"QUALIFICAÇAO_SOCIO\", StringType(), nullable=True),\n",
    "        StructField(\"DATA_ENTRADA_SOCIEDADE\", StringType(), nullable=True),\n",
    "        StructField(\"PAIS\", StringType(), nullable=True),\n",
    "        StructField(\"REPRESENTANTE_LEGAL\", StringType(), nullable=True),\n",
    "        StructField(\"NOME_REPRESENTANTE\", StringType(), nullable=True),\n",
    "        StructField(\"QUALIFICACAO_REPRESENTANTE_LEGAL\", StringType(), nullable=True),\n",
    "        StructField(\"FAIXA_ETARIA\", StringType(), nullable=True)\n",
    "    ])\n",
    "\n",
    "    simples = StructType([\n",
    "        StructField(\"CNPJ_BASICO\", StringType(), nullable=True),\n",
    "        StructField(\"OPCAO_PELO_SIMPLES\", StringType(), nullable=True),\n",
    "        StructField(\"DATA_OPCAO_PELO_SIMPLES\", StringType(), nullable=True),\n",
    "        StructField(\"DATA_EXCLUSAO_SIMPLES\", StringType(), nullable=True),\n",
    "        StructField(\"OPÇAO_PELO_MEI\", StringType(), nullable=True),\n",
    "        StructField(\"DATA_OPCAO_PELO_MEI\", StringType(), nullable=True),\n",
    "        StructField(\"DATA_EXCLUSAO_MEI\", StringType(), nullable=True)\n",
    "    ])\n",
    "\n",
    "    naturezas = StructType([\n",
    "        StructField(\"COD_NAT_JURICA\", StringType(), nullable=True),\n",
    "        StructField(\"NAT_JURICA\", StringType(), nullable=True)\n",
    "    ])\n",
    "    \n",
    "    motivos = StructType([\n",
    "        StructField(\"COD_MOTIVO\", StringType(), nullable=True),\n",
    "        StructField(\"NM_MOTIVO\", StringType(), nullable=True)\n",
    "    ])\n",
    "\n",
    "    dic_provedor = {\n",
    "        'OUTLOOK': 'OUTLOOK', 'OUTIOOK': 'OUTLOOK', 'OUTLLOK': 'OUTLOOK', 'OUTLLOOK': 'OUTLOOK', \n",
    "        'OUTLOCK': 'OUTLOOK', 'OUTLOK': 'OUTLOOK', 'OUTLOKK': 'OUTLOOK', 'OUTLOOCK': 'OUTLOOK', 'OUTLOOK': 'OUTLOOK', \n",
    "        'OUTLOOKL': 'OUTLOOK', 'OUTLOOL': 'OUTLOOK', 'OUTLOOOK': 'OUTLOOK', 'OUTLUK': 'OUTLOOK', 'OUTOLOOK': 'OUTLOOK',\n",
    "        'OUTOOK': 'OUTLOOK', 'OUTOOLK': 'OUTLOOK', 'OUTTLOOK': 'OUTLOOK', 'OUTULOOK': 'OUTLOOK', 'POP': 'POP',\n",
    "        'PROTON': 'PROTONMAIL', 'PROTONMAIL': 'PROTONMAIL', 'PUTLOOK': 'OUTLOOK', 'R7': 'R7', 'ROCKETMAIL': 'ROCKETMAIL', \n",
    "        'ROCKTMAIL': 'ROCKETMAIL', 'ROTMAIL': 'HOTMAIL', 'SERCOMTEL': 'SERCOMTEL', 'SETELAGOASGML': 'GMAIL', \n",
    "        'SUPERIG': 'SUPERIG', 'TAHOO': 'YAHOO', 'TERRA': 'TERRA', 'TERRRA': 'TERRA', 'TMAIL': 'GMAIL', \n",
    "        'TVGLOBO': 'GLOBO', 'UAHOO': 'YAHOO', 'UAI': 'UAI', 'UFV': 'UFV', 'UNESP': 'UNESP', 'UNOCHAPECO': 'UNOCHAPECO', \n",
    "        'UO': 'UOL', 'UOL': 'UOL', 'UOTLOOK': 'OUTLOOK', 'UPF': 'UPF', 'USP': 'USP', 'UTLOOK': 'OUTLOOK', 'VELOXMAIL': 'VELOXMAIL',\n",
    "        'WINDOWSLIVE': 'WINDOWSLIVE', 'YAAHOO': 'YAHOO', 'YAGOO': 'YAHOO', 'YAHAOO': 'YAHOO', 'YAHHO': 'YAHOO', 'YAHHOO': 'YAHOO', \n",
    "        'YAHO': 'YAHOO', 'YAHOO': 'YAHOO', 'YAHOOCOM': 'YAHOO', 'YAHOOL': 'YAHOO', 'YAHOOO': 'YAHOO', 'YAHOOU': 'YAHOO', \n",
    "        'YANHOO': 'YAHOO', 'YAOO': 'YAHOO', 'YAOOL': 'YAHOO', 'YAROO': 'YAHOO', 'YHAOO': 'YAHOO', 'YHOO': 'YAHOO', 'YMAIL': 'YMAIL', \n",
    "        'YOHOO': 'YAHOO', 'YOPMAIL': 'HOTMAIL', 'ZIPMAIL': 'ZIPMAIL', '_HOTMAIL': 'HOTMAIL', 'GMAUL': 'GMAIL','GMALE': 'GMAIL', \n",
    "        'GMAILE': 'GMAIL', 'GMILE': 'GMAIL', 'HOTMEL': 'HOTMAIL', 'HOTMELL': 'HOTMAIL','HOTMEAL': 'HOTMAIL', 'OUTLOKES': 'OUTLOOK', \n",
    "        'OTLOOKS': 'OUTLOOK', 'YAHU': 'YAHOO', 'YOHU': 'YAHOO', 'YAHUO': 'YAHOO', 'YAHEE': 'YAHOO', 'UOLL': 'UOL',\n",
    "        'UOOL': 'UOL', 'UULL': 'UOL', 'ICLODUE': 'ICLOUD', 'ICLAWD': 'ICLOUD', 'ROCKEDMAIL': 'ROCKETMAIL', 'ROKETMAIL': 'ROCKETMAIL',\n",
    "        'OUTLOKE': 'OUTLOOK', 'OUTLOOCKE': 'OUTLOOK', 'YAAHO': 'YAHOO', 'YAHOOE': 'YAHOO', 'YAHUE': 'YAHOO', 'HOTMILE': 'HOTMAIL', 'HOTMELE': 'HOTMAIL',\n",
    "        'FACEBOKE': 'FACEBOOK', 'FACBOOK': 'FACEBOOK', 'FCEBOOK': 'FACEBOOK', 'BOLL': 'BOL', 'BOLLE': 'BOL', 'BULE': 'BOL', 'GLOBOE': 'GLOBO',\n",
    "        'GLOBU': 'GLOBO', 'GMILE': 'GMAIL', 'MSNE': 'MSN', 'MSNN': 'MSN', 'ICLOOUD': 'ICLOUD', 'OUTLUKE': 'OUTLOOK', 'OUTLLOKE': 'OUTLOOK',\n",
    "        'PROTONMIAL': 'PROTONMAIL', 'PROTONMALE': 'PROTONMAIL', 'PROTOMAIL': 'PROTONMAIL', \n",
    "        'OULOOKCOM': 'OUTLOOK', 'YAHCOM': 'YAHOO',  'YAHOCOM': 'YAHOO','GAMILCOM': 'GMAIL', 'GMALCOM': 'GMAIL',  \n",
    "        'HOTMALCOM': 'HOTMAIL',  'HOTMILCOM': 'HOTMAIL', 'HOTMELCOM': 'HOTMAIL', 'ROCKMAIL': 'ROCKETMAIL', \n",
    "        'ROKMAIL': 'ROCKETMAIL', 'TERA': 'TERRA', 'TEERA': 'TERRA', \n",
    "        'FACBOOKCOM': 'FACEBOOK', 'FACEBOOKCOM': 'FACEBOOK', 'ICLOWD': 'ICLOUD', 'ICLOUND': 'ICLOUD', \n",
    "        'UOOLCOM': 'UOL', 'UOLLCOM': 'UOL', 'UOLCOMBR': 'UOL','LIVECOM': 'LIVE', 'LIVECOMBR': 'LIVE', \n",
    "        'GMAICOM': 'GMAIL',  'GMAILCOMBR': 'GMAIL',  'YAHOOBR': 'YAHOO', \n",
    "        'YAHOOOCOMBR': 'YAHOO', 'YAHOOOCOM': 'YAHOO', 'ZIPMAILE': 'ZIPMAIL', 'ZIPMAILL': 'ZIPMAIL',  \n",
    "        'IBESTT': 'IBEST', 'IBESTE': 'IBEST'\n",
    "    }\n",
    "\n",
    "    def __init__(self, spark: SparkSession):\n",
    "        \"\"\"\n",
    "        Inicializa a classe ReceitaLT.\n",
    "        \n",
    "        Parâmetros:\n",
    "        spark (SparkSession): Uma sessão Spark ativa.\n",
    "        \"\"\"\n",
    "        self.spark = spark\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        # A configuração do logger já foi feita no início do script\n",
    "\n",
    "    @staticmethod\n",
    "    def geocode_address(address):\n",
    "        \"\"\"\n",
    "        Geocodifica um endereço, convertendo-o em coordenadas de latitude e longitude.\n",
    "\n",
    "        Parâmetros:\n",
    "            address (str): Endereço a ser geocodificado.\n",
    "\n",
    "        Retorna:\n",
    "            tuple: Um par contendo a latitude e a longitude do endereço fornecido. \n",
    "                   Se o endereço não puder ser geocodificado, retorna (None, None).\n",
    "\n",
    "        Exemplo:\n",
    "            lat, lon = geocode_address(\"1600 Amphitheatre Parkway, Mountain View, CA\")\n",
    "\n",
    "        Notas:\n",
    "            - Usa o serviço Nominatim para a geocodificação.\n",
    "            - Incorpora um limitador de taxa para garantir que não excedamos os limites de requisições por segundo \n",
    "              do serviço.\n",
    "        \"\"\"\n",
    "        geolocator = Nominatim(user_agent=\"CNPJ_GEOLOCATION\")\n",
    "        geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n",
    "        location = geocode(address)\n",
    "        if location:\n",
    "            return (location.latitude, location.longitude)\n",
    "        else:\n",
    "            return (None, None)\n",
    "\n",
    "    @staticmethod\n",
    "    def geocode_udf(address):\n",
    "        \"\"\"\n",
    "        UDF do Spark para geocodificar um endereço dentro de um DataFrame.\n",
    "\n",
    "        Parâmetros:\n",
    "            address (str): Endereço a ser geocodificado.\n",
    "\n",
    "        Retorna:\n",
    "            dict: Dicionário contendo a 'latitude' e a 'longitude' do endereço fornecido.\n",
    "                  Se o endereço não puder ser geocodificado, os valores serão None.\n",
    "\n",
    "        Exemplo:\n",
    "            df.withColumn(\"location\", ReceitaLT.geocode_udf(df[\"address\"]))\n",
    "\n",
    "        Notas:\n",
    "            - Esta UDF encapsula a função `geocode_address`.\n",
    "            - Retorna um tipo de dado complexo (Struct) com dois campos: 'latitude' e 'longitude'.\n",
    "        \"\"\"\n",
    "        lat, lon = ReceitaLT.geocode_address(address)\n",
    "        return {\"latitude\": lat, \"longitude\": lon}\n",
    "\n",
    "    # Definição do Schema para a UDF\n",
    "    geocode_schema = StructType([\n",
    "        StructField(\"latitude\", FloatType(), nullable=True),\n",
    "        StructField(\"longitude\", FloatType(), nullable=True)\n",
    "    ])\n",
    "\n",
    "    # Criação do UDF com o schema definido\n",
    "    geocode_udf = udf(geocode_udf, geocode_schema)\n",
    "\n",
    "    @staticmethod\n",
    "    def detect_encoding(file_pattern_or_path, num_bytes=10000):\n",
    "        \"\"\"\n",
    "        Detecta a codificação do arquivo ou arquivos fornecidos.\n",
    "        \n",
    "        Parâmetros:\n",
    "            file_pattern_or_path (str): Caminho ou padrão do arquivo para detecção.\n",
    "            num_bytes (int, opcional): Número de bytes para ler para a detecção. Padrão é 10000.\n",
    "        \n",
    "        Retorna:\n",
    "            dict: Dicionário com caminho do arquivo como chave e codificação detectada como valor.\n",
    "        \"\"\"\n",
    "        files = glob.glob(file_pattern_or_path)\n",
    "        encodings = {}\n",
    "        for file_path in files:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                rawdata = f.read(num_bytes)\n",
    "                encodings[file_path] = chardet.detect(rawdata)[\"encoding\"]\n",
    "        return encodings\n",
    "\n",
    "    def read_data(self, schema_name, base_path=None):\n",
    "        \"\"\"\n",
    "        Lê dados de vários arquivos CSV de acordo com o esquema e caminho base fornecidos, consolidando-os \n",
    "        em um único DataFrame do Spark.\n",
    "\n",
    "        Parâmetros:\n",
    "            schema_name (str): Nome do esquema a ser usado para a leitura dos arquivos.\n",
    "                               Deve ser uma das chaves do dicionário `schemas`.\n",
    "\n",
    "            base_path (str, opcional): Caminho base dos arquivos CSV.\n",
    "                                       Se não for fornecido, ele tentará buscar da variável de ambiente 'BASE_PATH'.\n",
    "                                       Caso não encontre, o padrão \"./output\" será utilizado.\n",
    "\n",
    "        Retorna:\n",
    "            DataFrame: DataFrame do Spark contendo os dados consolidados dos arquivos CSV.\n",
    "\n",
    "        Exceções:\n",
    "            Pode lançar uma exceção se o arquivo não estiver presente no caminho especificado ou\n",
    "            se houver problemas de codificação ao ler o arquivo.\n",
    "\n",
    "        Exemplo:\n",
    "            receita_helper = ReceitaLT(spark_session)\n",
    "            df = receita_helper.read_data(\"estabelecimentos\", \"/path/to/csv/files\")\n",
    "\n",
    "        Notas:\n",
    "            - A função primeiro detecta a codificação dos arquivos antes de lê-los para garantir que \n",
    "              eles sejam lidos corretamente.\n",
    "            - A função lida com múltiplos arquivos CSV e os une em um único DataFrame.\n",
    "            - O formato de arquivo assumido é CSV com delimitador \";\", sem cabeçalho e com aspas para delimitar campos.\n",
    "        \"\"\"\n",
    "        schemas = {\n",
    "            \"estabelecimentos\": self.estabelecimentos,\n",
    "            \"empresas\": self.empresas,\n",
    "            \"municipios\": self.municipios,\n",
    "            \"cnaes\": self.cnaes,\n",
    "            \"socios\": self.socios,\n",
    "            \"simples\": self.simples,\n",
    "            \"naturezas\": self.naturezas,\n",
    "            \"qualificacoes\": self.qualificacoes,\n",
    "            \"motivos\": self.motivos,\n",
    "            \"paises\": self.paises\n",
    "        }\n",
    "\n",
    "        # Se o base_path não for fornecido, pegar da variável de ambiente ou usar um padrão.\n",
    "        if not base_path:\n",
    "            base_path = os.environ.get('BASE_PATH', \"./output\")\n",
    "\n",
    "        if schema_name not in schemas:\n",
    "            raise ValueError(f\"Schema '{schema_name}' não está definido.\")\n",
    "\n",
    "        # Definir o padrão do caminho dos arquivos\n",
    "        if schema_name in ['estabelecimentos', 'empresas', 'socios']:\n",
    "            file_location_pattern = os.path.join(base_path, schema_name.capitalize(), '*.csv')\n",
    "        else:\n",
    "            file_location_pattern = os.path.join(base_path, schema_name.capitalize(), f\"{schema_name.capitalize()}.csv\")\n",
    "\n",
    "        # Detectar codificações\n",
    "        encodings = self.detect_encoding(file_location_pattern)\n",
    "        self.logger.info(f\"Detected encodings: {encodings}\")\n",
    "\n",
    "        # Agora, vamos ler cada arquivo com sua codificação correta e armazenar em uma lista de DataFrames\n",
    "        dfs = []\n",
    "        for file_location, encoding in encodings.items():\n",
    "            try:\n",
    "                df = (self.spark.read.format(\"csv\")\n",
    "                      .option(\"sep\", \";\")\n",
    "                      .option(\"header\", \"false\")\n",
    "                      .option('quote', '\"')\n",
    "                      .option(\"escape\", '\"')\n",
    "                      .option(\"encoding\", encoding)\n",
    "                      .schema(schemas[schema_name])\n",
    "                      .load(file_location))\n",
    "                dfs.append(df)\n",
    "                self.logger.info(f\"Arquivo lido com sucesso: {file_location}\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Erro ao ler o arquivo {file_location}: {e}\")\n",
    "                raise e\n",
    "\n",
    "        # Unir todos os DataFrames em um único DataFrame\n",
    "        if dfs:\n",
    "            final_df = reduce(lambda a, b: a.union(b), dfs)\n",
    "        else:\n",
    "            final_df = self.spark.createDataFrame([], schemas[schema_name])\n",
    "\n",
    "        return final_df\n",
    "\n",
    "    def save_data(self, df, path, num_partitions=16, file_format=\"parquet\"):\n",
    "        \"\"\"\n",
    "        Salva o DataFrame no caminho especificado.\n",
    "        \n",
    "        Parâmetros:\n",
    "            df (DataFrame): DataFrame a ser salvo.\n",
    "            path (str): Caminho de destino.\n",
    "            num_partitions (int, opcional): Número de partições para salvar os dados (padrão é 1).\n",
    "            file_format (str, opcional): Formato do arquivo para salvar os dados (padrão é \"parquet\").\n",
    "        \"\"\"\n",
    "        # Reparticionar o DataFrame conforme a entrada do usuário\n",
    "        df = df.repartition(num_partitions)\n",
    "        \n",
    "        # Salvar o DataFrame no caminho e formato especificados\n",
    "        df.write.mode('overwrite').format(file_format).save(path)\n",
    "        self.logger.info(f\"DataFrame salvo com sucesso em {path} no formato {file_format}.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def download_nomes(save_base_path=\"./output/nomes\"):\n",
    "        \"\"\"\n",
    "        Baixa e extrai o arquivo nomes.csv.gz do dataset genero-nomes no Brasil.io.\n",
    "\n",
    "        Parâmetros:\n",
    "            save_base_path (str, opcional): Caminho base onde o arquivo será salvo. O padrão é './output/nomes'.\n",
    "\n",
    "        Descrição:\n",
    "            - Cria o diretório de salvamento se ele não existir.\n",
    "            - Baixa o arquivo nomes.csv.gz da URL especificada.\n",
    "            - Extrai o conteúdo do arquivo .gz.\n",
    "            - Remove o arquivo .gz original, mantendo apenas o arquivo CSV extraído.\n",
    "\n",
    "        Notas:\n",
    "            - Esta função usa a biblioteca `requests` para baixar o arquivo.\n",
    "            - A função verifica se a resposta do servidor é 200 (sucesso) antes de baixar o arquivo.\n",
    "            - O arquivo .gz é extraído usando a biblioteca `gzip`.\n",
    "        \"\"\"\n",
    "        os.makedirs(save_base_path, exist_ok=True)\n",
    "        url = \"https://data.brasil.io/dataset/genero-nomes/nomes.csv.gz\"\n",
    "        file_name = os.path.basename(url)\n",
    "        file_path = os.path.join(save_base_path, file_name)\n",
    "        extracted_file_path = os.path.join(save_base_path, file_name[:-3])  # remove .gz\n",
    "\n",
    "        # Baixe o arquivo\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(file_path, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=128):\n",
    "                    file.write(chunk)\n",
    "            logger.info(f\"Arquivo baixado com sucesso: {file_path}\")\n",
    "        else:\n",
    "            logger.error(f\"Falha ao baixar {url}. Status code: {response.status_code}\")\n",
    "            return\n",
    "\n",
    "        # Extraia o arquivo\n",
    "        try:\n",
    "            with gzip.open(file_path, 'rb') as f_in:\n",
    "                with open(extracted_file_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "            logger.info(f\"Arquivo extraído com sucesso: {extracted_file_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Erro ao extrair o arquivo {file_path}: {e}\")\n",
    "            raise e\n",
    "\n",
    "        # Apague o arquivo .gz\n",
    "        os.remove(file_path)\n",
    "        logger.info(f\"Arquivo .gz removido: {file_path}\")\n",
    "\n",
    "    def process_estabelecimentos(self, df):\n",
    "        \"\"\"\n",
    "        Processa e enriquece o DataFrame de estabelecimentos com informações adicionais e transformações.\n",
    "\n",
    "        Parâmetros:\n",
    "            df (DataFrame): DataFrame inicial contendo informações de estabelecimentos.\n",
    "\n",
    "        Retorna:\n",
    "            DataFrame: DataFrame processado e enriquecido com novas colunas e informações.\n",
    "\n",
    "        Descrição:\n",
    "            - Lê dataframes adicionais relacionados a países, municípios, cnaes e motivos.\n",
    "            - Realiza renomeações de colunas para facilitar junções.\n",
    "            - Enriquece o dataframe com informações de motivos, cnaes, municípios e países.\n",
    "            - Processa colunas de e-mail, separando provedores e corrigindo valores.\n",
    "            - Converte colunas de data de string para formato de data.\n",
    "            - Deriva colunas de ano e mês a partir de datas.\n",
    "            - Processa e deriva novas colunas com base em mapeamentos para situação cadastral e tipo de estabelecimento.\n",
    "            - Valida endereços de e-mail usando expressões regulares.\n",
    "            - Combina informações de endereço para formar uma coluna completa de endereço.\n",
    "            - Utiliza a função de geocodificação para obter coordenadas com base no endereço e, em caso de falha, com base no CEP.\n",
    "            - Realiza correções na coluna de provedor de e-mail usando um dicionário de mapeamento.\n",
    "\n",
    "        Notas:\n",
    "            - Esta função faz uso intensivo das operações de DataFrame do PySpark.\n",
    "            - Dependências: A função depende de outras funções e UDFs, como 'geocode_udf', bem como de variáveis de instância, como 'dic_provedor'.\n",
    "        \"\"\"\n",
    "        # Leitura dos DataFrames auxiliares\n",
    "        df_pais = self.read_data(schema_name='paises')\n",
    "        df_mun = self.read_data(schema_name='municipios')\n",
    "        df_cnaes = self.read_data(schema_name='cnaes')\n",
    "        df_motivos = self.read_data(schema_name='motivos')\n",
    "        \n",
    "        # Renomear colunas para facilitar as junções\n",
    "        df = df.withColumnRenamed(\"CNAE_1\", \"COD_CNAE\") \\\n",
    "               .withColumnRenamed(\"MUNICIPIO\", \"ID_MUNICPIO\") \\\n",
    "               .withColumnRenamed(\"PAIS\", \"COD_PAIS\") \\\n",
    "               .withColumnRenamed(\"MOTIVO_CADASTRAL\", \"COD_MOTIVO\") \n",
    "        \n",
    "        # Realizar junções com os DataFrames auxiliares\n",
    "        df = df.join(broadcast(df_motivos), \"COD_MOTIVO\", \"left\").drop(\"COD_MOTIVO\")\n",
    "        df = df.join(broadcast(df_cnaes), \"COD_CNAE\", \"left\").drop(\"COD_CNAE\")\n",
    "        df = df.join(broadcast(df_mun), \"ID_MUNICPIO\", \"left\").drop(\"ID_MUNICPIO\")\n",
    "        df = df.join(broadcast(df_pais), \"COD_PAIS\", \"left\").drop(\"COD_PAIS\")\n",
    "        \n",
    "        # Tratamento da coluna PROVEDOR\n",
    "        df = df.withColumn(\"PROVEDOR\", regexp_extract(\"EMAIL\", \"(?<=@)[^.]+(?=\\\\.)\", 0))\n",
    "        df = df.withColumn(\"PROVEDOR\", upper(col(\"PROVEDOR\")))\n",
    "        df = df.withColumn(\"EMAIL\", lower(col(\"EMAIL\")))\n",
    "\n",
    "        # Convertendo colunas de data\n",
    "        df = df.withColumn(\"DT_SIT_CADASTRAL\", to_date(col('DT_SIT_CADASTRAL'), \"yyyyMMdd\")) \\\n",
    "               .withColumn(\"DT_INICIO_ATIVIDADE\", to_date(col('DT_INICIO_ATIVIDADE'), \"yyyyMMdd\")) \\\n",
    "               .withColumn(\"DT_SIT_ESPECIAL\", to_date(col('DT_SIT_ESPECIAL'), \"yyyyMMdd\"))\n",
    "        \n",
    "        # Derivando colunas de ano e mês\n",
    "        df = df.withColumn(\"ano_cadastro\", F.year('DT_INICIO_ATIVIDADE')) \\\n",
    "               .withColumn(\"mes_cadastro\", F.month('DT_INICIO_ATIVIDADE')) \\\n",
    "               .withColumn(\"ano_sit_cadastral\", F.year('DT_SIT_CADASTRAL')) \\\n",
    "               .withColumn(\"mes_sit_cadastral\", F.month('DT_SIT_CADASTRAL'))\n",
    "        \n",
    "        # Mapeamento para Situação Cadastral\n",
    "        mapping_sit_cadastral = {1: 'NULA', 2: 'ATIVA', 3: 'SUSPENSA', 4: 'INAPTA', 8: 'BAIXADA'}\n",
    "        \n",
    "        # Criando a coluna NM_SIT_CADASTRAL\n",
    "        df = df.withColumn(\"NM_SIT_CADASTRAL\", \n",
    "                           when(col(\"SIT_CADASTRAL\").isin(list(mapping_sit_cadastral.keys())), col(\"SIT_CADASTRAL\")).otherwise(None))\n",
    "        \n",
    "        for key, value in mapping_sit_cadastral.items():\n",
    "            df = df.withColumn(\"NM_SIT_CADASTRAL\", \n",
    "                               when(col(\"SIT_CADASTRAL\") == key, value).otherwise(col(\"NM_SIT_CADASTRAL\")))\n",
    "        \n",
    "        # Validação de e-mails\n",
    "        email_pattern = r'^\\S+@\\S+\\.\\S+$'  # Padrão simples de endereço de e-mail\n",
    "        df = df.withColumn(\"valid_email\", regexp_extract(col(\"EMAIL\"), email_pattern, 0))\n",
    "        \n",
    "        # Mapeamento para Tipo de Estabelecimento\n",
    "        mapping_tipo = {1: 'MATRIZ', 2: 'FILIAL'}\n",
    "        \n",
    "        df = df.withColumn(\"NM_MATRIZ_FILIAL\", \n",
    "                           when(col(\"MATRIZ_FILIAL\").isin(list(mapping_tipo.keys())), col(\"MATRIZ_FILIAL\")).otherwise(None))\n",
    "        \n",
    "        for key, value in mapping_tipo.items():\n",
    "            df = df.withColumn(\"NM_MATRIZ_FILIAL\", \n",
    "                               when(col(\"MATRIZ_FILIAL\") == key, value).otherwise(col(\"NM_MATRIZ_FILIAL\")))\n",
    "        \n",
    "        # Criando a coluna ENDERECO_COMPLETO\n",
    "        df = df.withColumn(\"ENDERECO_COMPLETO\",\n",
    "                           concat_ws(\", \",\n",
    "                                     concat(col(\"TIPO_LOUGRADOURO\"), lit(\" \"), col(\"LOGRADOURO\")),\n",
    "                                     col(\"NUMERO\"),\n",
    "                                     concat_ws(\" - \", col(\"MUNICIPIO\"), col(\"UF\"))))\n",
    "        \n",
    "        # Adicionando a lógica de geocodificação\n",
    "        df = df.withColumn(\"COORDENADAS\", ReceitaLT.geocode_udf(col(\"ENDERECO_COMPLETO\")))\n",
    "        \n",
    "        # Caso a geocodificação falhe, tentar com o CEP\n",
    "        df = df.withColumn(\"COORDENADAS\",\n",
    "                           when((col(\"COORDENADAS.latitude\").isNull()) & (col(\"COORDENADAS.longitude\").isNull()),\n",
    "                                ReceitaLT.geocode_udf(col(\"CEP\"))).otherwise(col(\"COORDENADAS\")))\n",
    "\n",
    "        # Caso a geocodificação falhe com CEP\n",
    "        # df = df.withColumn(\"COORDENADAS\",\n",
    "        #                   when((col(\"COORDENADAS.latitude\").isNull()) & (col(\"COORDENADAS.longitude\").isNull()),\n",
    "        #                        ReceitaLT.geocode_udf(col(\"MUNICIPIO\"))).otherwise(col(\"COORDENADAS\")))\n",
    "        \n",
    "        # Correção da coluna PROVEDOR\n",
    "        df = df.replace(self.dic_provedor, subset=['PROVEDOR'])\n",
    "        \n",
    "        # Transformação das keys e values do dicionário em lowercase\n",
    "        dic_prov_lower = {k.lower(): str(v).lower() for k, v in self.dic_provedor.items()}\n",
    "        \n",
    "        # Correção dos provedores na coluna EMAIL\n",
    "        replace_expr = reduce(\n",
    "            lambda a, b: regexp_replace(a, rf\"\\b{b[0]}\\b\", b[1]),\n",
    "            dic_prov_lower.items(),\n",
    "            col(\"valid_email\")\n",
    "        )\n",
    "        \n",
    "        df = df.withColumn(\"valid_email\", replace_expr) \\\n",
    "               .withColumnRenamed(\"valid_email\", \"VALID_EMAIL\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def process_empresas(self, df):\n",
    "        \"\"\"\n",
    "        Processa e enriquece o DataFrame de empresas com informações adicionais e transformações.\n",
    "\n",
    "        Parâmetros:\n",
    "            df (DataFrame): DataFrame inicial contendo informações de empresas.\n",
    "\n",
    "        Retorna:\n",
    "            DataFrame: DataFrame processado e enriquecido com novas colunas e informações.\n",
    "\n",
    "        Descrição:\n",
    "            - Lê dataframes adicionais relacionados a naturezas jurídicas e qualificações.\n",
    "            - Realiza renomeação de colunas para facilitar junções.\n",
    "            - Enriquece o dataframe com informações de naturezas jurídicas e qualificações.\n",
    "            - Processa a coluna 'NOME_EMPRESA' para extrair informações potenciais de CPF.\n",
    "            - Deriva uma nova coluna baseada no porte da empresa, usando um mapeamento predefinido.\n",
    "            - Determina a probabilidade de um valor ser um CPF válido com base em seu comprimento.\n",
    "            - Criptografa possíveis valores de CPF usando AES e os armazena em uma nova coluna 'CPF_CRIPTOGRAFADO', enquanto remove a coluna original 'CPF'.\n",
    "\n",
    "        Notas:\n",
    "            - Esta função faz uso intensivo das operações de DataFrame do PySpark.\n",
    "            - O valor de criptografia (secret_key) é gerado dinamicamente a cada chamada da função. Portanto, cada execução resultará em valores de 'CPF_CRIPTOGRAFADO' diferentes para os mesmos CPFs.\n",
    "            - O método AES usado aqui é 'ECB', que não é considerado seguro para muitos casos de uso devido à falta de vetor de inicialização (IV). A utilização deste modo deve ser revista se a segurança for uma preocupação.\n",
    "        \"\"\"\n",
    "        # Leitura dos DataFrames auxiliares\n",
    "        df_nat = self.read_data(schema_name='naturezas')\n",
    "        df_qual = self.read_data(schema_name='qualificacoes')\n",
    "        \n",
    "        # Renomear coluna para facilitar a junção\n",
    "        df = df.withColumnRenamed(\"QUALIF_RESPONVAVEL\", \"COD_QUALIFICACAO\")\n",
    "        \n",
    "        # Realizar junções com os DataFrames auxiliares\n",
    "        df = df.join(broadcast(df_nat), \"COD_NAT_JURICA\", \"left\").drop(\"COD_NAT_JURICA\")\n",
    "        df = df.join(broadcast(df_qual), \"COD_QUALIFICACAO\", \"left\").drop(\"COD_QUALIFICACAO\")\n",
    "               \n",
    "        # Extração e tratamento da coluna CPF\n",
    "        df = df.withColumn(\"CPF\", regexp_replace(col(\"NOME_EMPRESA\"), \"[^0-9]\", \"\"))\n",
    "        df = df.withColumn(\"CPF\", when(col(\"CPF\") == \"\", None).otherwise(col(\"CPF\")))\n",
    "        df = df.withColumn('CPF_LEN', length('CPF'))\n",
    "        \n",
    "        # Mapeamento para Porte da Empresa\n",
    "        mapping_porte = {\n",
    "            0: 'NÃO INFORMADO',\n",
    "            1: 'MICRO EMPRESA',\n",
    "            3: 'EMPRESA DE PEQUENO PORTE',\n",
    "            5: 'DEMAIS',\n",
    "            8: 'BAIXADA'\n",
    "        }\n",
    "\n",
    "        # Criando a coluna NM_PORTE\n",
    "        df = df.withColumn(\"NM_PORTE\", \n",
    "                           when(col(\"PORTE\").isin(list(mapping_porte.keys())), col(\"PORTE\")).otherwise(None))\n",
    "        \n",
    "        for key, value in mapping_porte.items():\n",
    "            df = df.withColumn(\"NM_PORTE\", \n",
    "                               when(col(\"PORTE\") == key, value).otherwise(col(\"NM_PORTE\")))\n",
    "        \n",
    "        # Determinando a probabilidade de ser um CPF válido\n",
    "        df = df.withColumn(\"PROBABILIDADE_DE_SER_CPF\", \n",
    "                           when(col(\"CPF_LEN\") == 11, \"SIM\").otherwise(\"NAO\"))\n",
    "        \n",
    "        # Gerando uma chave secreta para criptografia\n",
    "        secret_key = secrets.token_urlsafe(24)\n",
    "        df = df.withColumn(\"CPF_CRIPTOGRAFADO\", \n",
    "                           expr(f\"base64(aes_encrypt(CPF, '{secret_key}', 'ECB', 'PKCS'))\")) \\\n",
    "               .drop(\"CPF\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def process_simples(self, df):\n",
    "        \"\"\"\n",
    "        Processa o DataFrame relacionado ao regime tributário SIMPLES das empresas.\n",
    "\n",
    "        Parâmetros:\n",
    "            df (pyspark.sql.DataFrame): DataFrame inicial contendo informações relacionadas ao regime tributário SIMPLES.\n",
    "\n",
    "        Retorna:\n",
    "            pyspark.sql.DataFrame: DataFrame processado com colunas de data convertidas e apenas as colunas relevantes selecionadas.\n",
    "\n",
    "        Descrição:\n",
    "            - Converte colunas que representam datas do formato \"yyyyMMdd\" para o tipo data.\n",
    "            - Seleciona apenas as colunas relevantes para o contexto, que são: 'CNPJ_BASICO', 'OPÇAO_PELO_MEI', 'DT_OPCAO_MEI', 'DT_EXCLUSAO_MEI', 'OPCAO_PELO_SIMPLES', 'DT_OPCAO_SIMPLES', e 'DT_EXCLUSAO_SIMPLES'.\n",
    "\n",
    "        Notas:\n",
    "            - Esta função assume que as colunas de data estão no formato \"yyyyMMdd\" e realiza a conversão para o tipo data.\n",
    "            - As colunas de datas que são processadas incluem: DATA_OPCAO_PELO_SIMPLES, DATA_EXCLUSAO_SIMPLES, DATA_EXCLUSAO_MEI e DATA_OPCAO_PELO_MEI.\n",
    "        \"\"\"\n",
    "        # Convertendo colunas de data\n",
    "        df = df.withColumn(\"DT_OPCAO_SIMPLES\", F.to_date(col('DATA_OPCAO_PELO_SIMPLES'), \"yyyyMMdd\")) \\\n",
    "               .withColumn(\"DT_EXCLUSAO_SIMPLES\", F.to_date(col('DATA_EXCLUSAO_SIMPLES'), \"yyyyMMdd\")) \\\n",
    "               .withColumn(\"DT_EXCLUSAO_MEI\", F.to_date(col('DATA_EXCLUSAO_MEI'), \"yyyyMMdd\")) \\\n",
    "               .withColumn(\"DT_OPCAO_MEI\", F.to_date(col('DATA_OPCAO_PELO_MEI'), \"yyyyMMdd\")) \\\n",
    "               .select('CNPJ_BASICO','OPÇAO_PELO_MEI','DT_OPCAO_MEI','DT_EXCLUSAO_MEI',\n",
    "                       'OPCAO_PELO_SIMPLES','DT_OPCAO_SIMPLES','DT_EXCLUSAO_SIMPLES')\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def process_mei(self, df, save_base_path=\"./output/nomes\", file_name=\"nomes.csv\"):\n",
    "        \"\"\"\n",
    "        Processa um DataFrame referente a MEIs, realiza joins com dados adicionais de naturezas jurídicas,\n",
    "        qualificações, e um conjunto de dados de nomes para extrair e categorizar o primeiro nome.\n",
    "\n",
    "        Parâmetros:\n",
    "            df (pyspark.sql.DataFrame): DataFrame inicial contendo dados sobre MEIs.\n",
    "            save_base_path (str, opcional): Caminho onde o arquivo com dados de nomes foi extraído. Padrão é './output/nomes'.\n",
    "            file_name (str, opcional): Nome do arquivo CSV contendo dados de nomes a ser lido. Padrão é 'nomes.csv'.\n",
    "\n",
    "        Retorna:\n",
    "            pyspark.sql.DataFrame: DataFrame processado após todas as transformações e joins.\n",
    "\n",
    "        Descrição:\n",
    "            1. Realiza join com DataFrames de 'naturezas' e 'qualificações'.\n",
    "            2. Extração e manipulação de dados de CPF.\n",
    "            3. Utiliza um dicionário para mapear e criar a coluna \"NM_PORTE\".\n",
    "            4. Criptografa a coluna de CPF.\n",
    "            5. Realiza filtragens baseado na probabilidade do nome ser um CPF válido.\n",
    "            6. Lê um conjunto de dados de nomes e realiza o explode na coluna 'alternative_names'.\n",
    "            7. Extrai o primeiro nome da coluna 'NOME_EMPRESA'.\n",
    "            8. Realiza o join com o conjunto de dados de nomes para categorizar o primeiro nome.\n",
    "            9. Retorna um DataFrame contendo informações relevantes após todas as transformações.\n",
    "        \"\"\"\n",
    "        # Leitura dos DataFrames auxiliares\n",
    "        df_nat = self.read_data(schema_name='naturezas')\n",
    "        df_qual = self.read_data(schema_name='qualificacoes')\n",
    "        \n",
    "        # Renomear coluna para facilitar a junção\n",
    "        df = df.withColumnRenamed(\"QUALIF_RESPONVAVEL\", \"COD_QUALIFICACAO\")\n",
    "        \n",
    "        # Realizar junções com os DataFrames auxiliares\n",
    "        df = df.join(broadcast(df_nat), \"COD_NAT_JURICA\", \"left\").drop(\"COD_NAT_JURICA\")\n",
    "        df = df.join(broadcast(df_qual), \"COD_QUALIFICACAO\", \"left\").drop(\"COD_QUALIFICACAO\")\n",
    "               \n",
    "        # Extração e tratamento da coluna CPF\n",
    "        df = df.withColumn(\"CPF\", regexp_replace(col(\"NOME_EMPRESA\"), \"[^0-9]\", \"\"))\n",
    "        df = df.withColumn(\"CPF\", when(col(\"CPF\") == \"\", None).otherwise(col(\"CPF\")))\n",
    "        df = df.withColumn('CPF_LEN', length('CPF'))\n",
    "        \n",
    "        # Mapeamento para Porte da Empresa\n",
    "        mapping_porte = {\n",
    "            0: 'NÃO INFORMADO',\n",
    "            1: 'MICRO EMPRESA',\n",
    "            3: 'EMPRESA DE PEQUENO PORTE',\n",
    "            5: 'DEMAIS',\n",
    "            8: 'BAIXADA'\n",
    "        }\n",
    "\n",
    "        # Criando a coluna NM_PORTE\n",
    "        df = df.withColumn(\"NM_PORTE\", \n",
    "                           when(col(\"PORTE\").isin(list(mapping_porte.keys())), col(\"PORTE\")).otherwise(None))\n",
    "        \n",
    "        for key, value in mapping_porte.items():\n",
    "            df = df.withColumn(\"NM_PORTE\", \n",
    "                               when(col(\"PORTE\") == key, value).otherwise(col(\"NM_PORTE\")))\n",
    "        \n",
    "        # Determinando a probabilidade de ser um CPF válido\n",
    "        df = df.withColumn(\"PROBABILIDADE_DE_SER_CPF\", \n",
    "                           when(col(\"CPF_LEN\") == 11, \"SIM\").otherwise(\"NAO\"))\n",
    "        \n",
    "        # Gerando uma chave secreta para criptografia\n",
    "        secret_key = secrets.token_urlsafe(24)\n",
    "        df = df.withColumn(\"CPF_CRIPTOGRAFADO\", \n",
    "                           expr(f\"base64(aes_encrypt(CPF, '{secret_key}', 'ECB', 'PKCS'))\")) \\\n",
    "               .drop(\"CPF\")\n",
    "        \n",
    "        # Filtrar df_processado baseado na coluna PROBABILIDADE_DE_SER_CPF\n",
    "        df_filter = df.filter(col('PROBABILIDADE_DE_SER_CPF') == 'SIM') \\\n",
    "                      .dropDuplicates(subset=['CPF_CRIPTOGRAFADO', 'NOME_EMPRESA'])\n",
    "        \n",
    "        # Caminho completo do arquivo de nomes\n",
    "        file_path = os.path.join(save_base_path, file_name)\n",
    "        \n",
    "        # Ler o arquivo CSV de nomes\n",
    "        df_csv = self.spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "        \n",
    "        # Explodir a coluna alternative_names para múltiplas linhas\n",
    "        df_csv = df_csv.withColumn(\"alternative_names\", explode(split(coalesce(col(\"alternative_names\"), col(\"first_name\")), \"\\\\|\")))\n",
    "        \n",
    "        # Selecionar as colunas desejadas\n",
    "        df_result = df_csv.select(\"alternative_names\", \"group_name\", \"ratio\", \"classification\") \\\n",
    "                          .dropDuplicates(subset=['alternative_names'])\n",
    "        \n",
    "        # Extrair o primeiro nome da coluna NOME_EMPRESA\n",
    "        df_filter = df_filter.withColumn(\"PRIMEIRO_NOME\", split(col(\"NOME_EMPRESA\"), \" \")[0])\n",
    "        \n",
    "        # Fazer o join entre df_filter e df_result\n",
    "        joined_df = df_filter.join(df_result, df_filter.PRIMEIRO_NOME == df_result.alternative_names, \"left\").dropDuplicates()\n",
    "        \n",
    "        # Selecionar as colunas finais\n",
    "        joined_df = joined_df.select(\n",
    "            'CNPJ', 'NOME_EMPRESA', 'CAP_SOCIAL', 'NM_PORTE', 'NAT_JURICA', \n",
    "            'ENTE_FEDERATIVO', 'NM_QUALIFICACAO', 'CPF_CRIPTOGRAFADO', 'CPF_LEN',\n",
    "            'PROBABILIDADE_DE_SER_CPF', 'PRIMEIRO_NOME',\n",
    "            col('group_name').alias('GRUPO_NOME'), \n",
    "            col('ratio').alias('PROBABILIDADE_CLASSIFICACAO'), \n",
    "            col('classification').alias('CLASSIFICACAO')\n",
    "        ).dropDuplicates()\n",
    "        \n",
    "        return joined_df\n",
    "\n",
    "    def process_socios(self, df, save_base_path=\"./output/nomes\", file_name=\"nomes.csv\"):\n",
    "        \"\"\"\n",
    "        Processa um DataFrame referente a sócios, realiza joins com dados adicionais de países, qualificações, \n",
    "        e um conjunto de dados de nomes para extrair e categorizar o primeiro nome.\n",
    "\n",
    "        Parâmetros:\n",
    "            df (pyspark.sql.DataFrame): DataFrame inicial contendo dados sobre sócios.\n",
    "            save_base_path (str, opcional): Caminho onde o arquivo com dados de nomes foi extraído. Padrão é './output/nomes'.\n",
    "            file_name (str, opcional): Nome do arquivo CSV contendo dados de nomes a ser lido. Padrão é 'nomes.csv'.\n",
    "\n",
    "        Retorna:\n",
    "            pyspark.sql.DataFrame: DataFrame processado após todas as transformações e joins.\n",
    "\n",
    "        Descrição:\n",
    "            1. Realiza join com DataFrames de 'países'.\n",
    "            2. Usa mapeamentos para criar colunas \"NM_FAIXA_ETARIA\" e \"NM_IDENTIFICADOR_SOCIO\".\n",
    "            3. Renomeia e realiza join com DataFrame de qualificações para obter descrições das qualificações.\n",
    "            4. Converte coluna de data \"DATA_ENTRADA_SOCIEDADE\" para o formato desejado.\n",
    "            5. Lê um conjunto de dados de nomes e realiza o explode na coluna 'alternative_names'.\n",
    "            6. Extrai o primeiro nome da coluna 'NOME_SOCIO_RAZAO_SOCIAL'.\n",
    "            7. Realiza o join entre o DataFrame processado e o conjunto de dados de nomes para categorizar o primeiro nome.\n",
    "            8. Retorna um DataFrame contendo informações relevantes após todas as transformações.\n",
    "        \"\"\"\n",
    "        # Leitura dos DataFrames auxiliares\n",
    "        df_pais = self.read_data(schema_name='paises')\n",
    "        df = df.withColumnRenamed(\"PAIS\", \"COD_PAIS\")\n",
    "        df = df.join(broadcast(df_pais), \"COD_PAIS\", \"left\").drop(\"COD_PAIS\")\n",
    "        \n",
    "        # Mapeamento de códigos para faixas etárias.\n",
    "        mapping_faixa_etaria = {\n",
    "            1: '0 a 12 anos',\n",
    "            2: '13 a 20 anos',\n",
    "            3: '21 a 30 anos',\n",
    "            4: '31 a 40 anos',\n",
    "            5: '41 a 50 anos',\n",
    "            6: '51 a 60 anos',\n",
    "            7: '61 a 70 anos',\n",
    "            8: '71 a 80 anos',\n",
    "            9: 'maiores de 80 anos',\n",
    "            0: 'NA'\n",
    "        }\n",
    "        \n",
    "        # Mapeamento de códigos para identificador de sócio.\n",
    "        mapping_id_socio = {\n",
    "            1: 'PESSOA JURIDICA',\n",
    "            2: 'PESSOA FISICA',\n",
    "            3: 'ESTRANGEIRO'\n",
    "        }\n",
    "        \n",
    "        # Leitura do DataFrame de qualificações\n",
    "        df_qual = self.read_data(schema_name='qualificacoes')\n",
    "        \n",
    "        # Criando a coluna NM_FAIXA_ETARIA\n",
    "        df = df.withColumn(\"NM_FAIXA_ETARIA\", \n",
    "                           when(col(\"FAIXA_ETARIA\").isin(list(mapping_faixa_etaria.keys())), col(\"FAIXA_ETARIA\")).otherwise(None))\n",
    "        \n",
    "        for key, value in mapping_faixa_etaria.items():\n",
    "            df = df.withColumn(\"NM_FAIXA_ETARIA\", \n",
    "                               when(col(\"FAIXA_ETARIA\") == key, value).otherwise(col(\"NM_FAIXA_ETARIA\")))\n",
    "        \n",
    "        # Criando a coluna NM_IDENTIFICADOR_SOCIO\n",
    "        df = df.withColumn(\"NM_IDENTIFICADOR_SOCIO\", \n",
    "                           when(col(\"IDENTIFICADOR_SOCIO\").isin(list(mapping_id_socio.keys())), col(\"IDENTIFICADOR_SOCIO\")).otherwise(None))\n",
    "        \n",
    "        for key, value in mapping_id_socio.items():\n",
    "            df = df.withColumn(\"NM_IDENTIFICADOR_SOCIO\", \n",
    "                               when(col(\"IDENTIFICADOR_SOCIO\") == key, value).otherwise(col(\"NM_IDENTIFICADOR_SOCIO\")))\n",
    "        \n",
    "        # Renomear e juntar com qualificações para obter descrições\n",
    "        df = df.withColumnRenamed(\"QUALIFICACAO_REPRESENTANTE_LEGAL\", \"COD_QUALIFICACAO\")\n",
    "        df = df.join(broadcast(df_qual), \"COD_QUALIFICACAO\", \"left\").drop(\"COD_QUALIFICACAO\") \\\n",
    "               .withColumnRenamed(\"NM_QUALIFICACAO\", \"NM_QUALIFICACAO_REPRESENTANTE_LEGAL\")\n",
    "        \n",
    "        df = df.withColumnRenamed(\"QUALIFICAÇAO_SOCIO\", \"COD_QUALIFICACAO\")\n",
    "        df = df.join(broadcast(df_qual), \"COD_QUALIFICACAO\", \"left\").drop(\"COD_QUALIFICACAO\") \\\n",
    "               .withColumnRenamed(\"NM_QUALIFICACAO\", \"NM_QUALIFICAÇAO_SOCIO\")\n",
    "        \n",
    "        # Conversão da coluna de data\n",
    "        df = df.withColumn(\"DT_ENTRADA_SOCIEDADE\", F.to_date(col('DATA_ENTRADA_SOCIEDADE'), \"yyyyMMdd\")) \\\n",
    "               .drop(\"DATA_ENTRADA_SOCIEDADE\")\n",
    "        \n",
    "        # Leitura do arquivo CSV de nomes\n",
    "        file_path = os.path.join(save_base_path, file_name)\n",
    "        df_csv = self.spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "        \n",
    "        # Explodir a coluna alternative_names para múltiplas linhas\n",
    "        df_csv = df_csv.withColumn(\"alternative_name2\", explode(split(col(\"alternative_names\"), \"\\|\")))\n",
    "        \n",
    "        # Selecionar as colunas desejadas\n",
    "        df_result = df_csv.select(\"alternative_name2\", \"group_name\", \"ratio\", \"classification\") \\\n",
    "                          .dropDuplicates([\"alternative_name2\"])\n",
    "        \n",
    "        # Extrair o primeiro nome da coluna NOME_SOCIO_RAZAO_SOCIAL\n",
    "        df = df.withColumn(\"PRIMEIRO_NOME\", split(col(\"NOME_SOCIO_RAZAO_SOCIAL\"), \" \")[0])\n",
    "        \n",
    "        # Fazer o join entre df_filter e df_result\n",
    "        joined_df = df.join(df_result, df.PRIMEIRO_NOME == df_result.alternative_name2, \"left\").dropDuplicates()\n",
    "        \n",
    "        # Selecionar as colunas finais\n",
    "        joined_df = joined_df.select(\n",
    "            'CNPJ_BASICO', 'NOME_SOCIO_RAZAO_SOCIAL', 'CNPJ_CPF_SOCIO', 'REPRESENTANTE_LEGAL',\n",
    "            'NOME_REPRESENTANTE', 'NM_PAIS', 'NM_FAIXA_ETARIA', 'NM_IDENTIFICADOR_SOCIO',\n",
    "            'NM_QUALIFICACAO_REPRESENTANTE_LEGAL', 'NM_QUALIFICAÇAO_SOCIO', 'DT_ENTRADA_SOCIEDADE',\n",
    "            'PRIMEIRO_NOME', \n",
    "            col('ratio').alias('PROBABILIDADE_CLASSIFICACAO'),\n",
    "            col('classification').alias('CLASSIFICACAO')\n",
    "        ).dropDuplicates()\n",
    "        \n",
    "        return joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f2352f5-939e-461e-bfb3-55c5019359ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baixar o arquivo de nomes para realizar tratamento de sexo nas tabelas MEI e Socios\n",
    "#receitaLT_processor = ReceitaLT(spark)\n",
    "#receitaLT_processor.download_nomes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74711e59-d4c7-4076-8559-069f3388c4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Detected encodings: {'./output\\\\Estabelecimentos\\\\Estabelecimentos.csv': 'ascii', './output\\\\Estabelecimentos\\\\Estabelecimentos0.csv': 'ascii', './output\\\\Estabelecimentos\\\\Estabelecimentos1.csv': 'ascii', './output\\\\Estabelecimentos\\\\Estabelecimentos2.csv': 'ascii', './output\\\\Estabelecimentos\\\\Estabelecimentos3.csv': 'ascii', './output\\\\Estabelecimentos\\\\Estabelecimentos4.csv': 'ISO-8859-1', './output\\\\Estabelecimentos\\\\Estabelecimentos5.csv': 'ascii', './output\\\\Estabelecimentos\\\\Estabelecimentos6.csv': 'ascii', './output\\\\Estabelecimentos\\\\Estabelecimentos7.csv': 'ascii', './output\\\\Estabelecimentos\\\\Estabelecimentos8.csv': 'ISO-8859-1', './output\\\\Estabelecimentos\\\\Estabelecimentos9.csv': 'ISO-8859-1'}\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Estabelecimentos\\Estabelecimentos.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Estabelecimentos\\Estabelecimentos0.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Estabelecimentos\\Estabelecimentos1.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Estabelecimentos\\Estabelecimentos2.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Estabelecimentos\\Estabelecimentos3.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Estabelecimentos\\Estabelecimentos4.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Estabelecimentos\\Estabelecimentos5.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Estabelecimentos\\Estabelecimentos6.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Estabelecimentos\\Estabelecimentos7.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Estabelecimentos\\Estabelecimentos8.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Estabelecimentos\\Estabelecimentos9.csv\n",
      "INFO:__main__:Detected encodings: {'./output\\\\Paises\\\\Paises.csv': 'ISO-8859-1'}\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Paises\\Paises.csv\n",
      "INFO:__main__:Detected encodings: {'./output\\\\Municipios\\\\Municipios.csv': 'ascii'}\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Municipios\\Municipios.csv\n",
      "INFO:__main__:Detected encodings: {'./output\\\\Cnaes\\\\Cnaes.csv': 'ISO-8859-1'}\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Cnaes\\Cnaes.csv\n",
      "INFO:__main__:Detected encodings: {'./output\\\\Motivos\\\\Motivos.csv': 'ascii'}\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Motivos\\Motivos.csv\n"
     ]
    }
   ],
   "source": [
    "receitaLT_processor = ReceitaLT(spark)\n",
    "df = receitaLT_processor.read_data(schema_name='estabelecimentos')\n",
    "df_estabelecimentos = receitaLT_processor.process_estabelecimentos(df)\n",
    "df_estabelecimentos = df_estabelecimentos.select('CNPJ_BASICO','CNPJ_ORDEM', 'NM_MATRIZ_FILIAL', 'NM_SIT_CADASTRAL', 'DT_SIT_CADASTRAL',\n",
    "    'NOME_FANTASIA', 'DT_INICIO_ATIVIDADE', 'ENDERECO_COMPLETO', 'TIPO_LOUGRADOURO', 'CEP', 'UF', 'COORDENADAS', 'MUNICIPIO',\n",
    "    'DDD1','TEL1', 'DDD2','TEL2', 'VALID_EMAIL', 'PROVEDOR', 'NM_MOTIVO', 'CNAE', 'NM_PAIS', 'ano_cadastro', 'mes_cadastro', 'ano_sit_cadastral','mes_sit_cadastral').cache()\n",
    "df_estabelecimentos.repartition(25).write.mode(\"overwrite\").parquet('C:/Users/pedro/Documents/Curso de pos graduação de EST/DADOS_CNPJ/df_estab/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8343e6d3-acb7-4060-a970-9b2bc679f2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Detected encodings: {'./output\\\\Estabelecimentos\\\\Estabelecimentos.csv': 'ascii', './output\\\\Estabelecimentos\\\\Estabelecimentos0.csv': 'ascii', './output\\\\Estabelecimentos\\\\Estabelecimentos1.csv': 'ascii', './output\\\\Estabelecimentos\\\\Estabelecimentos2.csv': 'ascii', './output\\\\Estabelecimentos\\\\Estabelecimentos3.csv': 'ascii', './output\\\\Estabelecimentos\\\\Estabelecimentos4.csv': 'ISO-8859-1', './output\\\\Estabelecimentos\\\\Estabelecimentos5.csv': 'ascii', './output\\\\Estabelecimentos\\\\Estabelecimentos6.csv': 'ascii', './output\\\\Estabelecimentos\\\\Estabelecimentos7.csv': 'ascii', './output\\\\Estabelecimentos\\\\Estabelecimentos8.csv': 'ISO-8859-1', './output\\\\Estabelecimentos\\\\Estabelecimentos9.csv': 'ISO-8859-1'}\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Estabelecimentos\\Estabelecimentos.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Estabelecimentos\\Estabelecimentos0.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Estabelecimentos\\Estabelecimentos1.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Estabelecimentos\\Estabelecimentos2.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Estabelecimentos\\Estabelecimentos3.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Estabelecimentos\\Estabelecimentos4.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Estabelecimentos\\Estabelecimentos5.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Estabelecimentos\\Estabelecimentos6.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Estabelecimentos\\Estabelecimentos7.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Estabelecimentos\\Estabelecimentos8.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Estabelecimentos\\Estabelecimentos9.csv\n",
      "INFO:__main__:Detected encodings: {'./output\\\\Paises\\\\Paises.csv': 'ISO-8859-1'}\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Paises\\Paises.csv\n",
      "INFO:__main__:Detected encodings: {'./output\\\\Municipios\\\\Municipios.csv': 'ascii'}\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Municipios\\Municipios.csv\n",
      "INFO:__main__:Detected encodings: {'./output\\\\Cnaes\\\\Cnaes.csv': 'ISO-8859-1'}\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Cnaes\\Cnaes.csv\n",
      "INFO:__main__:Detected encodings: {'./output\\\\Motivos\\\\Motivos.csv': 'ascii'}\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Motivos\\Motivos.csv\n"
     ]
    }
   ],
   "source": [
    "receitaLT_processor = ReceitaLT(spark)\n",
    "df = receitaLT_processor.read_data(schema_name='estabelecimentos')\n",
    "df_estabelecimentos = receitaLT_processor.process_estabelecimentos(df)\n",
    "df_estabelecimentos = df_estabelecimentos.select('CNPJ_BASICO','CNPJ_ORDEM', 'NM_MATRIZ_FILIAL', 'NM_SIT_CADASTRAL', 'DT_SIT_CADASTRAL',\n",
    "    'NOME_FANTASIA', 'DT_INICIO_ATIVIDADE', 'ENDERECO_COMPLETO', 'TIPO_LOUGRADOURO', 'CEP', 'UF', 'COORDENADAS', 'MUNICIPIO',\n",
    "    'DDD1','TEL1', 'DDD2','TEL2', 'VALID_EMAIL', 'PROVEDOR', 'NM_MOTIVO', 'CNAE', 'NM_PAIS', 'ano_cadastro', 'mes_cadastro', 'ano_sit_cadastral','mes_sit_cadastral').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16594524-8b59-44e9-9e9d-bdc392013bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estabelecimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac8721b0-5107-4997-8c62-2ce7f997a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estabelecimentos = df_estabelecimentos.select('CNPJ_BASICO','CNPJ_ORDEM', 'NM_MATRIZ_FILIAL', 'NM_SIT_CADASTRAL', 'DT_SIT_CADASTRAL',\n",
    "    'NOME_FANTASIA', 'DT_INICIO_ATIVIDADE', 'ENDERECO_COMPLETO', 'TIPO_LOUGRADOURO', 'CEP', 'UF', 'COORDENADAS', 'MUNICIPIO',\n",
    "    'DDD1','TEL1', 'DDD2','TEL2', 'VALID_EMAIL', 'PROVEDOR', 'NM_MOTIVO', 'CNAE', 'NM_PAIS', 'ano_cadastro', 'mes_cadastro', 'ano_sit_cadastral','mes_sit_cadastral').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c1a338-7169-4d71-8295-3c5aaf2dfa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estabelecimentos.repartition(25).write.mode(\"overwrite\").parquet('C:/Users/pedro/Documents/Curso de pos graduação de EST/DADOS_CNPJ/df_estab/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4c2d4f-4e51-4337-93ae-2ae84c0cc301",
   "metadata": {},
   "outputs": [],
   "source": [
    "Documents/Curso de pos graduação de EST/DADOS_CNPJ/df_estab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85a79c6b-a29c-4bf4-b6cb-44f22fd7b11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Detected encodings: {'./output\\\\Empresas\\\\Empresas0.csv': 'ascii', './output\\\\Empresas\\\\Empresas1.csv': 'ascii', './output\\\\Empresas\\\\Empresas2.csv': 'ascii', './output\\\\Empresas\\\\Empresas3.csv': 'ascii', './output\\\\Empresas\\\\Empresas4.csv': 'ascii', './output\\\\Empresas\\\\Empresas5.csv': 'ascii', './output\\\\Empresas\\\\Empresas6.csv': 'ascii', './output\\\\Empresas\\\\Empresas7.csv': 'ascii', './output\\\\Empresas\\\\Empresas8.csv': 'ascii', './output\\\\Empresas\\\\Empresas9.csv': 'ascii'}\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Empresas\\Empresas0.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Empresas\\Empresas1.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Empresas\\Empresas2.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Empresas\\Empresas3.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Empresas\\Empresas4.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Empresas\\Empresas5.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Empresas\\Empresas6.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Empresas\\Empresas7.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Empresas\\Empresas8.csv\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Empresas\\Empresas9.csv\n",
      "INFO:__main__:Detected encodings: {'./output\\\\Naturezas\\\\Naturezas.csv': 'ISO-8859-1'}\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Naturezas\\Naturezas.csv\n",
      "INFO:__main__:Detected encodings: {'./output\\\\Qualificacoes\\\\Qualificacoes.csv': 'ISO-8859-1'}\n",
      "INFO:__main__:Arquivo lido com sucesso: ./output\\Qualificacoes\\Qualificacoes.csv\n"
     ]
    }
   ],
   "source": [
    "receitaLT_processor = ReceitaLT(spark) \n",
    "dfmei = receitaLT_processor.read_data(schema_name='empresas')\n",
    "df_mei = receitaLT_processor.process_mei(dfmei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74da1228-1f7f-48de-aa74-fcd34d11cd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mei = df_mei.withColumn('CLASSIFICACAO', when((df_mei.PRIMEIRO_NOME == 'JOSE') & \n",
    "                                         (df_mei.CLASSIFICACAO == 'F'), 'M').otherwise(df_mei.CLASSIFICACAO))\n",
    "df_mei = df_mei.select('CNPJ','NOME_EMPRESA','CAP_SOCIAL', 'NAT_JURICA', 'NM_QUALIFICACAO', 'CLASSIFICACAO', 'PROBABILIDADE_CLASSIFICACAO', ).cache()\n",
    "df_mei.repartition(20).write.mode(\"overwrite\").parquet('C:/Users/pedro/Documents/Curso de pos graduação de EST/DADOS_CNPJ/df_mei/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bf5bb4c-e6d6-4949-ba90-ed120493a676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>CNPJ</th><th>NOME_EMPRESA</th><th>CAP_SOCIAL</th><th>NM_PORTE</th><th>NAT_JURICA</th><th>ENTE_FEDERATIVO</th><th>NM_QUALIFICACAO</th><th>CPF_CRIPTOGRAFADO</th><th>CPF_LEN</th><th>PROBABILIDADE_DE_SER_CPF</th><th>PRIMEIRO_NOME</th><th>GRUPO_NOME</th><th>PROBABILIDADE_CLASSIFICACAO</th><th>CLASSIFICACAO</th></tr>\n",
       "<tr><td>34848565</td><td>KARINA BELARMINO ...</td><td>1000,00</td><td>MICRO EMPRESA</td><td>Empres&aacute;rio (Indiv...</td><td>NULL</td><td>Empres&aacute;rio</td><td>+++I1S0KUamFe+pHK...</td><td>11</td><td>SIM</td><td>KARINA</td><td>KARINA</td><td>0.9961922744079931</td><td>F</td></tr>\n",
       "<tr><td>28191842</td><td>ROSEMEIRE PERES R...</td><td>4000,00</td><td>MICRO EMPRESA</td><td>Empres&aacute;rio (Indiv...</td><td>NULL</td><td>Empres&aacute;rio</td><td>+++OOoMxkfHjcQ8Gm...</td><td>11</td><td>SIM</td><td>ROSEMEIRE</td><td>ROSEMEIRE</td><td>1.0</td><td>F</td></tr>\n",
       "<tr><td>45631914</td><td>ARTUR LUIS PEREIR...</td><td>300,00</td><td>MICRO EMPRESA</td><td>Empres&aacute;rio (Indiv...</td><td>NULL</td><td>Empres&aacute;rio</td><td>+++uvDpUzPWjKtub/...</td><td>11</td><td>SIM</td><td>ARTUR</td><td>ARTUR</td><td>0.988292948543425</td><td>M</td></tr>\n",
       "<tr><td>15595525</td><td>GILMAR COSTA DA S...</td><td>1,00</td><td>MICRO EMPRESA</td><td>Empres&aacute;rio (Indiv...</td><td>NULL</td><td>Empres&aacute;rio</td><td>++/hF69NvCTMiYLZz...</td><td>11</td><td>SIM</td><td>GILMAR</td><td>GILMAR</td><td>1.0</td><td>F</td></tr>\n",
       "<tr><td>43695636</td><td>GABRIELA MARTINS ...</td><td>5000,00</td><td>MICRO EMPRESA</td><td>Empres&aacute;rio (Indiv...</td><td>NULL</td><td>Empres&aacute;rio</td><td>++/y1+lNL/wqrRn56...</td><td>11</td><td>SIM</td><td>GABRIELA</td><td>GABRIELA</td><td>1.0</td><td>F</td></tr>\n",
       "<tr><td>12279870</td><td>MARIA APPARECIDA ...</td><td>1,00</td><td>MICRO EMPRESA</td><td>Empres&aacute;rio (Indiv...</td><td>NULL</td><td>Empres&aacute;rio</td><td>++09S+KdUALSSk61I...</td><td>11</td><td>SIM</td><td>MARIA</td><td>MARIA</td><td>1.0</td><td>F</td></tr>\n",
       "<tr><td>17158845</td><td>FERNANDA DE SOUZA...</td><td>5000,00</td><td>MICRO EMPRESA</td><td>Empres&aacute;rio (Indiv...</td><td>NULL</td><td>Empres&aacute;rio</td><td>++18KWzBzrMCR94pA...</td><td>11</td><td>SIM</td><td>FERNANDA</td><td>FERNANDA</td><td>1.0</td><td>F</td></tr>\n",
       "<tr><td>19872585</td><td>KELY ACOSTA ARIM ...</td><td>100,00</td><td>MICRO EMPRESA</td><td>Empres&aacute;rio (Indiv...</td><td>NULL</td><td>Empres&aacute;rio</td><td>++1IiPpha6RhYEkfb...</td><td>11</td><td>SIM</td><td>KELY</td><td>KELI</td><td>0.5982142857142857</td><td>F</td></tr>\n",
       "<tr><td>12071862</td><td>LUIZ ALBERTO OLIV...</td><td>1,00</td><td>MICRO EMPRESA</td><td>Empres&aacute;rio (Indiv...</td><td>NULL</td><td>Empres&aacute;rio</td><td>++29WNrhM2ZfWI1Vg...</td><td>11</td><td>SIM</td><td>LUIZ</td><td>LUIZ</td><td>1.0</td><td>M</td></tr>\n",
       "<tr><td>42768909</td><td>WAYANY COUTINHO B...</td><td>5000,00</td><td>MICRO EMPRESA</td><td>Empres&aacute;rio (Indiv...</td><td>NULL</td><td>Empres&aacute;rio</td><td>++2xDlWYsZrRQPH2v...</td><td>11</td><td>SIM</td><td>WAYANY</td><td>NULL</td><td>NULL</td><td>NULL</td></tr>\n",
       "</table>\n",
       "only showing top 10 rows\n"
      ],
      "text/plain": [
       "+--------+--------------------+----------+-------------+--------------------+---------------+---------------+--------------------+-------+------------------------+-------------+----------+---------------------------+-------------+\n",
       "|    CNPJ|        NOME_EMPRESA|CAP_SOCIAL|     NM_PORTE|          NAT_JURICA|ENTE_FEDERATIVO|NM_QUALIFICACAO|   CPF_CRIPTOGRAFADO|CPF_LEN|PROBABILIDADE_DE_SER_CPF|PRIMEIRO_NOME|GRUPO_NOME|PROBABILIDADE_CLASSIFICACAO|CLASSIFICACAO|\n",
       "+--------+--------------------+----------+-------------+--------------------+---------------+---------------+--------------------+-------+------------------------+-------------+----------+---------------------------+-------------+\n",
       "|34848565|KARINA BELARMINO ...|   1000,00|MICRO EMPRESA|Empresário (Indiv...|           NULL|     Empresário|+++I1S0KUamFe+pHK...|     11|                     SIM|       KARINA|    KARINA|         0.9961922744079931|            F|\n",
       "|28191842|ROSEMEIRE PERES R...|   4000,00|MICRO EMPRESA|Empresário (Indiv...|           NULL|     Empresário|+++OOoMxkfHjcQ8Gm...|     11|                     SIM|    ROSEMEIRE| ROSEMEIRE|                        1.0|            F|\n",
       "|45631914|ARTUR LUIS PEREIR...|    300,00|MICRO EMPRESA|Empresário (Indiv...|           NULL|     Empresário|+++uvDpUzPWjKtub/...|     11|                     SIM|        ARTUR|     ARTUR|          0.988292948543425|            M|\n",
       "|15595525|GILMAR COSTA DA S...|      1,00|MICRO EMPRESA|Empresário (Indiv...|           NULL|     Empresário|++/hF69NvCTMiYLZz...|     11|                     SIM|       GILMAR|    GILMAR|                        1.0|            F|\n",
       "|43695636|GABRIELA MARTINS ...|   5000,00|MICRO EMPRESA|Empresário (Indiv...|           NULL|     Empresário|++/y1+lNL/wqrRn56...|     11|                     SIM|     GABRIELA|  GABRIELA|                        1.0|            F|\n",
       "|12279870|MARIA APPARECIDA ...|      1,00|MICRO EMPRESA|Empresário (Indiv...|           NULL|     Empresário|++09S+KdUALSSk61I...|     11|                     SIM|        MARIA|     MARIA|                        1.0|            F|\n",
       "|17158845|FERNANDA DE SOUZA...|   5000,00|MICRO EMPRESA|Empresário (Indiv...|           NULL|     Empresário|++18KWzBzrMCR94pA...|     11|                     SIM|     FERNANDA|  FERNANDA|                        1.0|            F|\n",
       "|19872585|KELY ACOSTA ARIM ...|    100,00|MICRO EMPRESA|Empresário (Indiv...|           NULL|     Empresário|++1IiPpha6RhYEkfb...|     11|                     SIM|         KELY|      KELI|         0.5982142857142857|            F|\n",
       "|12071862|LUIZ ALBERTO OLIV...|      1,00|MICRO EMPRESA|Empresário (Indiv...|           NULL|     Empresário|++29WNrhM2ZfWI1Vg...|     11|                     SIM|         LUIZ|      LUIZ|                        1.0|            M|\n",
       "|42768909|WAYANY COUTINHO B...|   5000,00|MICRO EMPRESA|Empresário (Indiv...|           NULL|     Empresário|++2xDlWYsZrRQPH2v...|     11|                     SIM|       WAYANY|      NULL|                       NULL|         NULL|\n",
       "+--------+--------------------+----------+-------------+--------------------+---------------+---------------+--------------------+-------+------------------------+-------------+----------+---------------------------+-------------+\n",
       "only showing top 10 rows"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd2b0de6-bd0b-4eaa-97a6-9d0a9de2bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_mei.join(df_estabelecimentos, df_mei.CNPJ==df_estabelecimentos.CNPJ_BASICO, 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dc5fd2-f8ec-43ee-9503-9fce598cc4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632f9df4-6a8c-42b0-8071-3f0659b00137",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab4c31d6-4889-4d72-9b5e-5f483ddb39d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def detect_encoding(file_pattern_or_path, num_bytes=10000):\n",
    "        \"\"\"\n",
    "        Detecta a codificação do arquivo ou arquivos fornecidos.\n",
    "        \n",
    "        Parâmetros:\n",
    "            file_pattern_or_path (str): Caminho ou padrão do arquivo para detecção.\n",
    "            num_bytes (int, opcional): Número de bytes para ler para a detecção. Padrão é 10000.\n",
    "        \n",
    "        Retorna:\n",
    "            dict: Dicionário com caminho do arquivo como chave e codificação detectada como valor.\n",
    "        \"\"\"\n",
    "        files = glob.glob(file_pattern_or_path)\n",
    "        encodings = {}\n",
    "        for file_path in files:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                rawdata = f.read(num_bytes)\n",
    "                encodings[file_path] = chardet.detect(rawdata)[\"encoding\"]\n",
    "        return encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d01125-1631-4554-8a52-905e28f7a63a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5160ad-90d1-4fc3-80d6-70200ca8dff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "receitaLT_processor = ReceitaLT(spark)\n",
    "df = receitaLT_processor.read_data(schema_name='socios')\n",
    "df_socios = receitaLT_processor.process_socios(df).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec66f6a-d738-489a-994b-981afd2cbe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_socios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea57b3d4-03c8-44ab-a92b-884074d8d160",
   "metadata": {},
   "outputs": [],
   "source": [
    "receitaLT_processor = ReceitaLT(spark)\n",
    "df = receitaLT_processor.read_data(schema_name='simples')\n",
    "df_simples = receitaLT_processor.process_simples(df).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed9852e-a0d3-40de-a529-d2483507ce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9853dcbd-9dae-4fa7-9323-a61998c533ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "receitaLT_processor = ReceitaLT(spark) \n",
    "df = receitaLT_processor.read_data(schema_name='empresas')\n",
    "df_empresas = receitaLT_processor.process_empresas(df).cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
