{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55495f14-9c5c-4b0e-bc0c-812a3bc982ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "print(os.environ.get(\"SPARK_HOME\"))\n",
    "print(os.environ.get(\"HADOOP_HOME\"))\n",
    "print(os.environ.get(\"JAVA_HOME\"))\n",
    "os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a07de3-a739-4a94-9a5d-e5461ef7ae58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from ReceitaCNPJApi import ReceitaCNPJApi\n",
    "#from ReceitaLT import ReceitaLT\n",
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d2c3fe-bae3-44ae-bfc7-15c0903cb601",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28308144-73f0-4527-83f0-ce005cd169fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "    .master(\"local[*]\")  \n",
    "    .config(\"spark.driver.cores\", \"3\") \n",
    "    .config(\"spark.driver.memory\", \"15g\") \n",
    "    .config(\"spark.default.parallelism\", \"50\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"50\") \n",
    "    .config(\"spark.executor.cores\", \"2\")  \n",
    "    .config(\"spark.executor.instances\", \"4\") \n",
    "    .config(\"spark.executor.memory\", \"6g\") \n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \n",
    "    .config(\"spark.memory.storageFraction\", \"0.5\")\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\") \n",
    "    .config(\"spark.memory.offHeap.size\", \"3g\")  \n",
    "    .config(\"spark.driver.maxResultSize\", \"10g\")\n",
    "    .config(\"spark.executor.memoryOverhead\", \"2g\")  \n",
    "    .config('spark.sql.repl.eagerEval.enabled', True)\n",
    "    .config(\"spark.sql.repl.eagerEval.maxNumRows\", 10)\n",
    "    .appName('dataset_cnpj')\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b7c5fc-888f-4bd0-8290-a6f6be8e3b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "    .master(\"local[*]\")  \n",
    "    .config(\"spark.driver.cores\", \"3\") \n",
    "    .config(\"spark.driver.memory\", \"15g\") \n",
    "    .config(\"spark.default.parallelism\", \"20\") \n",
    "    .config(\"spark.executor.cores\", \"2\")  \n",
    "    .config(\"spark.executor.instances\", \"3\") \n",
    "    .config(\"spark.executor.memory\", \"5g\") \n",
    "    .config(\"spark.memory.fraction\", \"0.8\") \n",
    "    .config(\"spark.memory.storageFraction\", \"0.5\")\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\") \n",
    "    .config(\"spark.memory.offHeap.size\", \"2g\")  \n",
    "    .config(\"spark.executor.memoryOverhead\", \"1g\")  \n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\")\n",
    "    .config('spark.sql.repl.eagerEval.enabled', True)\n",
    "    .config(\"spark.sql.repl.eagerEval.maxNumRows\", 10)\n",
    "    .appName('dataset_cnpj')\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c2fd3e-2cef-4155-972c-847d1879e0a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07efcfff-1da8-468b-b441-65d8a5840636",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import os\n",
    "import chardet\n",
    "import logging\n",
    "import glob\n",
    "import secrets\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import regexp_replace, when,length,to_date,upper,lower,col,split,explode,coalesce,concat_ws,concat,lit,broadcast,regexp_extract,month,year,to_date\n",
    "from pyspark.sql.functions import broadcast,expr,udf\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "   \n",
    "def geocode_address(address):\n",
    "    \"\"\"\n",
    "    Geocodifica um endereço, convertendo-o em coordenadas de latitude e longitude.\n",
    "\n",
    "    Parâmetros:\n",
    "        address (str): Endereço a ser geocodificado.\n",
    "\n",
    "    Retorna:\n",
    "        tuple: Um par contendo a latitude e a longitude do endereço fornecido. \n",
    "                Se o endereço não puder ser geocodificado, retorna (None, None).\n",
    "\n",
    "    Exemplo:\n",
    "        lat, lon = geocode_address(\"1600 Amphitheatre Parkway, Mountain View, CA\")\n",
    "\n",
    "    Notas:\n",
    "        - Usa o serviço Nominatim para a geocodificação.\n",
    "        - Incorpora um limitador de taxa para garantir que não excedamos os limites de requisições por segundo \n",
    "            do serviço.\n",
    "    \"\"\"\n",
    "    geolocator = Nominatim(user_agent=\"CNPJ_GEOLOCATION\")\n",
    "    geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1)\n",
    "    location = geocode(address)\n",
    "    if location:\n",
    "        return (location.latitude, location.longitude)\n",
    "    else:\n",
    "        return (None, None)\n",
    "\n",
    "\n",
    "class ReceitaLT:\n",
    "    \"\"\"\n",
    "    A classe `ReceitaLT` facilita a manipulação e análise de dados da Receita Federal do Brasil.\n",
    "\n",
    "    Atributos:\n",
    "        spark (SparkSession): Sessão Spark para manipulação de dataframes.\n",
    "        logger (Logger): Logger para capturar e exibir logs.\n",
    "        \n",
    "    Atributos estáticos:\n",
    "        - estabelecimentos: Schema para dados de estabelecimentos.\n",
    "        - empresas: Schema para dados das empresas.\n",
    "        - municipios: Schema para municípios.\n",
    "        - cnaes: Schema para CNAEs.\n",
    "        - paises: Schema para países.\n",
    "        - qualificacoes: Schema para qualificações.\n",
    "        - socios: Schema para sócios.\n",
    "        - simples: Schema para opções do Simples Nacional.\n",
    "        - naturezas: Schema para naturezas jurídicas.\n",
    "        - motivos: Schema para motivos de situações cadastrais.\n",
    "        - dic_provedor: Dicionário para correção de nomes de provedores de email.\n",
    "        \n",
    "    Métodos:\n",
    "        detect_encoding(file_pattern_or_path, num_bytes=10000): Detecta a codificação do arquivo ou arquivos fornecidos.\n",
    "\n",
    "    Uso:\n",
    "        1. Instancie a classe com uma sessão Spark.\n",
    "        2. Utilize os schemas estáticos para leitura de arquivos.\n",
    "        3. Use o método `detect_encoding` para determinar a codificação de arquivos antes de lê-los.\n",
    "        \n",
    "    Exemplo:\n",
    "        from pyspark.sql import SparkSession\n",
    "        \n",
    "        spark_session = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "        receita_helper = ReceitaLT(spark_session)\n",
    "        encodings = receita_helper.detect_encoding(\"path/to/datafile.csv\")\n",
    "        df = spark_session.read.csv(\"path/to/datafile.csv\", schema=ReceitaLT.empresas, encoding=encodings[\"path/to/datafile.csv\"])\n",
    "    \"\"\"\n",
    "    def __init__(self, spark: SparkSession):\n",
    "        \"\"\"\n",
    "        Inicializa a classe ReceitaLT.\n",
    "        \n",
    "        Parâmetros:\n",
    "        spark (SparkSession): Uma sessão Spark ativa.\n",
    "        \"\"\"\n",
    "        self.spark = spark\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        logging.basicConfig(level=logging.INFO) \n",
    "        \n",
    "    # Definindo os schemas:\n",
    "    estabelecimentos = StructType([\n",
    "        StructField(\"CNPJ_BASICO\", StringType(), nullable=True),\n",
    "        StructField(\"CNPJ_ORDEM\", StringType(), nullable=True),\n",
    "        StructField(\"CNPJ_DV\", StringType(), nullable=True),\n",
    "        StructField(\"MATRIZ_FILIAL\", StringType(), nullable=True),\n",
    "        StructField(\"NOME_FANTASIA\", StringType(), nullable=True),\n",
    "        StructField(\"SIT_CADASTRAL\", IntegerType(), nullable=True),\n",
    "        StructField(\"DT_SIT_CADASTRAL\", StringType(), nullable=True),\n",
    "        StructField(\"MOTIVO_CADASTRAL\", StringType(), nullable=True),\n",
    "        StructField(\"NOME_CIDADE_EXTERIOR\", StringType(), nullable=True),\n",
    "        StructField(\"PAIS\", StringType(), nullable=True),\n",
    "        StructField(\"DT_INICIO_ATIVIDADE\", StringType(), nullable=True),\n",
    "        StructField(\"CNAE_1\", StringType(), nullable=True),\n",
    "        StructField(\"CNAE_2\", StringType(), nullable=True),\n",
    "        StructField(\"TIPO_LOUGRADOURO\", StringType(), nullable=True),\n",
    "        StructField(\"LOGRADOURO\", StringType(), nullable=True),\n",
    "        StructField(\"NUMERO\", IntegerType(), nullable=True),\n",
    "        StructField(\"COMPLEMENTO\", StringType(), nullable=True),\n",
    "        StructField(\"BAIRRO\", StringType(), nullable=True),\n",
    "        StructField(\"CEP\", IntegerType(), nullable=True),\n",
    "        StructField(\"UF\", StringType(), nullable=True),\n",
    "        StructField(\"MUNICIPIO\", StringType(), nullable=True),\n",
    "        StructField(\"DDD1\", StringType(), nullable=True),\n",
    "        StructField(\"TEL1\", StringType(), nullable=True),\n",
    "        StructField(\"DDD2\", StringType(), nullable=True),\n",
    "        StructField(\"TEL2\", StringType(), nullable=True),\n",
    "        StructField(\"DDD_FAX\", IntegerType(), nullable=True),\n",
    "        StructField(\"FAX\", IntegerType(), nullable=True),\n",
    "        StructField(\"EMAIL\", StringType(), nullable=True),\n",
    "        StructField(\"SIT_ESPECIAL\", StringType(), nullable=True),\n",
    "        StructField(\"DT_SIT_ESPECIAL\", StringType(), nullable=True)])\n",
    "\n",
    "    empresas = StructType([\n",
    "        StructField(\"CNPJ\", StringType(), nullable=True),\n",
    "        StructField(\"NOME_EMPRESA\", StringType(), nullable=True),\n",
    "        StructField(\"COD_NAT_JURICA\", StringType(), nullable=True),\n",
    "        StructField(\"QUALIF_RESPONVAVEL\", StringType(), nullable=True),\n",
    "        StructField(\"CAP_SOCIAL\", StringType(), nullable=True),\n",
    "        StructField(\"PORTE\", StringType(), nullable=True),\n",
    "        StructField(\"ENTE_FEDERATIVO\", StringType(), nullable=True)])\n",
    "\n",
    "    municipios = StructType([\n",
    "        StructField(\"ID_MUNICPIO\", StringType(), nullable=True),\n",
    "        StructField(\"MUNICIPIO\", StringType(), nullable=True)])\n",
    "\n",
    "    cnaes = StructType([\n",
    "        StructField(\"COD_CNAE\", StringType(), nullable=True),\n",
    "        StructField(\"CNAE\", StringType(), nullable=True)])\n",
    "    \n",
    "    paises = StructType([\n",
    "        StructField(\"COD_PAIS\", StringType(), nullable=True),\n",
    "        StructField(\"NM_PAIS\", StringType(), nullable=True)])\n",
    "    \n",
    "    qualificacoes = StructType([\n",
    "        StructField(\"COD_QUALIFICACAO\", StringType(), nullable=True),\n",
    "        StructField(\"NM_QUALIFICACAO\", StringType(), nullable=True)])\n",
    "\n",
    "    socios = StructType([\n",
    "        StructField(\"CNPJ_BASICO\", StringType(), nullable=True),\n",
    "        StructField(\"IDENTIFICADOR_SOCIO\", IntegerType(), nullable=True),\n",
    "        StructField(\"NOME_SOCIO_RAZAO_SOCIAL\", StringType(), nullable=True),\n",
    "        StructField(\"CNPJ_CPF_SOCIO\", StringType(), nullable=True),\n",
    "        StructField(\"QUALIFICAÇAO_SOCIO\", StringType(), nullable=True),\n",
    "        StructField(\"DATA_ENTRADA_SOCIEDADE\", StringType(), nullable=True),\n",
    "        StructField(\"PAIS\", StringType(), nullable=True),\n",
    "        StructField(\"REPRESENTANTE_LEGAL\", StringType(), nullable=True),\n",
    "        StructField(\"NOME_REPRESENTANTE\", StringType(), nullable=True),\n",
    "        StructField(\"QUALIFICACAO_REPRESENTANTE_LEGAL\", StringType(), nullable=True),\n",
    "        StructField(\"FAIXA_ETARIA\", StringType(), nullable=True)])\n",
    "\n",
    "    simples = StructType([\n",
    "        StructField(\"CNPJ_BASICO\", StringType(), nullable=True),\n",
    "        StructField(\"OPCAO_PELO_SIMPLES\", StringType(), nullable=True),\n",
    "        StructField(\"DATA_OPCAO_PELO_SIMPLES\", StringType(), nullable=True),\n",
    "        StructField(\"DATA_EXCLUSAO_SIMPLES\", StringType(), nullable=True),\n",
    "        StructField(\"OPÇAO_PELO_MEI\", StringType(), nullable=True),\n",
    "        StructField(\"DATA_OPCAO_PELO_MEI\", StringType(), nullable=True),\n",
    "        StructField(\"DATA_EXCLUSAO_MEI\", StringType(), nullable=True)])\n",
    "\n",
    "    naturezas = StructType([\n",
    "        StructField(\"COD_NAT_JURICA\", StringType(), nullable=True),\n",
    "        StructField(\"NAT_JURICA\", StringType(), nullable=True)])\n",
    "    \n",
    "    motivos = StructType([\n",
    "        StructField(\"COD_MOTIVO\", StringType(), nullable=True),\n",
    "        StructField(\"NM_MOTIVO\", StringType(), nullable=True)])\n",
    "    \n",
    "    dic_provedor = {'0UTLOOK': 'OUTLOOK', '123GMAIL': 'GMAIL', '12GMAIL': 'GMAIL', '19GMAIL': 'GMAIL', '1HOTMAIL': 'HOTMAIL', \n",
    "                    '2010HOTMAIL': 'HOTMAIL', '20GMAIL': 'GMAIL', '23GMAIL': 'GMAIL', '2GMAIL': 'GMAIL', '2HOTMAIL': 'HOTMAIL', \n",
    "                    '30GMAIL': 'GMAIL', '7GMAIL': 'GMAIL', 'ADV': 'ADV', 'AGMAIL': 'GMAIL', 'AHOO': 'YAHOO', 'AIL': 'AOL', 'ALUNO': 'ALUNO', \n",
    "                    'AOL': 'AOL', 'AUTLOOK': 'OUTLOOK', 'BB': 'BB', 'BOL': 'BOL', 'BOLL': 'BOL', 'BOOL': 'BOL', 'BRTURBO': 'OI', \n",
    "                    'CAIXA': 'CAIXA', 'CLICK21': 'CLICK21', 'CLOUD': 'ICLOUD', 'CRECI': 'CRECI', 'EDU': 'EDU', 'EMAIL': 'EMAIL', \n",
    "                    'FACEBOOK': 'FACEBOOK', 'FMAIL': 'GMAIL', 'G': 'GMAIL', 'G-MAIL': 'GMAIL', 'GAIL': 'GMAIL', 'GAMAIL': 'GMAIL', \n",
    "                    'GAMEIL': 'GMAIL', 'GAMIAL': 'GMAIL', 'GAMIL': 'GMAIL', 'GEMAIL': 'GMAIL', 'GGMAIL': 'GMAIL', 'GHMAIL': 'GMAIL', \n",
    "                    'GHOTMAIL': 'HOTMAIL', 'GIMAIL': 'GMAIL', 'GLOBO': 'GLOBO', 'GLOBOMAIL': 'LWMAIL', 'GMA': 'GMAIL', 'GMAAIL': 'GMAIL', \n",
    "                    'GMAI': 'GMAIL', 'GMAIAL': 'GMAIL', 'GMAII': 'GMAIL', 'GMAIIL': 'GMAIL', 'GMAIK': 'GMAIL', 'GMAIL': 'GMAIL', \n",
    "                    'GMAILC': 'GMAIL', 'GMAILGMAIL': 'GMAIL', 'GMAILL': 'GMAIL', 'GMAILMAIL': 'GMAIL', 'GMAILO': 'GMAIL', 'GMAIM': 'GMAIL', \n",
    "                    'GMAIO': 'GMAIL', 'GMAIOL': 'GMAIL', 'GMAIS': 'GMAIL', 'GMAISL': 'GMAIL', 'GMAIUL': 'GMAIL', 'GMAL': 'GMAIL', \n",
    "                    'GMALI': 'GMAIL', 'GMAOL': 'GMAIL', 'GMAQIL': 'GMAIL', 'GMASIL': 'GMAIL', 'GMAUIL': 'GMAIL', 'GMAUL': 'GMAIL',\n",
    "                    'GMEIL': 'GMAIL', 'GMIAL': 'GMAIL', 'GMIL': 'GMAIL', 'GML': 'GMAIL', 'GMMAIL': 'GMAIL', 'GMNAIL': 'GMAIL', \n",
    "                    'GMQIL': 'GMAIL', 'GMSIL': 'GMAIL', 'GNAIL': 'GMAIL', 'GNMAIL': 'GMAIL', 'GOMAIL': 'GMAIL', 'GOOGLEMAIL': 'GMAIL',\n",
    "                    'GOTMAIL': 'HOTMAIL', 'GTMAIL': 'GMAIL', 'H0TMAIL': 'HOTMAIL', 'HAHOO': 'YAHOO', 'HATMAIL': 'HOTMAIL', 'HAYOO': 'YAHOO', \n",
    "                    'HGMAIL': 'GMAIL', 'HHOTMAIL': 'HOTMAIL', 'HIOTMAIL': 'HOTMAIL', 'HITMAIL': 'HOTMAIL', 'HJOTMAIL': 'HOTMAIL', \n",
    "                    'HMAIL': 'HOTMAIL', 'HOITMAIL': 'HOTMAIL', 'HOLMAIL': 'HOTMAIL', 'HOLTMAIL': 'HOTMAIL', 'HOMAIL': 'HOTMAIL', \n",
    "                    'HOMTAIL': 'HOTMAIL', 'HOMTIAL': 'HOTMAIL', 'HOMTMAIL': 'HOTMAIL', 'HOOTMAIL': 'HOTMAIL', 'HOPTMAIL': 'HOTMAIL', \n",
    "                    'HORMAIL': 'HOTMAIL', 'HORTMAIL': 'HOTMAIL', 'HOT': 'HOTMAIL', 'HOTAIL': 'HOTMAIL', 'HOTAMAIL': 'HOTMAIL', \n",
    "                    'HOTAMIL': 'HOTMAIL', 'HOTEMAIL': 'HOTMAIL', 'HOTGMAIL': 'HOTMAIL', 'HOTIMAIL': 'HOTMAIL', 'HOTIMAL': 'HOTMAIL', \n",
    "                    'HOTLMAIL': 'HOTMAIL', 'HOTLOOK': 'OUTLOOK', 'HOTMA': 'HOTMAIL', 'HOTMAAIL': 'HOTMAIL', 'HOTMAI': 'HOTMAIL', \n",
    "                    'HOTMAIAL': 'HOTMAIL', 'HOTMAII': 'HOTMAIL', 'HOTMAIIL': 'HOTMAIL', 'HOTMAIL': 'HOTMAIL', 'HOTMAILC': 'HOTMAIL', \n",
    "                    'HOTMAILL': 'HOTMAIL', 'HOTMAILO': 'HOTMAIL', 'HOTMAIM': 'HOTMAIL', 'HOTMAIO': 'HOTMAIL', 'HOTMAIOL': 'HOTMAIL', \n",
    "                    'HOTMAIUL': 'HOTMAIL', 'HOTMAL': 'HOTMAIL', 'HOTMALI': 'HOTMAIL', 'HOTMAMIL': 'HOTMAIL', 'HOTMAOL': 'HOTMAIL', \n",
    "                    'HOTMAQIL': 'HOTMAIL', 'HOTMASIL': 'HOTMAIL', 'HOTMAUIL': 'HOTMAIL', 'HOTMAUL': 'HOTMAIL', 'HOTMEIL': 'HOTMAIL', \n",
    "                    'HOTMIAIL': 'HOTMAIL', 'HOTMIAL': 'HOTMAIL', 'HOTMIL': 'HOTMAIL', 'HOTMMAIL': 'HOTMAIL', 'HOTMNAIL': 'HOTMAIL',\n",
    "                    'HOTMQIL': 'HOTMAIL', 'HOTMSIL': 'HOTMAIL', 'HOTNAIL': 'HOTMAIL', 'HOTOMAIL': 'HOTMAIL', 'HOTRMAIL': 'HOTMAIL', \n",
    "                    'HOTTMAIL': 'HOTMAIL', 'HOTYMAIL': 'HOTMAIL', 'HOUTLOOK': 'OUTLOOK', 'HOYMAIL': 'HOTMAIL', 'HPTMAIL': 'HOTMAIL', \n",
    "                    'HTMAIL': 'HOTMAIL', 'HTOMAIL': 'HOTMAIL', 'HYAHOO': 'YAHOO', 'IAHOO': 'YAHOO', 'IBEST': 'IBEST', 'ICLAUD': 'ICLOUD', \n",
    "                    'ICLOD': 'ICLOUD', 'ICLOID': 'ICLOUD', 'ICLOOD': 'ICLOUD', 'ICLOU': 'ICLOUD', 'ICLOUD': 'ICLOUD', 'ICLOUDE': 'ICLOUD', \n",
    "                    'ICLOULD': 'ICLOUD', 'ICLOUND': 'ICLOUD', 'ICLUD': 'ICLOUD', 'ICLUOD': 'ICLOUD', 'ICOUD': 'ICLOUD', 'ICOULD': 'ICLOUD', \n",
    "                    'ID': 'IG', 'IG': 'IG', 'IGMAIL': 'GMAIL', 'IGUI': 'IG', 'IMAIL': 'GMAIL', 'INCLOUD': 'ICLOUD', 'ITELEFONICA': 'ITELEFONICA',\n",
    "                    'JMAIL': 'GMAIL', 'JOTMAIL': 'HOTMAIL', 'LIVE': 'LIVE', 'LWMAIL': 'LWMAIL', 'MAIL': 'MAIL', 'ME': 'ME', 'MSM': 'MSN', \n",
    "                    'MSN': 'MSN', 'NETSITE': 'NETSITE', 'OI': 'OI', 'OIMAIL': 'HOTMAIL', 'OITLOOK': 'OUTLOOK', 'OLTLOOK': 'OUTLOOK', \n",
    "                    'OOUTLOOK': 'OUTLOOK', 'OTLOOK': 'OUTLOOK', 'OTMAIL': 'HOTMAIL', 'OUL': 'UOL', 'OULOOK': 'OUTLOOK', 'OULTLOOK': 'OUTLOOK',\n",
    "                    'OULTOOK': 'OUTLOOK', 'OUTILOOK': 'OUTLOOK', 'OUTIOOK': 'OUTLOOK', 'OUTLLOK': 'OUTLOOK', 'OUTLLOOK': 'OUTLOOK', \n",
    "                    'OUTLOCK': 'OUTLOOK', 'OUTLOK': 'OUTLOOK', 'OUTLOKK': 'OUTLOOK', 'OUTLOOCK': 'OUTLOOK', 'OUTLOOK': 'OUTLOOK', \n",
    "                    'OUTLOOKL': 'OUTLOOK', 'OUTLOOL': 'OUTLOOK', 'OUTLOOOK': 'OUTLOOK', 'OUTLUK': 'OUTLOOK', 'OUTOLOOK': 'OUTLOOK',\n",
    "                    'OUTOOK': 'OUTLOOK', 'OUTOOLK': 'OUTLOOK', 'OUTTLOOK': 'OUTLOOK', 'OUTULOOK': 'OUTLOOK', 'POP': 'POP',\n",
    "                    'PROTON': 'PROTONMAIL', 'PROTONMAIL': 'PROTONMAIL', 'PUTLOOK': 'OUTLOOK', 'R7': 'R7', 'ROCKETMAIL': 'ROCKETMAIL', \n",
    "                    'ROCKTMAIL': 'ROCKETMAIL', 'ROTMAIL': 'HOTMAIL', 'SERCOMTEL': 'SERCOMTEL', 'SETELAGOASGML': 'GMAIL', \n",
    "                    'SUPERIG': 'SUPERIG', 'TAHOO': 'YAHOO', 'TERRA': 'TERRA', 'TERRRA': 'TERRA', 'TMAIL': 'GMAIL', \n",
    "                    'TVGLOBO': 'GLOBO', 'UAHOO': 'YAHOO', 'UAI': 'UAI', 'UFV': 'UFV', 'UNESP': 'UNESP', 'UNOCHAPECO': 'UNOCHAPECO', \n",
    "                    'UO': 'UOL', 'UOL': 'UOL', 'UOTLOOK': 'OUTLOOK', 'UPF': 'UPF', 'USP': 'USP', 'UTLOOK': 'OUTLOOK', 'VELOXMAIL': 'VELOXMAIL',\n",
    "                    'WINDOWSLIVE': 'WINDOWSLIVE', 'YAAHOO': 'YAHOO', 'YAGOO': 'YAHOO', 'YAHAOO': 'YAHOO', 'YAHHO': 'YAHOO', 'YAHHOO': 'YAHOO', \n",
    "                    'YAHO': 'YAHOO', 'YAHOO': 'YAHOO', 'YAHOOCOM': 'YAHOO', 'YAHOOL': 'YAHOO', 'YAHOOO': 'YAHOO', 'YAHOOU': 'YAHOO', \n",
    "                    'YANHOO': 'YAHOO', 'YAOO': 'YAHOO', 'YAOOL': 'YAHOO', 'YAROO': 'YAHOO', 'YHAOO': 'YAHOO', 'YHOO': 'YAHOO', 'YMAIL': 'YMAIL', \n",
    "                    'YOHOO': 'YAHOO', 'YOPMAIL': 'HOTMAIL', 'ZIPMAIL': 'ZIPMAIL', '_HOTMAIL': 'HOTMAIL',     'GMAUL': 'GMAIL','GMALE': 'GMAIL', \n",
    "                    'GMAILE': 'GMAIL', 'GMILE': 'GMAIL', 'HOTMEL': 'HOTMAIL', 'HOTMELL': 'HOTMAIL','HOTMEAL': 'HOTMAIL', 'OUTLOKES': 'OUTLOOK', \n",
    "                    'OTLOOKS': 'OUTLOOK', 'YAHU': 'YAHOO', 'YOHU': 'YAHOO', 'YAHUO': 'YAHOO', 'YAHEE': 'YAHOO', 'UOLL': 'UOL',\n",
    "                    'UOOL': 'UOL', 'UULL': 'UOL', 'ICLODUE': 'ICLOUD', 'ICLAWD': 'ICLOUD', 'ROCKEDMAIL': 'ROCKETMAIL', 'ROKETMAIL': 'ROCKETMAIL',\n",
    "                    'OUTLOKE': 'OUTLOOK', 'OUTLOOCKE': 'OUTLOOK', 'YAAHO': 'YAHOO', 'YAHOOE': 'YAHOO', 'YAHUE': 'YAHOO', 'HOTMILE': 'HOTMAIL', 'HOTMELE': 'HOTMAIL',\n",
    "                    'FACEBOKE': 'FACEBOOK', 'FACBOOK': 'FACEBOOK', 'FCEBOOK': 'FACEBOOK', 'BOLL': 'BOL', 'BOLLE': 'BOL', 'BULE': 'BOL', 'GLOBOE': 'GLOBO',\n",
    "                    'GLOBU': 'GLOBO', 'GMILE': 'GMAIL', 'MSNE': 'MSN', 'MSNN': 'MSN', 'ICLOOUD': 'ICLOUD', 'OUTLUKE': 'OUTLOOK', 'OUTLLOKE': 'OUTLOOK',\n",
    "                    'GMAUL': 'GMAIL','GMALE': 'GMAIL', 'GMAILE': 'GMAIL', 'GMILE': 'GMAIL', 'HOTMEL': 'HOTMAIL', 'HOTMELL': 'HOTMAIL',\n",
    "                    'HOTMEAL': 'HOTMAIL', 'OUTLOKES': 'OUTLOOK', 'OTLOOKS': 'OUTLOOK', 'YAHU': 'YAHOO', 'YOHU': 'YAHOO', 'YAHUO': 'YAHOO', \n",
    "                    'YAHEE': 'YAHOO', 'UOLL': 'UOL', 'UOOL': 'UOL', 'UULL': 'UOL', 'ICLODUE': 'ICLOUD', 'ICLAWD': 'ICLOUD',  'ROCKEDMAIL': 'ROCKETMAIL',\n",
    "                    'ROKETMAIL': 'ROCKETMAIL', 'OUTLOKE': 'OUTLOOK', 'OUTLOOCKE': 'OUTLOOK', 'YAAHO': 'YAHOO', 'YAHOOE': 'YAHOO', 'YAHUE': 'YAHOO', \n",
    "                    'HOTMILE': 'HOTMAIL', 'HOTMELE': 'HOTMAIL', 'FACEBOKE': 'FACEBOOK', 'FACBOOK': 'FACEBOOK', 'FCEBOOK': 'FACEBOOK', 'BOLL': 'BOL',\n",
    "                    'BOLLE': 'BOL', 'BULE': 'BOL', 'GLOBOE': 'GLOBO',  'GLOBU': 'GLOBO', 'GMILE': 'GMAIL', 'MSNE': 'MSN', 'MSNN': 'MSN', 'ICLOOUD': 'ICLOUD',\n",
    "                    'OUTLUKE': 'OUTLOOK', 'OUTLLOKE': 'OUTLOOK', 'PROTONMIAL': 'PROTONMAIL',  'PROTONMALE': 'PROTONMAIL', 'PROTOMAIL': 'PROTONMAIL', \n",
    "                    'OULOOKCOM': 'OUTLOOK', 'YAHCOM': 'YAHOO',  'YAHOCOM': 'YAHOO','GAMILCOM': 'GMAIL', 'GMALCOM': 'GMAIL',  'HOTMALCOM': 'HOTMAIL',  \n",
    "                    'HOTMILCOM': 'HOTMAIL', 'HOTMELCOM': 'HOTMAIL', 'ROCKMAIL': 'ROCKETMAIL', 'ROKMAIL': 'ROCKETMAIL', 'TERA': 'TERRA', 'TEERA': 'TERRA', \n",
    "                    'FACBOOKCOM': 'FACEBOOK', 'FACEBOOKCOM': 'FACEBOOK', 'ICLOWD': 'ICLOUD', 'ICLOUND': 'ICLOUD', 'UOOLCOM': 'UOL', 'UOLLCOM': 'UOL', \n",
    "                    'UOLCOMBR': 'UOL','LIVECOM': 'LIVE', 'LIVECOMBR': 'LIVE', 'GMAICOM': 'GMAIL',  'GMAILCOMBR': 'GMAIL',  'YAHOOBR': 'YAHOO', \n",
    "                    'YAHOOOCOMBR': 'YAHOO', 'YAHOOOCOM': 'YAHOO', 'ZIPMAILE': 'ZIPMAIL', 'ZIPMAILL': 'ZIPMAIL',  'IBESTT': 'IBEST', 'IBESTE': 'IBEST'}\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def detect_encoding(file_pattern_or_path, num_bytes=10000):\n",
    "        \"\"\"\n",
    "        Detecta a codificação do arquivo ou arquivos fornecidos.\n",
    "        \n",
    "        Parâmetros:\n",
    "            file_pattern_or_path (str): Caminho ou padrão do arquivo para detecção.\n",
    "            num_bytes (int, opcional): Número de bytes para ler para a detecção. Padrão é 10000.\n",
    "        \n",
    "        Retorna:\n",
    "            dict: Dicionário com caminho do arquivo como chave e codificação detectada como valor.\n",
    "        \"\"\"\n",
    "        files = glob.glob(file_pattern_or_path)\n",
    "        encodings = {}\n",
    "        for file_path in files:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                rawdata = f.read(num_bytes)\n",
    "                encodings[file_path] = chardet.detect(rawdata)[\"encoding\"]\n",
    "        return encodings\n",
    "\n",
    "\n",
    "    def read_data(self, schema_name, base_path=None):\n",
    "        \"\"\"\n",
    "        Lê dados de vários arquivos CSV de acordo com o esquema e caminho base fornecidos, consolidando-os \n",
    "        em um único DataFrame do Spark.\n",
    "\n",
    "        Parâmetros:\n",
    "            schema_name (str): Nome do esquema a ser usado para a leitura dos arquivos.\n",
    "                               Deve ser uma das chaves do dicionário `schemas`.\n",
    "\n",
    "            base_path (str, opcional): Caminho base dos arquivos CSV.\n",
    "                                       Se não for fornecido, ele tentará buscar da variável de ambiente 'BASE_PATH'.\n",
    "                                       Caso não encontre, o padrão \"./output\" será utilizado.\n",
    "\n",
    "        Retorna:\n",
    "            DataFrame: DataFrame do Spark contendo os dados consolidados dos arquivos CSV.\n",
    "\n",
    "        Exceções:\n",
    "            Pode lançar uma exceção se o arquivo não estiver presente no caminho especificado ou\n",
    "            se houver problemas de codificação ao ler o arquivo.\n",
    "\n",
    "        Exemplo:\n",
    "            receita_helper = ReceitaLT(spark_session)\n",
    "            df = receita_helper.read_data(\"estabelecimentos\", \"/path/to/csv/files\")\n",
    "\n",
    "        Notas:\n",
    "            - A função primeiro detecta a codificação dos arquivos antes de lê-los para garantir que \n",
    "              eles sejam lidos corretamente.\n",
    "            - A função lida com múltiplos arquivos CSV e os une em um único DataFrame.\n",
    "            - O formato de arquivo assumido é CSV com delimitador \";\", sem cabeçalho e com aspas para delimitar campos.\n",
    "        \"\"\"\n",
    "        schemas = {\n",
    "            \"estabelecimentos\": self.estabelecimentos,\n",
    "            \"empresas\": self.empresas,\n",
    "            \"municipios\": self.municipios,\n",
    "            \"cnaes\": self.cnaes,\n",
    "            \"socios\": self.socios,\n",
    "            \"simples\": self.simples,\n",
    "            \"naturezas\": self.naturezas,\n",
    "            \"qualificacoes\": self.qualificacoes,\n",
    "            \"motivos\": self.motivos,\n",
    "            \"paises\": self.paises}\n",
    "\n",
    "        # Se o base_path não for fornecido, pegar da variável de ambiente ou usar um padrão.\n",
    "        if not base_path:\n",
    "            base_path = os.environ.get('BASE_PATH', \"./output\")\n",
    "\n",
    "        if schema_name in ['estabelecimentos', 'empresas', 'socios']:\n",
    "            file_location_pattern = os.path.join(base_path, schema_name.capitalize(), '*.csv')\n",
    "        else:\n",
    "            file_location_pattern = os.path.join(base_path, schema_name.capitalize(), f\"{schema_name.capitalize()}.csv\")\n",
    "\n",
    "        # Detectar codificações\n",
    "        encodings = self.detect_encoding(file_location_pattern)\n",
    "        self.logger.info(f\"Detected encodings: {encodings}\")\n",
    "\n",
    "        # Agora, vamos ler cada arquivo com sua codificação correta e armazenar em uma lista de DataFrames\n",
    "        dfs = []\n",
    "        for file_location, encoding in encodings.items():\n",
    "            df = (self.spark.read.format(\"csv\")\n",
    "                  .option(\"sep\", \";\")\n",
    "                  .option(\"header\", \"false\")\n",
    "                  .option('quote', '\"')\n",
    "                  .option(\"escape\", '\"')\n",
    "                  .option(\"encoding\", encoding)\n",
    "                  .schema(schemas[schema_name])\n",
    "                  .load(file_location))\n",
    "            dfs.append(df)\n",
    "\n",
    "        # Unir todos os DataFrames em um único DataFrame\n",
    "        if dfs:\n",
    "            final_df = reduce(lambda a, b: a.union(b), dfs)\n",
    "        else:\n",
    "            final_df = self.spark.createDataFrame([], schemas[schema_name])\n",
    "\n",
    "        return final_df\n",
    "    \n",
    "\n",
    "    # Define the UDF\n",
    "    schema = StructType([\n",
    "        StructField(\"latitude\", FloatType(), nullable=True),\n",
    "        StructField(\"longitude\", FloatType(), nullable=True)\n",
    "    ])\n",
    "    \n",
    "    @udf(schema)\n",
    "    def geocode_udf(address):\n",
    "        \"\"\"\n",
    "        UDF do Spark para geocodificar um endereço dentro de um DataFrame.\n",
    "\n",
    "        Parâmetros:\n",
    "            address (str): Endereço a ser geocodificado.\n",
    "\n",
    "        Retorna:\n",
    "            dict: Dicionário contendo a 'latitude' e a 'longitude' do endereço fornecido.\n",
    "                  Se o endereço não puder ser geocodificado, os valores serão None.\n",
    "\n",
    "        Exemplo:\n",
    "            df.withColumn(\"location\", geocode_udf(df[\"address\"]))\n",
    "\n",
    "        Notas:\n",
    "            - Esta UDF encapsula a função `geocode_address`.\n",
    "            - Retorna um tipo de dado complexo (Struct) com dois campos: 'latitude' e 'longitude'.\n",
    "        \"\"\"\n",
    "        global geocode_address\n",
    "        lat, lon = geocode_address(address)\n",
    "        return {\"latitude\": lat, \"longitude\": lon}\n",
    "    \n",
    "    def process_estabelecimentos(self, df):\n",
    "        \n",
    "        \"\"\"\n",
    "        Processa e enriquece o DataFrame de estabelecimentos com informações adicionais e transformações.\n",
    "\n",
    "        Parâmetros:\n",
    "            df (DataFrame): DataFrame inicial contendo informações de estabelecimentos.\n",
    "\n",
    "        Retorna:\n",
    "            DataFrame: DataFrame processado e enriquecido com novas colunas e informações.\n",
    "\n",
    "        Descrição:\n",
    "            - Lê dataframes adicionais relacionados a países, municípios, cnaes e motivos.\n",
    "            - Realiza renomeações de colunas para facilitar junções.\n",
    "            - Enriquece o dataframe com informações de motivos, cnaes, municípios e países.\n",
    "            - Processa colunas de e-mail, separando provedores e corrigindo valores.\n",
    "            - Converte colunas de data de string para formato de data.\n",
    "            - Deriva colunas de ano e mês a partir de datas.\n",
    "            - Processa e deriva novas colunas com base em mapeamentos para situação cadastral e tipo de estabelecimento.\n",
    "            - Valida endereços de e-mail usando expressões regulares.\n",
    "            - Combina informações de endereço para formar uma coluna completa de endereço.\n",
    "            - Utiliza a função de geocodificação para obter coordenadas com base no endereço e, em caso de falha, com base no CEP.\n",
    "            - Realiza correções na coluna de provedor de e-mail usando um dicionário de mapeamento.\n",
    "\n",
    "        Notas:\n",
    "            - Esta função faz uso intensivo das operações de DataFrame do PySpark.\n",
    "            - Dependências: A função depende de outras funções e UDFs, como 'geocode_udf', bem como de variáveis de instância, como 'dic_provedor'.\n",
    "        \"\"\"\n",
    "        \n",
    "        df_pais = self.read_data(schema_name='paises')\n",
    "        df_mun = self.read_data(schema_name='municipios')\n",
    "        df_cnaes = self.read_data(schema_name='cnaes')\n",
    "        df_motivos = self.read_data(schema_name='motivos')\n",
    "        \n",
    "        df = df.withColumnRenamed(\"CNAE_1\", \"COD_CNAE\")\n",
    "        df = df.withColumnRenamed(\"MUNICIPIO\", \"ID_MUNICPIO\")\n",
    "        df = df.withColumnRenamed(\"PAIS\", \"COD_PAIS\")\n",
    "        df = df.withColumnRenamed(\"MOTIVO_CADASTRAL\", \"COD_MOTIVO\") \n",
    "        \n",
    "\n",
    "        df = df.join(broadcast(df_motivos), \"COD_MOTIVO\", \"left\").drop(df.COD_MOTIVO)\n",
    "        df = df.join(broadcast(df_cnaes), \"COD_CNAE\", \"left\").drop(df.COD_CNAE)\n",
    "        df = df.join(broadcast(df_mun), \"ID_MUNICPIO\", \"left\").drop(df.ID_MUNICPIO)\n",
    "        df = df.join(broadcast(df_pais), \"COD_PAIS\", \"left\").drop(df.COD_PAIS)\n",
    "        \n",
    "        dic_provedor = self.dic_provedor\n",
    "        # Tratamento da coluna provedor\n",
    "        df = df.withColumn(\"PROVEDOR\",  regexp_extract(\"EMAIL\", \"(?<=@)[^.]+(?=\\\\.)\", 0))\n",
    "        \n",
    "        # Colocando em caixa alta o provedor\n",
    "        df = df.withColumn(\"PROVEDOR\", upper(col(\"PROVEDOR\")))\n",
    "        \n",
    "        # Colocando em caixa baixa o email\n",
    "        df = df.withColumn(\"EMAIL\", lower(col(\"EMAIL\")))\n",
    "\n",
    "        # Convertendo colunas de data\n",
    "        df = df.withColumn(\"DT_SIT_CADASTRAL\", to_date(col('DT_SIT_CADASTRAL'), \"yyyyMMdd\"))\n",
    "        df = df.withColumn(\"DT_INICIO_ATIVIDADE\", to_date(col('DT_INICIO_ATIVIDADE'), \"yyyyMMdd\"))\n",
    "        df = df.withColumn(\"DT_SIT_ESPECIAL\", to_date(col('DT_SIT_ESPECIAL'), \"yyyyMMdd\"))\n",
    "        \n",
    "        df = df.withColumn( \"ano_cadastro\", year('DT_INICIO_ATIVIDADE'))\n",
    "        df = df.withColumn( \"mes_cadastro\", month('DT_INICIO_ATIVIDADE'))\n",
    "        df = df.withColumn( \"ano_sit_cadastral\", year('DT_SIT_CADASTRAL'))\n",
    "        df = df.withColumn( \"mes_sit_cadastral\", month('DT_SIT_CADASTRAL'))\n",
    "        \n",
    "        # Defina o dicionário de mapeamento\n",
    "        mapping = {1: 'NULA',2: 'ATIVA',3: 'SUSPENSA',4: 'INAPTA',8: 'BAIXADA'}\n",
    "        \n",
    "        # Use a função 'when' para criar a nova coluna 'NM_SIT_CADASTRAL'\n",
    "        df = df.withColumn(\"NM_SIT_CADASTRAL\",\n",
    "                           when(df[\"SIT_CADASTRAL\"].isin(list(mapping.keys())), df[\"SIT_CADASTRAL\"]).otherwise(None))\n",
    "        \n",
    "        # Substitua os valores na nova coluna com base no dicionário de mapeamento\n",
    "        for key, value in mapping.items():\n",
    "            df = df.withColumn(\"NM_SIT_CADASTRAL\", when(df[\"SIT_CADASTRAL\"] == key, value).otherwise(df[\"NM_SIT_CADASTRAL\"]))\n",
    "            \n",
    "        # Use uma expressão regular para validar os endereços de e-mail\n",
    "        email_pattern = r'^\\S+@\\S+\\.\\S+$'  # Padrão simples de endereço de e-mail\n",
    "        \n",
    "        # Use a função 'regexp_extract' para extrair endereços de e-mail válidos\n",
    "        df = df.withColumn(\"valid_email\", regexp_extract(col(\"EMAIL\"), email_pattern, 0))\n",
    "        \n",
    "        # Defina o dicionário de mapeamento\n",
    "        mapping = {1: 'MATRIZ',2: 'FILIAL'}\n",
    "        \n",
    "        # Use a função 'when' para criar a nova coluna 'NM_MATRIZ_FILIAL'\n",
    "        df = df.withColumn(\"NM_MATRIZ_FILIAL\",\n",
    "                           when(df[\"MATRIZ_FILIAL\"].isin(list(mapping.keys())), df[\"MATRIZ_FILIAL\"]).otherwise(None))\n",
    "        \n",
    "        # Substitua os valores na nova coluna com base no dicionário de mapeamento\n",
    "        for key, value in mapping.items():\n",
    "            df = df.withColumn(\"NM_MATRIZ_FILIAL\", when(df[\"MATRIZ_FILIAL\"] == key, value).otherwise(df[\"NM_MATRIZ_FILIAL\"]))\n",
    "            \n",
    "        # Criando a nova coluna \"ENDERECO_COMPLETO\"\n",
    "        df = df.withColumn(\"ENDERECO_COMPLETO\",\n",
    "                           concat_ws(\", \",\n",
    "                                     concat(df[\"TIPO_LOUGRADOURO\"], lit(\" \"), df[\"LOGRADOURO\"]),\n",
    "                                     \"NUMERO\",concat_ws(\" - \", \"MUNICIPIO\", \"UF\")))\n",
    "        \n",
    "        # Adicione a lógica de geocodificação aqui\n",
    "        #df = df.withColumn(\"COORDENADAS\", ReceitaLT.geocode_udf(df[\"ENDERECO_COMPLETO\"]))\n",
    "        \n",
    "        #df = df.withColumn(\"COORDENADAS\",\n",
    "        #                   when((col(\"COORDENADAS.latitude\").isNull()) & (col(\"COORDENADAS.longitude\").isNull()),\n",
    "        #                        ReceitaLT.geocode_udf(df[\"CEP\"])).otherwise(col(\"COORDENADAS\")))\n",
    "        \n",
    "        # Correção da coluna provedor\n",
    "        df = df.replace(dic_provedor, subset=['PROVEDOR'])\n",
    "\n",
    "        # Transformação das keys e values do dicionário em lowercase\n",
    "        dic_prov_lower = {k.lower(): str(v).lower() for k, v in dic_provedor.items()}\n",
    "\n",
    "        # Correção dos provedores na coluna EMAIL\n",
    "        replace_expr = reduce(\n",
    "            lambda a, b: regexp_replace(a, rf\"\\b{b[0]}\\b\", b[1]),\n",
    "            dic_prov_lower.items(),\n",
    "            col(\"valid_email\"))\n",
    "\n",
    "        df = df.withColumn(\"valid_email\", replace_expr)\n",
    "        df = df.withColumnRenamed(\"valid_email\", \"VALILD_EMAIL\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def process_empresas(self, df):\n",
    "        \"\"\"\n",
    "        Processa e enriquece o DataFrame de empresas com informações adicionais e transformações.\n",
    "\n",
    "        Parâmetros:\n",
    "            df (DataFrame): DataFrame inicial contendo informações de empresas.\n",
    "\n",
    "        Retorna:\n",
    "            DataFrame: DataFrame processado e enriquecido com novas colunas e informações.\n",
    "\n",
    "        Descrição:\n",
    "            - Lê dataframes adicionais relacionados a naturezas jurídicas e qualificações.\n",
    "            - Realiza renomeação de colunas para facilitar junções.\n",
    "            - Enriquece o dataframe com informações de naturezas jurídicas e qualificações.\n",
    "            - Processa a coluna 'NOME_EMPRESA' para extrair informações potenciais de CPF.\n",
    "            - Deriva uma nova coluna baseada no porte da empresa, usando um mapeamento predefinido.\n",
    "            - Determina a probabilidade de um valor ser um CPF válido com base em seu comprimento.\n",
    "            - Criptografa possíveis valores de CPF usando AES e os armazena em uma nova coluna 'CPF_CRIPTOGRAFADO', enquanto remove a coluna original 'CPF'.\n",
    "\n",
    "        Notas:\n",
    "            - Esta função faz uso intensivo das operações de DataFrame do PySpark.\n",
    "            - O valor de criptografia (secret_key) é gerado dinamicamente a cada chamada da função. Portanto, cada execução resultará em valores de 'CPF_CRIPTOGRAFADO' diferentes para os mesmos CPFs.\n",
    "            - O método AES usado aqui é 'ECB', que não é considerado seguro para muitos casos de uso devido à falta de vetor de inicialização (IV). A utilização deste modo deve ser revista se a segurança for uma preocupação.\n",
    "        \"\"\"\n",
    "        df_nat = self.read_data(schema_name='naturezas')\n",
    "        df_qual = self.read_data(schema_name='qualificacoes')\n",
    "        \n",
    "        df = df.withColumnRenamed(\"QUALIF_RESPONVAVEL\", \"COD_QUALIFICACAO\")\n",
    "        \n",
    "        df = df.join(broadcast(df_nat), \"COD_NAT_JURICA\", \"left\").drop(df.COD_NAT_JURICA)\n",
    "        df = df.join(broadcast(df_qual), \"COD_QUALIFICACAO\", \"left\").drop(df.COD_QUALIFICACAO)\n",
    "               \n",
    "        df = df.withColumn(\"CPF\", regexp_replace(\"NOME_EMPRESA\", \"[^0-9]\", \"\"))\n",
    "        df = df.withColumn(\"CPF\", when(col(\"CPF\") == \"\", None).otherwise(col(\"CPF\")))\n",
    "        df = df.withColumn('CPF_LEN', length('CPF'))\n",
    "        # Defina o dicionário de mapeamento\n",
    "        mapping = {0: 'NÃO INFORMADO',1: 'MICRO EMPRESA',3: ' EMPRESA DE PEQUENO PORTE',5: 'DEMAIS',8: 'BAIXADA'}\n",
    "        # Use a função 'when' para criar a nova coluna 'NM_SIT_CADASTRAL'\n",
    "        df = df.withColumn(\"NM_PORTE\",\n",
    "                           when(df[\"PORTE\"].isin(list(mapping.keys())), df[\"PORTE\"]).otherwise(None))\n",
    "        \n",
    "        # Substitua os valores na nova coluna com base no dicionário de mapeamento\n",
    "        for key, value in mapping.items():\n",
    "            df = df.withColumn(\"NM_PORTE\", when(df[\"PORTE\"] == key, value).otherwise(df[\"NM_PORTE\"]))\n",
    "\n",
    "        df = df.withColumn(\"PROBABILIDADE_DE_SER_CPF\", when(df[\"CPF_LEN\"] == 11, \"SIM\").otherwise(\"NAO\"))\n",
    "        \n",
    "        secret_key = secrets.token_urlsafe(24)\n",
    "        df = df.withColumn(\"CPF_CRIPTOGRAFADO\", expr(f\"base64(aes_encrypt(CPF, '{secret_key}', 'ECB', 'PKCS'))\")).drop(df.CPF)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def process_simples(self, df):\n",
    "        \"\"\"\n",
    "        Processa o DataFrame relacionado ao regime tributário SIMPLES das empresas.\n",
    "\n",
    "        Parâmetros:\n",
    "            df (DataFrame): DataFrame inicial contendo informações relacionadas ao regime tributário SIMPLES.\n",
    "\n",
    "        Retorna:\n",
    "            DataFrame: DataFrame processado com colunas de data convertidas e apenas as colunas relevantes selecionadas.\n",
    "\n",
    "        Descrição:\n",
    "            - Converte colunas que representam datas do formato \"yyyyMMdd\" para o tipo data.\n",
    "            - Seleciona apenas as colunas relevantes para o contexto, que são: 'CNPJ_BASICO', 'OPÇAO_PELO_MEI', 'DT_OPCAO_MEI', 'DT_EXCLUSAO_MEI', 'OPCAO_PELO_SIMPLES', 'DT_OPCAO_SIMPLES', e 'DT_EXCLUSAO_SIMPLES'.\n",
    "\n",
    "        Notas:\n",
    "            - Esta função assume que as colunas de data estão no formato \"yyyyMMdd\" e realiza a conversão para o tipo data.\n",
    "            - As colunas de datas que são processadas incluem: DATA_OPCAO_PELO_SIMPLES, DATA_EXCLUSAO_SIMPLES, DATA_EXCLUSAO_MEI e DATA_OPCAO_PELO_MEI.\n",
    "        \"\"\"\n",
    "        df = df.withColumn(\"DT_OPCAO_SIMPLES\", to_date,(col('DATA_OPCAO_PELO_SIMPLES'), \"yyyyMMdd\"))\n",
    "        df = df.withColumn(\"DT_EXCLUSAO_SIMPLES\", to_date(col('DATA_EXCLUSAO_SIMPLES'), \"yyyyMMdd\"))\n",
    "        df = df.withColumn(\"DT_EXCLUSAO_MEI\", to_date(col('DATA_EXCLUSAO_MEI'), \"yyyyMMdd\"))\n",
    "        df = df.withColumn(\"DT_OPCAO_MEI\", to_date(col('DATA_OPCAO_PELO_MEI'), \"yyyyMMdd\"))\n",
    "        df = df.select('CNPJ_BASICO','OPÇAO_PELO_MEI','DT_OPCAO_MEI','DT_EXCLUSAO_MEI','OPCAO_PELO_SIMPLES','DT_OPCAO_SIMPLES','DT_EXCLUSAO_SIMPLES')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def save_data(self, df, path, num_partitions=1, file_format=\"parquet\"):\n",
    "        \"\"\"\n",
    "        Save the DataFrame to the specified path.\n",
    "        \n",
    "        :param df: DataFrame to be saved\n",
    "        :param path: Destination path\n",
    "        :param num_partitions: Number of partitions for saving data (default is 1)\n",
    "        :param file_format: File format to save the data (default is \"parquet\")\n",
    "        \"\"\"\n",
    "        \n",
    "        # Repartitioning the DataFrame based on user input\n",
    "        df = df.repartition(num_partitions)\n",
    "        \n",
    "        # Saving the DataFrame to the specified path and format\n",
    "        df.write.mode('overwrite').format(file_format).save(path)\n",
    "        \n",
    "    @staticmethod\n",
    "    def download_nomes(save_base_path=\"./output/nomes\"):        \n",
    "        \"\"\"\n",
    "        Baixa e extrai o arquivo nomes.csv.gz do dataset genero-nomes no Brasil.io.\n",
    "\n",
    "        Parâmetros:\n",
    "            save_base_path (str, opcional): Caminho base onde o arquivo será salvo. O padrão é './output/nomes'.\n",
    "\n",
    "        Descrição:\n",
    "            - Cria o diretório de salvamento se ele não existir.\n",
    "            - Baixa o arquivo nomes.csv.gz da URL especificada.\n",
    "            - Extrai o conteúdo do arquivo .gz.\n",
    "            - Remove o arquivo .gz original, mantendo apenas o arquivo CSV extraído.\n",
    "\n",
    "        Notas:\n",
    "            - Esta função usa a biblioteca `requests` para baixar o arquivo.\n",
    "            - A função verifica se a resposta do servidor é 200 (sucesso) antes de baixar o arquivo.\n",
    "            - O arquivo .gz é extraído usando a biblioteca `gzip`.\n",
    "        \"\"\"\n",
    "        # Certifique-se de que o diretório de salvamento exista\n",
    "        os.makedirs(save_base_path, exist_ok=True)\n",
    "        url = \"https://data.brasil.io/dataset/genero-nomes/nomes.csv.gz\"\n",
    "        # Derive o nome do arquivo da URL\n",
    "        file_name = os.path.basename(url)\n",
    "        file_path = os.path.join(save_base_path, file_name)\n",
    "        extracted_file_path = os.path.join(save_base_path, file_name[:-3])  # remove .gz\n",
    "\n",
    "        # Baixe o arquivo\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(file_path, 'wb') as file:\n",
    "                for chunk in response.iter_content(chunk_size=128):\n",
    "                    file.write(chunk)\n",
    "        else:\n",
    "            print(f\"Failed to download {url}. Status code: {response.status_code}\")\n",
    "            return\n",
    "\n",
    "        # Extraia o arquivo\n",
    "        with gzip.open(file_path, 'rb') as f_in:\n",
    "            with open(extracted_file_path, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "        # Apague o arquivo .gz\n",
    "        os.remove(file_path)\n",
    "        \n",
    "    def process_mei(self, df, save_base_path=\"./output/nomes\", file_name=\"nomes.csv\"):\n",
    "        \"\"\"\n",
    "        Processa um DataFrame referente a MEIs, realiza joins com dados adicionais de naturezas jurídicas,\n",
    "        qualificações, e um conjunto de dados de nomes para extração e categorização de primeiro nome.\n",
    "\n",
    "        Parâmetros:\n",
    "            df (pyspark.sql.DataFrame): DataFrame inicial contendo dados sobre MEIs.\n",
    "            save_base_path (str, opcional): Caminho onde o arquivo com dados de nomes foi extraído. Padrão é './output/nomes'.\n",
    "            file_name (str, opcional): Nome do arquivo CSV contendo dados de nomes a ser lido. Padrão é 'nomes.csv'.\n",
    "\n",
    "        Retorna:\n",
    "            pyspark.sql.DataFrame: DataFrame processado após todas as transformações e joins.\n",
    "\n",
    "        Descrição:\n",
    "            1. Realiza join com DataFrames de 'naturezas' e 'qualificações'.\n",
    "            2. Extração e manipulação de dados de CPF.\n",
    "            3. Utiliza um dicionário para mapear e criar a coluna \"NM_PORTE\".\n",
    "            4. Criptografa a coluna de CPF.\n",
    "            5. Realiza filtragens baseado na probabilidade do nome ser um CPF válido.\n",
    "            6. Lê um conjunto de dados de nomes e realiza o explode na coluna 'alternative_names'.\n",
    "            7. Extrai o primeiro nome da coluna 'NOME_EMPRESA'.\n",
    "            8. Realiza o join com o conjunto de dados de nomes para categorizar o primeiro nome.\n",
    "            9. Retorna um DataFrame contendo informações relevantes após todas as transformações.\n",
    "        \"\"\"\n",
    "\n",
    "        df_nat = self.read_data(schema_name='naturezas')\n",
    "        df_qual = self.read_data(schema_name='qualificacoes')\n",
    "        \n",
    "        df = df.withColumnRenamed(\"QUALIF_RESPONVAVEL\", \"COD_QUALIFICACAO\")\n",
    "        \n",
    "        df = df.join(broadcast(df_nat), \"COD_NAT_JURICA\", \"left\").drop(df.COD_NAT_JURICA)\n",
    "        df = df.join(broadcast(df_qual), \"COD_QUALIFICACAO\", \"left\").drop(df.COD_QUALIFICACAO)\n",
    "               \n",
    "        df = df.withColumn(\"CPF\", regexp_replace(\"NOME_EMPRESA\", \"[^0-9]\", \"\"))\n",
    "        df = df.withColumn(\"CPF\", when(col(\"CPF\") == \"\", None).otherwise(col(\"CPF\")))\n",
    "        df = df.withColumn('CPF_LEN', length('CPF'))\n",
    "        # Defina o dicionário de mapeamento\n",
    "        mapping = {0: 'NÃO INFORMADO',1: 'MICRO EMPRESA',3: ' EMPRESA DE PEQUENO PORTE',5: 'DEMAIS',8: 'BAIXADA'}\n",
    "        # Use a função 'when' para criar a nova coluna 'NM_SIT_CADASTRAL'\n",
    "        df = df.withColumn(\"NM_PORTE\",\n",
    "                           when(df[\"PORTE\"].isin(list(mapping.keys())), df[\"PORTE\"]).otherwise(None))\n",
    "        \n",
    "        # Substitua os valores na nova coluna com base no dicionário de mapeamento\n",
    "        for key, value in mapping.items():\n",
    "            df = df.withColumn(\"NM_PORTE\", when(df[\"PORTE\"] == key, value).otherwise(df[\"NM_PORTE\"]))\n",
    "\n",
    "        df = df.withColumn(\"PROBABILIDADE_DE_SER_CPF\", when(df[\"CPF_LEN\"] == 11, \"SIM\").otherwise(\"NAO\"))\n",
    "        \n",
    "        secret_key = secrets.token_urlsafe(24)\n",
    "        df = df.withColumn(\"CPF_CRIPTOGRAFADO\", expr(f\"base64(aes_encrypt(CPF, '{secret_key}', 'ECB', 'PKCS'))\")).drop(df.CPF)\n",
    "        \n",
    "        # Caminho completo do arquivo\n",
    "        file_path = os.path.join(save_base_path, file_name)\n",
    "        \n",
    "        # Filtrar df_processed baseado na coluna PROBABILIDADE_DE_SER_CPF\n",
    "        df_filter = df.filter(col('PROBABILIDADE_DE_SER_CPF') == 'SIM').dropDuplicates(subset=['CPF_CRIPTOGRAFADO', 'NOME_EMPRESA'])\n",
    "\n",
    "        # Ler o arquivo CSV\n",
    "        df = self.spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "        # Explodir a coluna alternative_names para múltiplas linhas\n",
    "        df_expanded = df.withColumn(\"alternative_names\", explode(split(coalesce(col(\"alternative_names\"), col(\"first_name\")), \"\\\\|\")))\n",
    "\n",
    "        # Selecionar as colunas desejadas\n",
    "        df_result = df_expanded.select(\"alternative_names\", \"group_name\", \"ratio\", \"classification\").dropDuplicates(subset=['alternative_names'])\n",
    "\n",
    "        # Extrair o primeiro nome da coluna NOME_EMPRESA\n",
    "        df_filter = df_filter.withColumn(\"PRIMEIRO_NOME\", split(col(\"NOME_EMPRESA\"), \" \")[0])\n",
    "        \n",
    "        # Fazer o join entre df_filter e df_result\n",
    "        joined_df = df_filter.join(df_result, df_filter.PRIMEIRO_NOME == df_result.alternative_names, \"left\").dropDuplicates()\n",
    "        \n",
    "        joined_df = joined_df.select('CNPJ','NOME_EMPRESA','CAP_SOCIAL','NM_PORTE','NAT_JURICA','ENTE_FEDERATIVO','NM_QUALIFICACAO','CPF_CRIPTOGRAFADO','CPF_LEN',\n",
    "                    'PROBABILIDADE_DE_SER_CPF','PRIMEIRO_NOME',col('group_name').alias('GRUPO_NOME'), \n",
    "                    col('ratio').alias('PROBABILIDADE_CLASSIFICACAO'), col('classification').alias('CLASSIFICACAO')).dropDuplicates()\n",
    "        \n",
    "        return joined_df\n",
    "    \n",
    "    def process_socios(self, df, save_base_path=\"./output/nomes\", file_name=\"nomes.csv\"):\n",
    "        \"\"\"\n",
    "        Processa um DataFrame referente a sócios, realiza joins com dados adicionais de países, qualificações, \n",
    "        e um conjunto de dados de nomes para extração e categorização de primeiro nome.\n",
    "\n",
    "        Parâmetros:\n",
    "            df (pyspark.sql.DataFrame): DataFrame inicial contendo dados sobre sócios.\n",
    "            save_base_path (str, opcional): Caminho onde o arquivo com dados de nomes foi extraído. Padrão é './output/nomes'.\n",
    "            file_name (str, opcional): Nome do arquivo CSV contendo dados de nomes a ser lido. Padrão é 'nomes.csv'.\n",
    "\n",
    "        Retorna:\n",
    "            pyspark.sql.DataFrame: DataFrame processado após todas as transformações e joins.\n",
    "\n",
    "        Descrição:\n",
    "            1. Realiza join com DataFrames de 'países'.\n",
    "            2. Usa mapeamentos para criar colunas \"NM_FAIXA_ETARIA\" e \"NM_IDENTIFICADOR_SOCIO\".\n",
    "            3. Renomeia e realiza join com DataFrame de qualificações para obter descrições das qualificações.\n",
    "            4. Converte coluna de data \"DATA_ENTRADA_SOCIEDADE\" para o formato desejado.\n",
    "            5. Lê e processa um conjunto de dados de nomes, explodindo e selecionando colunas relevantes.\n",
    "            6. Extração do primeiro nome da coluna 'NOME_SOCIO_RAZAO_SOCIAL'.\n",
    "            7. Realiza o join entre o DataFrame processado e o conjunto de dados de nomes para categorizar o primeiro nome.\n",
    "            8. Retorna um DataFrame contendo informações relevantes após todas as transformações.\n",
    "        \"\"\"\n",
    "        \n",
    "        df_pais = self.read_data(schema_name='paises')\n",
    "        df = df.withColumnRenamed(\"PAIS\", \"COD_PAIS\")\n",
    "        df = df.join(broadcast(df_pais), \"COD_PAIS\", \"left\").drop(df.COD_PAIS)\n",
    "        \n",
    "        # Mapeamento de códigos para faixas etárias.\n",
    "        mapping = {\n",
    "            1: '0 a 12 anos',\n",
    "            2: '13 a 20 anos',\n",
    "            3: '21 a 30 anos',\n",
    "            4: '31 a 40 anos',\n",
    "            5: '41 a 50 anos',\n",
    "            6: '51 a 60 anos',\n",
    "            7: '61 a 70 anos',\n",
    "            8: '71 a 80 anos',\n",
    "            9: 'maiores de 80 anos',\n",
    "            0: 'NA'\n",
    "        }\n",
    "        \n",
    "                \n",
    "        # Mapeamento de códigos para faixas etárias.\n",
    "        id_socio = {\n",
    "            1: 'PESSOA JURIDICA',\n",
    "            2: 'PESSOA FISICA',\n",
    "            3: 'ESTRANGEIRO'}\n",
    "        \n",
    "        df_qual = self.read_data(schema_name='qualificacoes')\n",
    "        \n",
    "        \n",
    "        # Use a função 'when' para criar a nova coluna 'NM_SIT_CADASTRAL'\n",
    "        df = df.withColumn(\"NM_FAIXA_ETARIA\",\n",
    "                           when(df[\"FAIXA_ETARIA\"].isin(list(mapping.keys())), df[\"FAIXA_ETARIA\"]).otherwise(None))\n",
    "        \n",
    "        # Substitua os valores na nova coluna com base no dicionário de mapeamento\n",
    "        for key, value in mapping.items():\n",
    "            df = df.withColumn(\"NM_FAIXA_ETARIA\", when(df[\"FAIXA_ETARIA\"] == key, value).otherwise(df[\"NM_FAIXA_ETARIA\"]))\n",
    "            \n",
    "\n",
    "        # Use a função 'when' para criar a nova coluna 'NM_SIT_CADASTRAL'\n",
    "        df = df.withColumn(\"NM_IDENTIFICADOR_SOCIO\",\n",
    "                           when(df[\"IDENTIFICADOR_SOCIO\"].isin(list(id_socio.keys())), df[\"IDENTIFICADOR_SOCIO\"]).otherwise(None))\n",
    "        \n",
    "        # Substitua os valores na nova coluna com base no dicionário de mapeamento\n",
    "        for key, value in id_socio.items():\n",
    "            df = df.withColumn(\"NM_IDENTIFICADOR_SOCIO\", when(df[\"IDENTIFICADOR_SOCIO\"] == key, value).otherwise(df[\"NM_IDENTIFICADOR_SOCIO\"]))\n",
    "\n",
    "\n",
    "        df_qual = self.read_data(schema_name='qualificacoes')\n",
    "        # Renomeação e join com df_qual para obter descrições de qualificações.\n",
    "        df = df.withColumnRenamed(\"QUALIFICACAO_REPRESENTANTE_LEGAL\", \"COD_QUALIFICACAO\")\n",
    "        df = df.join(broadcast(df_qual), \"COD_QUALIFICACAO\", \"left\").drop(\"COD_QUALIFICACAO\")\n",
    "        df = df.withColumnRenamed(\"NM_QUALIFICACAO\", \"NM_QUALIFICACAO_REPRESENTANTE_LEGAL\")\n",
    "\n",
    "        df = df.withColumnRenamed(\"QUALIFICAÇAO_SOCIO\", \"COD_QUALIFICACAO\")\n",
    "        df = df.join(broadcast(df_qual), \"COD_QUALIFICACAO\", \"left\").drop(\"COD_QUALIFICACAO\")\n",
    "        df = df.withColumnRenamed(\"NM_QUALIFICACAO\", \"NM_QUALIFICAÇAO_SOCIO\")\n",
    "\n",
    "        # Conversão da coluna de data.\n",
    "        df = df.withColumn(\"DT_ENTRADA_SOCIEDADE\", to_date(col('DATA_ENTRADA_SOCIEDADE'), \"yyyyMMdd\")).drop(df.DATA_ENTRADA_SOCIEDADE)\n",
    "\n",
    "        # Leitura do arquivo CSV.\n",
    "        file_path = os.path.join(save_base_path, file_name)\n",
    "        df_csv = self.spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "        df_csv = df_csv.withColumn(\"alternative_name2\", explode(split(df_csv[\"alternative_names\"], \"\\|\")))\n",
    "        df_result = df_csv.select(\"alternative_name2\", \"group_name\", \"ratio\", \"classification\").dropDuplicates([\"alternative_name2\"])\n",
    "\n",
    "        # Extração do primeiro nome.\n",
    "        df = df.withColumn(\"PRIMEIRO_NOME\", split(col(\"NOME_SOCIO_RAZAO_SOCIAL\"), \" \")[0]).dropDuplicates()\n",
    "\n",
    "        # Join entre dataframes.\n",
    "        joined_df = df.join(df_result, df.PRIMEIRO_NOME == df_result.alternative_name2, \"left\").dropDuplicates()\n",
    "        \n",
    "        joined_df = joined_df.select('CNPJ_BASICO','NOME_SOCIO_RAZAO_SOCIAL','CNPJ_CPF_SOCIO','REPRESENTANTE_LEGAL',\n",
    "        'NOME_REPRESENTANTE','NM_PAIS','NM_FAIXA_ETARIA','NM_IDENTIFICADOR_SOCIO','NM_QUALIFICACAO_REPRESENTANTE_LEGAL',\n",
    "        'NM_QUALIFICAÇAO_SOCIO','DT_ENTRADA_SOCIEDADE','PRIMEIRO_NOME',col('ratio').alias('PROBABILIDADE_CLASSIFICACAO'),\n",
    "        col('classification').alias('CLASSIFICACAO')).dropDuplicates()\n",
    "\n",
    "        return joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71b0b65-8b2b-42f6-946c-23cf43021a07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#api = ReceitaCNPJApi()\n",
    "receitaLT_processor = ReceitaLT(spark) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fda066f-aca7-4c27-866d-8ff38b874246",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = receitaLT_processor.read_data(schema_name='empresas')\n",
    "df_mei = receitaLT_processor.process_mei(df)\n",
    "df_mei.repartition(20).write.mode(\"overwrite\").parquet('C:/Users/pedro/Documents/Curso de pos graduação de EST/DADOS_CNPJ/df_mei/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b69fc8b-9a36-4bac-b3b9-96efd43c2b9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mei.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249da5aa-5903-43a7-86a9-82962e6bab11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = receitaLT_processor.read_data(schema_name='estabelecimentos')\n",
    "df_estabelecimentos = receitaLT_processor.process_estabelecimentos(df)\n",
    "df_estabelecimentos.write.mode(\"overwrite\").parquet('C:/Users/pedro/Documents/Curso de pos graduação de EST/DADOS_CNPJ/df_estab/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21dd179-8fa7-49f9-9457-eb5f806a6892",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mei = spark.read.parquet(\"C:/Users/pedro/Documents/Curso de pos graduação de EST/DADOS_CNPJ/df_mei/\").select('CNPJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d73da6-a44f-41b6-ad93-c84b9e48a393",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "df_final = df_mei.join(df_estabelecimentos, df_mei.CNPJ==df_estabelecimentos.CNPJ_BASICO, 'inner').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c4bb49-9c74-4a5c-acd9-2cbe3a4406b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_estabelecimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb028974-82be-475e-a55f-6bef08fe1648",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = receitaLT_processor.read_data(schema_name='estabelecimentos')\n",
    "df_estabelecimentos = receitaLT_processor.process_estabelecimentos(df).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13cdc45-6cd3-4d1b-a995-7315b79a33b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d3fb6-e2e4-42d6-91bd-333fa38d13e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_final.repartition(25).write.mode(\"overwrite\").parquet('C:/Users/pedro/Documents/Curso de pos graduação de EST/DADOS_CNPJ/df_estab/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dc056e-1248-4b9c-bc04-98f38599e80e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_estabelecimentos.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9866df84-02ef-4816-818e-095ab7cebc5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = receitaLT_processor.read_data(schema_name='estabelecimentos')\n",
    "df_estabelecimentos = receitaLT_processor.process_estabelecimentos(df)\n",
    "df_estabelecimentos = df_estabelecimentos.select('CNPJ_BASICO','NM_MATRIZ_FILIAL', 'NM_SIT_CADASTRAL', 'DT_SIT_CADASTRAL', 'DT_INICIO_ATIVIDADE',\t\n",
    "'ENDERECO_COMPLETO', 'TIPO_LOUGRADOURO', 'CEP', 'UF', 'COORDENADAS',\n",
    "'MUNICIPIO', 'DDD1', 'TEL1','DDD2', 'TEL2','VALILD_EMAIL',\n",
    "'PROVEDOR', 'NM_MOTIVO', 'CNAE', 'NM_PAIS', 'ano_cadastro', 'mes_cadastro', 'DT_SIT_CADASTRAL',\n",
    "'ano_sit_cadastral'\t, 'mes_sit_cadastral').cache()\n",
    "df_estabelecimentos.repartition(25).write.mode(\"overwrite\").parquet('C:/Users/pedro/Documents/Curso de pos graduação de EST/DADOS_CNPJ/df_estab/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff6e9eb-060f-484e-8dbc-f34707362cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e37543a-40a2-45a7-95ab-b269a4599ff3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b84aa41-038d-47b8-a5b4-f4fe28274534",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mei = spark.read.parquet(\"C:/Users/pedro/Documents/Curso de pos graduação de EST/DADOS_CNPJ/df_mei/\")\n",
    "df_estabelecimentos = spark.read.parquet('C:/Users/pedro/Documents/Curso de pos graduação de EST/DADOS_CNPJ/df_estab/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e7f0a8-aba1-444a-aca4-588bdbd9f8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_mei.join(df_estabelecimentos, df_mei.CNPJ==df_estabelecimentos.CNPJ_BASICO, 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbdb70b-b6a7-4e20-9d02-e043cfb6d9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb31e99-a3b9-4c71-997e-962c8780c72e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_final = df_final.withColumn('CLASSIFICACAO', when((df_final.PRIMEIRO_NOME == 'JOSE') & \n",
    "                                         (df_final.CLASSIFICACAO == 'F'), 'M').otherwise(df_final.CLASSIFICACAO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28db2bdf-5999-4893-b98b-15f9f1a9b6a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_final.write.mode(\"overwrite\").parquet('C:/Users/pedro/Documents/Curso de pos graduação de EST/DADOS_CNPJ/df_final/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0730fd-8ebb-4127-9f91-81d59b55f015",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_final(path='/dataset_parte_2/df_mei_estab', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b7b31e-6c1d-45fa-9519-ab05ba313f40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = receitaLT_processor.read_data(schema_name='estabelecimentos')\n",
    "df_mei = spark.read.parquet(\"C:/Users/pedro/Documents/Curso de pos graduação de EST/DADOS_CNPJ/df_mei/\").select('CNPJ')\n",
    "df = df_mei.join(df, df_mei.CNPJ==df.CNPJ_BASICO, 'inner').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdf13a0-fcb8-4525-8510-83f2c08dabc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_pais = receitaLT_processor.read_data(schema_name='paises')\n",
    "df_mun = receitaLT_processor.read_data(schema_name='municipios')\n",
    "df_cnaes = receitaLT_processor.read_data(schema_name='cnaes')\n",
    "df_motivos = receitaLT_processor.read_data(schema_name='motivos')\n",
    "\n",
    "df = df.withColumnRenamed(\"CNAE_1\", \"COD_CNAE\")\n",
    "df = df.withColumnRenamed(\"MUNICIPIO\", \"ID_MUNICPIO\")\n",
    "df = df.withColumnRenamed(\"PAIS\", \"COD_PAIS\")\n",
    "df = df.withColumnRenamed(\"MOTIVO_CADASTRAL\", \"COD_MOTIVO\") \n",
    "        \n",
    "\n",
    "df = df.join(broadcast(df_motivos), \"COD_MOTIVO\", \"left\").drop(df.COD_MOTIVO)\n",
    "df = df.join(broadcast(df_cnaes), \"COD_CNAE\", \"left\").drop(df.COD_CNAE)\n",
    "df = df.join(broadcast(df_mun), \"ID_MUNICPIO\", \"left\").drop(df.ID_MUNICPIO)\n",
    "df = df.join(broadcast(df_pais), \"COD_PAIS\", \"left\").drop(df.COD_PAIS)\n",
    "        \n",
    "dic_provedor = receitaLT_processor.dic_provedor\n",
    "# Tratamento da coluna provedor\n",
    "df = df.withColumn(\"PROVEDOR\",  regexp_extract(\"EMAIL\", \"(?<=@)[^.]+(?=\\\\.)\", 0))\n",
    "        \n",
    "# Colocando em caixa alta o provedor\n",
    "df = df.withColumn(\"PROVEDOR\", upper(col(\"PROVEDOR\")))\n",
    "        \n",
    "# Colocando em caixa baixa o email\n",
    "df = df.withColumn(\"EMAIL\", lower(col(\"EMAIL\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bb58f3-6887-4f29-8c88-cdba8ceb7ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo colunas de data\n",
    "df = df.withColumn(\"DT_SIT_CADASTRAL\", to_date(col('DT_SIT_CADASTRAL'), \"yyyyMMdd\"))\n",
    "df = df.withColumn(\"DT_INICIO_ATIVIDADE\", to_date(col('DT_INICIO_ATIVIDADE'), \"yyyyMMdd\"))\n",
    "df = df.withColumn(\"DT_SIT_ESPECIAL\", to_date(col('DT_SIT_ESPECIAL'), \"yyyyMMdd\"))\n",
    "        \n",
    "df = df.withColumn( \"ano_cadastro\", year('DT_INICIO_ATIVIDADE'))\n",
    "df = df.withColumn( \"mes_cadastro\", month('DT_INICIO_ATIVIDADE'))\n",
    "df = df.withColumn( \"ano_sit_cadastral\", year('DT_SIT_CADASTRAL'))\n",
    "df = df.withColumn( \"mes_sit_cadastral\", month('DT_SIT_CADASTRAL'))\n",
    "        \n",
    "# Defina o dicionário de mapeamento\n",
    "mapping = {1: 'NULA',2: 'ATIVA',3: 'SUSPENSA',4: 'INAPTA',8: 'BAIXADA'}\n",
    "        \n",
    "# Use a função 'when' para criar a nova coluna 'NM_SIT_CADASTRAL'\n",
    "df = df.withColumn(\"NM_SIT_CADASTRAL\",\n",
    "                   when(df[\"SIT_CADASTRAL\"].isin(list(mapping.keys())), df[\"SIT_CADASTRAL\"]).otherwise(None))\n",
    "        \n",
    "# Substitua os valores na nova coluna com base no dicionário de mapeamento\n",
    "for key, value in mapping.items():\n",
    "    df = df.withColumn(\"NM_SIT_CADASTRAL\", when(df[\"SIT_CADASTRAL\"] == key, value).otherwise(df[\"NM_SIT_CADASTRAL\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0519ad0e-3bd2-4ba1-b003-b2f51b8c0c20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use uma expressão regular para validar os endereços de e-mail\n",
    "email_pattern = r'^\\S+@\\S+\\.\\S+$'  # Padrão simples de endereço de e-mail\n",
    "        \n",
    "# Use a função 'regexp_extract' para extrair endereços de e-mail válidos\n",
    "df = df.withColumn(\"valid_email\", regexp_extract(col(\"EMAIL\"), email_pattern, 0))\n",
    "        \n",
    "# Defina o dicionário de mapeamento\n",
    "mapping = {1: 'MATRIZ',2: 'FILIAL'}\n",
    "        \n",
    "# Use a função 'when' para criar a nova coluna 'NM_MATRIZ_FILIAL'\n",
    "df = df.withColumn(\"NM_MATRIZ_FILIAL\",\n",
    "                           when(df[\"MATRIZ_FILIAL\"].isin(list(mapping.keys())), df[\"MATRIZ_FILIAL\"]).otherwise(None))\n",
    "        \n",
    "# Substitua os valores na nova coluna com base no dicionário de mapeamento\n",
    "for key, value in mapping.items():\n",
    "    df = df.withColumn(\"NM_MATRIZ_FILIAL\", when(df[\"MATRIZ_FILIAL\"] == key, value).otherwise(df[\"NM_MATRIZ_FILIAL\"]))\n",
    "            \n",
    "# Criando a nova coluna \"ENDERECO_COMPLETO\"\n",
    "df = df.withColumn(\"ENDERECO_COMPLETO\",\n",
    "                   concat_ws(\", \",\n",
    "                             concat(df[\"TIPO_LOUGRADOURO\"], lit(\" \"), df[\"LOGRADOURO\"]),\n",
    "                             \"NUMERO\",concat_ws(\" - \", \"MUNICIPIO\", \"UF\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea99e38-e320-48ac-a2be-31effd5a5c08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.select('CNPJ_BASICO','NM_MATRIZ_FILIAL', 'NM_SIT_CADASTRAL', 'DT_INICIO_ATIVIDADE',\t\n",
    "'ENDERECO_COMPLETO', 'TIPO_LOUGRADOURO', 'CEP', 'UF',# 'COORDENADAS',\n",
    "'MUNICIPIO', 'DDD1', 'TEL1','DDD2', 'TEL2','valid_email',\n",
    "'PROVEDOR', 'NM_MOTIVO', 'CNAE', 'NM_PAIS', 'ano_cadastro', 'mes_cadastro', 'DT_SIT_CADASTRAL',\n",
    "'ano_sit_cadastral'\t, 'mes_sit_cadastral').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d1a060-ab9d-428f-920f-c29bcad51c8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400005e5-6577-4c3b-9a02-5076ec46255e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"CNPJ_BASICO\", StringType(), True),\n",
    "    StructField(\"NM_MATRIZ_FILIAL\", StringType(), True),\n",
    "    StructField(\"NM_SIT_CADASTRAL\", StringType(), True),\n",
    "    StructField(\"DT_INICIO_ATIVIDADE\", DateType(), True),\n",
    "    StructField(\"ENDERECO_COMPLETO\", StringType(), False),\n",
    "    StructField(\"TIPO_LOUGRADOURO\", StringType(), True),\n",
    "    StructField(\"CEP\", IntegerType(), True),\n",
    "    StructField(\"UF\", StringType(), True),\n",
    "    StructField(\"MUNICIPIO\", StringType(), True),\n",
    "    StructField(\"DDD1\", StringType(), True),\n",
    "    StructField(\"TEL1\", StringType(), True),\n",
    "    StructField(\"DDD2\", StringType(), True),\n",
    "    StructField(\"TEL2\", StringType(), True),\n",
    "    StructField(\"valid_email\", StringType(), True),\n",
    "    StructField(\"PROVEDOR\", StringType(), True),\n",
    "    StructField(\"NM_MOTIVO\", StringType(), True),\n",
    "    StructField(\"CNAE\", StringType(), True),\n",
    "    StructField(\"NM_PAIS\", StringType(), True),\n",
    "    StructField(\"ano_cadastro\", IntegerType(), True),\n",
    "    StructField(\"mes_cadastro\", IntegerType(), True),\n",
    "    StructField(\"DT_SIT_CADASTRAL\", DateType(), True),\n",
    "    StructField(\"ano_sit_cadastral\", IntegerType(), True),\n",
    "    StructField(\"mes_sit_cadastral\", IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b76fe5b-1c9e-442b-ad0e-fdfe4172108a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").schema(schema).parquet('C:/Users/pedro/Documents/Curso de pos graduação de EST/DADOS_CNPJ/df_estab/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa05f2a9-7a90-4570-98db-aaf66af9d6a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdbff4f-4ffc-4610-9688-7fe7e8bb835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"C:/Users/pedro/Documents/Curso de pos graduação de EST/DADOS_CNPJ/df_estab/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485f20d6-c614-4f6a-80cf-0095e397c7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02610cc6-6a95-4361-aefa-9d1dca7e8b64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1ba40c-2ed5-4660-8c07-5a8ca918b023",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2079487-a475-49db-892a-7138940a1783",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Correção da coluna provedor\n",
    "df = df.replace(dic_provedor, subset=['PROVEDOR'])\n",
    "\n",
    "# Transformação das keys e values do dicionário em lowercase\n",
    "dic_prov_lower = {k.lower(): str(v).lower() for k, v in dic_provedor.items()}\n",
    "\n",
    "# Correção dos provedores na coluna EMAIL\n",
    "replace_expr = reduce(\n",
    "lambda a, b: regexp_replace(a, rf\"\\b{b[0]}\\b\", b[1]),\n",
    "dic_prov_lower.items(),\n",
    "col(\"valid_email\"))\n",
    "\n",
    "df = df.withColumn(\"valid_email\", replace_expr)\n",
    "df = df.withColumnRenamed(\"valid_email\", \"VALILD_EMAIL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194365a9-aa9d-4b3e-a70b-aeed2e68b4d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").parquet('C:/Users/pedro/Documents/Curso de pos graduação de EST/DADOS_CNPJ/df_estab/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eda5e8a-9133-4a33-9c25-6a37463c5bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "        # Adicione a lógica de geocodificação aqui\n",
    "        #df = df.withColumn(\"COORDENADAS\", ReceitaLT.geocode_udf(df[\"ENDERECO_COMPLETO\"]))\n",
    "        \n",
    "        #df = df.withColumn(\"COORDENADAS\",\n",
    "        #                   when((col(\"COORDENADAS.latitude\").isNull()) & (col(\"COORDENADAS.longitude\").isNull()),\n",
    "        #                        ReceitaLT.geocode_udf(df[\"CEP\"])).otherwise(col(\"COORDENADAS\")))\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
